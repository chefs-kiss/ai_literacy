[
  {
    "objectID": "00-INTRO/lab-python.html",
    "href": "00-INTRO/lab-python.html",
    "title": "Task1: Understanding the data",
    "section": "",
    "text": "Group Members:\n\nTask1: Understanding the data\n\nFollow the link https://github.com/washingtonpost/data-police-shootings to read about the data. Write a three sentence summary of what is included in this data, and where it comes from. This dataset allows only a small number of categories for race, and the dataset fails to recognize that some people are multiracial. This is a weakness of the dataset. Include at least one other observation that may be a weakness of the data\n\n\nWritten Answer\n\n\nYou can view csv by running the code chunk below.\n\n\nimport pandas as pd\n\nfatal_police_shootings_data = pd.read_csv('https://raw.githubusercontent.com/washingtonpost/data-police-shootings/refs/heads/master/v2/fatal-police-shootings-data.csv')\n\nfatal_police_shootings_data.head(10)\n\n\n    \n\n\n\n\n\n\nid\ndate\nthreat_type\nflee_status\narmed_with\ncity\ncounty\nstate\nlatitude\nlongitude\nlocation_precision\nname\nage\ngender\nrace\nrace_source\nwas_mental_illness_related\nbody_camera\nagency_ids\n\n\n\n\n0\n3\n2015-01-02\npoint\nnot\ngun\nShelton\nMason\nWA\n47.246826\n-123.121592\nnot_available\nTim Elliot\n53.0\nmale\nA\nnot_available\nTrue\nFalse\n73\n\n\n1\n4\n2015-01-02\npoint\nnot\ngun\nAloha\nWashington\nOR\n45.487421\n-122.891696\nnot_available\nLewis Lee Lembke\n47.0\nmale\nW\nnot_available\nFalse\nFalse\n70\n\n\n2\n5\n2015-01-03\nmove\nnot\nunarmed\nWichita\nSedgwick\nKS\n37.694766\n-97.280554\nnot_available\nJohn Paul Quintero\n23.0\nmale\nH\nnot_available\nFalse\nFalse\n238\n\n\n3\n8\n2015-01-04\npoint\nnot\nreplica\nSan Francisco\nSan Francisco\nCA\n37.762910\n-122.422001\nnot_available\nMatthew Hoffman\n32.0\nmale\nW\nnot_available\nTrue\nFalse\n196\n\n\n4\n9\n2015-01-04\npoint\nnot\nother\nEvans\nWeld\nCO\n40.383937\n-104.692261\nnot_available\nMichael Rodriguez\n39.0\nmale\nH\nnot_available\nFalse\nFalse\n473\n\n\n5\n11\n2015-01-04\nattack\nnot\ngun\nGuthrie\nLogan\nOK\n35.876991\n-97.423454\nnot_available\nKenneth Joe Brown\n18.0\nmale\nW\nnot_available\nFalse\nFalse\n101\n\n\n6\n13\n2015-01-05\nshoot\ncar\ngun\nChandler\nMaricopa\nAZ\n33.327887\n-111.840959\nnot_available\nKenneth Arnold Buck\n22.0\nmale\nH\nnot_available\nFalse\nFalse\n195\n\n\n7\n15\n2015-01-06\npoint\nnot\ngun\nAssaria\nSaline\nKS\n38.703755\n-97.563904\nnot_available\nBrock Nichols\n35.0\nmale\nW\nnot_available\nFalse\nFalse\n490\n\n\n8\n16\n2015-01-06\naccident\nnot\nunarmed\nBurlington\nDes Moines\nIA\n40.809250\n-91.118875\nnot_available\nAutumn Steele\n34.0\nfemale\nW\nnot_available\nFalse\nTrue\n287\n\n\n9\n17\n2015-01-06\npoint\nnot\nreplica\nKnoxville\nAllegheny\nPA\n40.412936\n-79.991408\nnot_available\nLeslie Sapp III\n47.0\nmale\nB\nnot_available\nFalse\nFalse\n26254\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nList out some of the columns that are contained in this csv.\n\nWritten Answer\n\nFor each of those columns, give some examples for the data in that column, and what they mean. For example, some examples of values in the first column id are 22, 325, and 140.\n\nWritten Answer\n\nIn order to compare the representation of black people among the subjects of fatal police shootings to their representation in the general population, we need to know the percent of the United States population which is black. You can find this by googling ‚Äôracial demographics of the United States‚Äò. Include a citation with the organization and url.\n\nWritten Answer\n\n\n\n\nTask2: Reading data into a dict\nRun the code chunk below. This will select some of the features from the dataset, and put it into a Python dictionary.\n\npolicing = fatal_police_shootings_data[['id', 'name', 'date', 'armed_with', 'age', 'gender', 'race', 'state']]\npolicing_dict = policing.set_index('id').to_dict(orient='index')\n\n\nWhat information from the original csv is stored in policing_dict? What information from the original csv do we use as the keys in policing_dict? What is the type of the values in policing_dict? Hint: use type(dict[key]).\n\n\nStudent Answer Hint: create a code chunk with type(dict[key]) to find the data type of the values in our dictionary\n\n\n\nTask3: Using the database\n\nFinding a particular record: Find the record of the fatal police shooting with ID number 1694. If you‚Äôve followed current events in the past few years, there should be a familiar name here.\n\n\nStudent Answer Add policing_dict[1694] to a code chunk\n\n\nDisplaying data from a single record: Print the name and state of the individual with ID number 1694.\n\nStudent Answer\n\nDisplaying data from multiple records: The following code will display the entire dictionary using a for loop:\n\nfor person,data in policing dict.items():\nprint(policing dict[person])\nAdd an if statement to the for loop to filter out only individuals from Minnesota. This means you‚Äôll need to check the state for each person and only print data if the state is Minnesota.\n\nStudent Answer\n\n\n\nTask4: Summarizing our data\n\nCreating a new dictionary as a subset: Instead of displaying all the information, let‚Äôs create a new dictionary that only contains inviduals from Minnesota. The syntax for this is: key:values for key, values in dict.items() if condition where condition is what you‚Äôre filtering on and dict is your dictionary. Name this new dictionary MN_selection.\n\n\nStudent Answer\n\n\nCreating summarization functions: We‚Äôd like to create a function that takes in our database dictionary and returns a dictionary with counts of occurrences of each race among subjects of fatal police shootings. The keys in this dictionary should be races, and the corresponding values should be the number of subjects of that race.\n\n\nStudent Answer The starter code for this function is below:\n\n\ndef get_race_counts(database:dict)-&gt;dict:\n  race_counts = {}\n  for person, data in database.items():\n    #add logic to populate race counts\n\n\n  return race_counts\n\n\nUsing a summarization function: Print the fraction of fatal police shootings with a black subject. This should be a number between 0 and 1, and can be computed by dividing the number of fatal police shootings with a black subject, by the total number of fatal police shootings.\n\n\nStudent Answer Note: using get, len, and round may be helpful.\n\n\nHow does the proportion of black subjects in fatal police shootings compare to the proportion of black people in the United States population?\n\n\nStudent Answer\n\n\n\nTask5: Comparing Summarizations\n\nCreate a new dictionary called unarmed_selection, which is built using dictionary comprehension on the original dictionary policing_dict. This dictionary should have the same structure as policing dict, except it will only contain entries for fatal police shootings where the subject was unarmed.\n\n\nStudent Answer\n\n\nCreate a new dictionary, called unarmed race counts, which is created using the get_race_counts function on the dictionary unarmed_selection. The purpose of this dictionary will be to count the number of occurrences of each race among subjects of fatal police shootings, including only those where the subject is unarmed.\n\n\nStudent Answer\n\n\nPrint the fraction of unarmed fatal police shootings with a black subject. This should be a number between 0 and 1, and can be computed by dividing the number of unarmed fatal police shootings with a black subject (you can get this from the dictionary unarmed race counts), by the total number of unarmed fatal police shootings.\n\n\nStudent Answer\n\n\nHow does the proportion of black subjects in fatal police shootings where the subject is unarmed compare to the proportion of black people in the United States population? How does it compare to the proportion of black subjects in all police shootings?\n\n\nStudent Answer\n\n\n\nTask6: Trends over time\n\nCreating a list of a single feature: Let‚Äôs now take a look at creating a list with our data. Using the code below, edit it so that only dates are shown (hint: do something to data). Now, get rid of anything that isn‚Äôt the year (use some string slicing). Name this list year_col, which contains only the year for each record in our database.\n\n\nStudent Answer Edit [data for person, data in policing dict.items()]\n\n\nCreating counts from a list: Summarize the list you just created into a dictionary where each key is a single year and the value is the count of that year in our database. Note: If you do this with dictionary comprehension use the list method count and set(year col).\n\n\nStudent Answer\n\n\nHow do the number of fatal police shootings change over time? Do you notice any patterns or trends?\n\n\nStudent Answer\n\n\n\nTask7: More stories to be told\nSources: * U.S. Crime and Arrest Data (FBI UCR) * U.S. Demographic Data (Census Bureau) * Police Departments‚Äô Use of Force Data (BJS) * Pew Research on Policing * Other relevant sources (Reddit, social media, etc.)\n\nList at least two sources you found informative. Include the organization, url, and brief description of the information the source contains. This source could be an additional dataset, or it could be a result of an analysis done. Our goal is to try to link the insights from the previous tasks to other insights found elsewhere.\n\n\nStudent Answer\n\n\nCome up with at least three questions/insights that could supplement the analysis you have done so far. These questions should address some aspects of the following:\n\n\nvalidation: how can the data from these sources support or challenge the findings you‚Äôve already made about racial disparities in fatal police shootings?\nexpansion: how can additional data be used to expand upon the analysis in new areas, or further explore systemic issues related to police shootings?\ncomparisons: how do the findings from these sources compare to the conclusion you‚Äôve drawn from other tasks? Are there any contradictions or surprising findings?\n\n\nStudent Answer\n\n\n\nTask8: Reflection\n\nWrite a reflection (at least five sentences) on what you learned from this project. This can include your reaction to the results of the project, as well as the process of working with the data.\n\n\nStudent Answer\n\nWhen your group is done, each partner must submit a link to their workbook to Moodle. Make sure the link you shared is public."
  },
  {
    "objectID": "02-EVL/lab-EVL.html",
    "href": "02-EVL/lab-EVL.html",
    "title": "Task 5: Reflection",
    "section": "",
    "text": "Name:\nWho you worked with:\n##Objectives The goals of this project are to: * Implement different cross-validation techniques to evaluate model performance * Audit a model to discuss the ethical considerations in model selection and performance evaluation\n##Overview In this assignment, you will explore the Wisconsin Breast Cancer dataset and focus on key aspects of model evaluation and resampling techniques. You will comment on the complexity of chosen models in a developed workflow, discuss resampling techniques, as well as the implications on various evaluation metrics. You will also be asked to audit the algorithms using the ethical matrix framework discussed in class.\n##Schedule Here is the suggested schedule for working on this project: * Over the weekend, read through the project instructions and complete Task 0. * By Sunday, 2/23, complete Tasks 1-2 of the project, and start Task 3 of the project. * By Tuesday, 2/25, complete Tasks 3-4 of the project, and start Task 5. * By Wednesday, 2/26, complete Task 5 and check your solutions against the grading rubric (included at the end of this workbook), and submit your workbook url through moodle.\nThis project is due on Thursday, 2/27, by 11:59pm.\n#Task 0: Breast Cancer Workflow\nYou will use the Breast Cancer dataset from sklearn.datasets. It contains features of cell nuclei obtained from breast cancer biopsies, and the target variable indicates whether the tumor is malignant or benign.\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\n# Load the dataset\ndata = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target, name=\"target\")"
  },
  {
    "objectID": "02-EVL/lab-EVL.html#question-1-describe-dataset",
    "href": "02-EVL/lab-EVL.html#question-1-describe-dataset",
    "title": "Task 5: Reflection",
    "section": "‚úè Question 1: Describe dataset",
    "text": "‚úè Question 1: Describe dataset\n\nDescribe the type of data in our dataset.\nWhat is our target?\nWhat does our feature set contain?\n\n##‚úè Question 2: The who behind the data To answer the following questions, you may have to search the internet with search like ‚Äúwisconsin breast cancer dataset who is in the data‚Äù or something similar * Can you find who curated this dataset? * Include a url to cite this information. * Can you find the demographics of the individuals in the dataset? * In your opinion, why would these types of questions be important to know when dealing with the data?\n#Task 1: Model Complexity\nBefore jumping into evaluation and cross-validation, we‚Äôre going to start by performing basic preprocessing and setting up three models for comparison: a null model (also called a baseline model), a basic model, and a complex model.\nNull Model: This will predict the majority class from our target variable.\nBasic Model: This will be a Logistic Regression classifier.\nComplex Model:This will be a Random Forest classifier.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nnull_model = DummyClassifier(strategy='most_frequent', random_state=42)\nbasic_model = LogisticRegression(solver='liblinear', random_state=42)\ncomplex_model = RandomForestClassifier(random_state=42)\n\n\nnull_model.fit(X_train, y_train)\nbasic_model.fit(X_train, y_train)\ncomplex_model.fit(X_train, y_train)\n\n\nnull_pred = null_model.predict(X_test)\nbasic_pred = basic_model.predict(X_test)\ncomplex_pred = complex_model.predict(X_test)\n\n\nprint(\"Null Model Accuracy:\", accuracy_score(y_test, null_pred))\nprint(\"Basic Model Accuracy:\", accuracy_score(y_test, basic_pred))\nprint(\"Complex Model Accuracy:\", accuracy_score(y_test, complex_pred))\n\n\nprint(\"Null Model Confusion Matrix:\", confusion_matrix(y_test, null_pred))\nprint(\"Basic Model Confusion Matrix:\", confusion_matrix(y_test, basic_pred))\nprint(\"Complex Model Confusion Matrix:\", confusion_matrix(y_test, complex_pred))\n\n##üíª Question 3: Comments\n\nAdd comments to the code above that describe what each section of code is doing (to the best of your ability). You may want to consult our workbook on cross-validation (EVL2)."
  },
  {
    "objectID": "02-EVL/lab-EVL.html#question-4-accuracy-discussion",
    "href": "02-EVL/lab-EVL.html#question-4-accuracy-discussion",
    "title": "Task 5: Reflection",
    "section": "‚úè Question 4: Accuracy Discussion",
    "text": "‚úè Question 4: Accuracy Discussion\n\nCompare the accuracy metric for each of the three models. Does increasing model complexity drastically change the accuracy of the models? How well does the null (baseline) model compare to the simple and complex?\n\n#Task 2: Resampling\nFor this task, we‚Äôre going to use different cross-validation techniques to evaluate the models‚Äô performance more robustly.\n\nStratified K-Fold Cross-Validation (to maintain class distribution) using scikit-learn StratifiedKFold\nRepeated Cross-Validation (to get more robust performance metrics) using scikit-learn RepeatedStratifiedKFold\nBootstrapping (random sampling with replacement) using scikit-learn resample\n\n\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, RepeatedStratifiedKFold\nimport numpy as np\n\n\n# Stratified K-Fold Cross-Validation\nstratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nstratified_scores = cross_val_score(basic_model, X_train, y_train, cv=stratified_cv)\nprint(\"Stratified K-Fold Cross-Validation Scores:\", stratified_scores)\nprint(\"Mean Accuracy:\", np.mean(stratified_scores))\n\n\n# Repeated Cross-Validation\nrepeated_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\nrepeated_scores = cross_val_score(basic_model, X_train, y_train, cv=repeated_cv)\nprint(\"Repeated Cross-Validation Scores:\", repeated_scores)\nprint(\"Mean Accuracy:\", np.mean(repeated_scores))\n\n\n# Bootstrapping (Resampling)\nbootstrap_scores = []\nfor _ in range(50):\n    X_resampled, y_resampled = resample(X_train, y_train, random_state=42)\n    basic_model.fit(X_resampled, y_resampled)\n    score = basic_model.score(X_test, y_test)\n    bootstrap_scores.append(score)\n\nprint(\"Bootstrapping Accuracy (100 resamples):\", np.mean(bootstrap_scores))\n\n##üíª Question 5: Comments This is similar to #3 * Add comments to each of the code chunks above.\n##‚úè Question 6: Accuracy Discussion This is the same question as #4, but now considering the resampled accuracy metrics. * Describe how to compare the accuracy metric for each of the three models."
  },
  {
    "objectID": "02-EVL/lab-EVL.html#question-7-continued-evaluation",
    "href": "02-EVL/lab-EVL.html#question-7-continued-evaluation",
    "title": "Task 5: Reflection",
    "section": "‚úè Question 7: Continued evaluation",
    "text": "‚úè Question 7: Continued evaluation\nEven if a model has high accuracy, it may not be the best choice for our given situation.\n\nDescribe why this may be.\nWhat would be a solution for this problem?\n\n#Task 3: Evaluation Metrics\nIn this task, we will evaluate the performance of the models using different evaluation metrics such as accuracy, precision, recall, and F1 score.\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Train and evaluate model\nmodels = [(null_model, \"null model\"), (basic_model, \"basic model\"), (complex_model, \"complex model\")]\nfor model_type in models:\n  y_pred = model_type[0].predict(X_test)\n  # Evaluate\n  print(f\"{model_type[1]}: {model_type[0]}\")\n  print(\"Accuracy:\", round(accuracy_score(y_test, y_pred),3))\n  print(\"Precision:\", round(precision_score(y_test, y_pred),3))\n  print(\"Recall:\", round(recall_score(y_test, y_pred),3))\n  print(\"F1 Score:\", round(f1_score(y_test, y_pred),3), \"\\n\")\n\n##Questions 8-11: Interpret Metrics\nFor each model, interpret the following evaluation metrics:\n##‚úè Question 8: Accuracy\nNull model:\nBasic model:\nComplex model:\n##‚úè Question 9: Precision\nNull model:\nBasic model:\nComplex model:\n##‚úè Question 10: Recall\nNull model:\nBasic model:\nComplex model:\n##‚úè Question 11: Which is the best Once you have interpreted all of metrics, we‚Äôd like to choose which model is the best given the problem. You‚Äôll need to consider the trade-offs between precision, recall, and accuracy and how that impacts the model‚Äôs suitability for real-world application, especially in a healthcare context.\n\nDiscuss which of the models is the best performing.\n\n#Task 4: Ethical Matrix and Audit\nIn this task, you will create an ethical matrix to audit the algorithms you have been working with. The goal is to analyze the broader implications of your machine learning model and how it might affect different stakeholders. Following the guidelines outlined in class, your ethical audit should include key questions that explore the potential harms, benefits, fairness, and accountability of the algorithm.\n##‚úè Question 12: Define the key stakeholders\nIn any algorithmic system, there are various stakeholders who will be affected by the decisions made by the model. For example, one stakeholder is the patient being screened for breast cancer.\n\nAdd at least three more stakeholders:\n\n##Questions 13-15: Identify the ethical dimensions\nFor each stakeholder, consider the potential benefits and harms of using the model. Also, think about the ethical principles of fairness and accountability.\nThe main issues we‚Äôve covered in class are listed below: * Benefits: What positive outcomes might each stakeholder experience if the model is deployed? * Harms: What potential negative consequences might arise for each stakeholder? Could the model cause harm or lead to incorrect conclusions? * Fairness: Is the model equally fair to all stakeholders, especially those from under-represented or vulnerable groups? Does the model avoid reinforcing bias? * Accountability: Who is responsible if the model makes a mistake? What steps should be taken if the model‚Äôs predictions are inaccurate or harmful?\nIf you decide to consider different issues, make sure to update the text boxes below to account for that. For example, if you want to consider something like medical advances you might replace the issue fairness with medical advances.\n##‚úè Question 13:\nStakeholder1:\nBenefits:\nHarms:\nFairness:\nAccountability:\n##‚úè Question 14:\nStakeholder2:\nBenefits:\nHarms:\nFairness:\nAccountability:\n##‚úè Question 15:\nStakeholder3:\nBenefits:\nHarms:\nFairness:\nAccountability:\n##‚úè Question 16: Mitigating Harm\nBased on your matrix, come up with two concrete actions that can be taken to mitigate potential harms. Example action: we‚Äôd like to ensure the model is interpretable for healthcare providers so they can understand and trust its predictions. This means we‚Äôd need to choose a model that is less complex but still robust enough to give good results.\n\nFirst option:\nSecond option:"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "Stuff about labs\n\n\nThis lab uses Python dictionaries to look at national policing with regard to Black Americans.\n\n\n\nThis lab uses Titanic data to use basic EDA techniques.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#overview",
    "href": "labs.html#overview",
    "title": "Labs",
    "section": "",
    "text": "Stuff about labs\n\n\nThis lab uses Python dictionaries to look at national policing with regard to Black Americans.\n\n\n\nThis lab uses Titanic data to use basic EDA techniques.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "04-DATA/lab-DATA.html",
    "href": "04-DATA/lab-DATA.html",
    "title": "Task 1: NYC Data",
    "section": "",
    "text": "Name:\nWho you worked with:"
  },
  {
    "objectID": "04-DATA/lab-DATA.html#objectives",
    "href": "04-DATA/lab-DATA.html#objectives",
    "title": "Task 1: NYC Data",
    "section": "Objectives",
    "text": "Objectives\nThe goals of this project are to: - Perform EDA and data cleaning - Implement transformations and filtering functions - Look for EDA and data cleaning inspiration from outside sources"
  },
  {
    "objectID": "04-DATA/lab-DATA.html#overview",
    "href": "04-DATA/lab-DATA.html#overview",
    "title": "Task 1: NYC Data",
    "section": "Overview",
    "text": "Overview\nFor this programming assignment, you will practice some data cleaning and preprocessing, using a dataset of AirBNB data from New York City. In working with this dataset, our goal will be to train a model that can predict the price of an AirBNB rental. Imagine that you are a data scientist working for AirBNB, and your boss has asked you to develop this as a tool that can suggest prices for new listings."
  },
  {
    "objectID": "04-DATA/lab-DATA.html#schedule",
    "href": "04-DATA/lab-DATA.html#schedule",
    "title": "Task 1: NYC Data",
    "section": "Schedule",
    "text": "Schedule\nHere is the suggested schedule for working on this project: - Weekend: Read through project instructions, complete Task 1. - Tuesday: Complete Tasks 2-3. - Wednesday: Complete Tasks 4-5. - Thursday: Complete Task 6.\nThis project is due on Thursday, 3/13, by 11:59pm."
  },
  {
    "objectID": "04-DATA/lab-DATA.html#held-out-set",
    "href": "04-DATA/lab-DATA.html#held-out-set",
    "title": "Task 1: NYC Data",
    "section": "Held-Out Set",
    "text": "Held-Out Set\nNow, I‚Äôm going to select a few records that we‚Äôll use at the end to test a prediction function. I‚Äôm selecting these here, since we‚Äôll be making changes to the dataset later.\n\nchosen_indices = range(0, 9000, 1000)\n\nheld_out = nyc.iloc[chosen_indices]\n\nheld_out"
  },
  {
    "objectID": "04-DATA/lab-DATA.html#price",
    "href": "04-DATA/lab-DATA.html#price",
    "title": "Task 1: NYC Data",
    "section": "Price",
    "text": "Price\nWe‚Äôll start by looking at our target, price. Let‚Äôs look at a histogram, to see how the prices are distributed.\n\nnyc['price'].hist(bins = 80)\n\nThis is extremely skewed. We have a price that‚Äôs $100,000, but the vast majority have a price under $1000.\nThinking about the tool we‚Äôre building, we might want to focus on training a model that predicts well for lower/typical prices, and ignore the really expensive ones. Let‚Äôs look at what happens with a few different choices for restricting the prices.\nFirst, we look at prices below $2,000.\n\nprint(\"Number of entries: \", len(nyc[nyc['price'] &lt; 2000]))\n\nnyc[nyc['price'] &lt; 2000]['price'].hist(bins = 100)\n\nNext, let‚Äôs look at prices below $1,000.\n\nprint(\"Number of entries: \", len(nyc[nyc['price'] &lt; 1000]))\n\nnyc[nyc['price'] &lt; 1000]['price'].hist(bins = 100)\n\nHere, we look at prices below $500.\n\nprint(\"Number of entries: \", len(nyc[nyc['price'] &lt; 500]))\n\nnyc[nyc['price'] &lt; 500]['price'].hist(bins = 100)\n\nLet‚Äôs focus on prices below $500. This still captures a lot of the data, while limiting us to trying to predict prices for more typical listings. Note that the distribution is still skewed - we‚Äôll handle this in a bit.\n\nnyc = nyc[nyc['price'] &lt; 500]\n\nnyc.describe(include = \"all\")\n\nNotice that the minimum price is $0, which seems like nonsense. Let‚Äôs look at the listings with a price of $0.\n\nnyc[nyc['price'] &lt;= 0]\n\nThere aren‚Äôt too many of them, so let‚Äôs just exclude those.\n\nnyc = nyc[nyc['price'] &gt; 0]\n\nnyc.describe(include = \"all\")\n\nWe‚Äôve seen that even restricting to prices below $500, we still have a skewed distribution. Let‚Äôs see if applying a transformation can help. First, we try applying a log function.\n\nnyc[\"price\"].apply(np.log).hist(bins = 50)\n\nLet‚Äôs also see what happens with applying a square root function.\n\nnyc[\"price\"].apply(np.sqrt).hist(bins = 50)\n\nWe‚Äôll choose to apply the log transformation. Note that applying such a transformation does make sense here, because the difference between prices $400 and $450 is less significant than the difference between prices $50 and $100.\n##üíª Q5: Log Price Add a new column called log_price, which has the log transformation appied to the column price. Then, drop price from the dataframe.\n\n# add column log_price\nnyc[\"log_price\"] = #your code here\n\n#drop price\n#your code here\n\nnyc.describe(include = \"all\")\n\n\nassert(np.isclose(nyc[\"log_price\"].mean(), 4.60618))\nassert(\"price\" not in nyc.columns)"
  },
  {
    "objectID": "04-DATA/lab-DATA.html#latitude-and-longitude",
    "href": "04-DATA/lab-DATA.html#latitude-and-longitude",
    "title": "Task 1: NYC Data",
    "section": "Latitude and Longitude",
    "text": "Latitude and Longitude\nNow, let‚Äôs take a look at latitude and longitude. We‚Äôll look at the maximum and mininum values, to see the range they fall in.\n\nprint(nyc['latitude'].max())\nprint(nyc['latitude'].min())\nprint(nyc['longitude'].max())\nprint(nyc['longitude'].min())\n\nThis range makes sense here, since all of the listings are in New York City. Now, let‚Äôs look at histograms.\n\nnyc['latitude'].hist(bins = 50)\n\n\nnyc['longitude'].hist(bins = 50)\n\nThe distributions look pretty close to normal, so we‚Äôll leave them as they are."
  },
  {
    "objectID": "04-DATA/lab-DATA.html#number-of-reviews",
    "href": "04-DATA/lab-DATA.html#number-of-reviews",
    "title": "Task 1: NYC Data",
    "section": "Number of Reviews",
    "text": "Number of Reviews\nNow, let‚Äôs look at number of reviews. We‚Äôll create a histogram to see the distribution of the data.\n\nnyc['number_of_reviews'].hist(bins = 50)\n\nThis has a very dramatic right skew, so let‚Äôs try applying some transformations. We‚Äôll start with a log transformation.\nNow, if we were to apply the log transformation to this feature we would end up with an error. To fix this we need to add 1 to each value prior to log transforming it. This process is called smoothing and can be particularly helpful when working with text data in the future.\n##‚úè Q6: Add one smoothing\nExplain why we need to add 1. Hint: are there any values that when you apply the log transformation you would run into issues?\n[your answer here]\nNow let‚Äôs visualize this add-one smoothing log transformation\n\nnyc[\"number_of_reviews\"].apply(lambda x: (np.log(x+1))).hist(bins = 50)\n\nThis is an improvement, but still right skewed. Let‚Äôs look at a square root transformation next.\n\nnyc[\"number_of_reviews\"].apply(np.sqrt).hist(bins = 50)\n\nThat‚Äôs not as good as the log transformation.\nSince the result of the log transformation is still pretty skewed, let‚Äôs try applying the log transformation again. Again, we‚Äôll have to add 1 before applying the log transformation.\n##üíª Q7: log-log function In the cell below, define a function called log_log that does the following computation: \\[ f(x) = \\log(\\log(x+1)+1)\\].\nNote: replace the keyword pass with the computation above.\n\ndef log_log(value):\n  pass\n\n# create histogram\nnyc[\"number_of_reviews\"].apply(log_log).hist(bins = 50)\n\nThis still isn‚Äôt great, but we can see that we‚Äôre going to have trouble improving much more, since the number of reviews is going to be very discrete for small numbers (hence the tall, isolated columns on the left side). So, we‚Äôll use the log-log transformation.\nNow construct a new column, log_log_number_of_reviews, by applying the log_log function to the column number_of_reviews. Drop the old column, number_of_reviews, from the data frame\n\n# your code here\n\n\nnyc.describe(include = \"all\")\n\n\nassert(\"log_log_number_of_reviews\" in nyc.columns)\nassert(\"number_of_reviews\" not in nyc.columns)\nassert(np.isclose(nyc[\"log_log_number_of_reviews\"].mean(), 0.896842))"
  },
  {
    "objectID": "04-DATA/lab-DATA.html#calculated-host-listings-count",
    "href": "04-DATA/lab-DATA.html#calculated-host-listings-count",
    "title": "Task 1: NYC Data",
    "section": "Calculated Host Listings Count",
    "text": "Calculated Host Listings Count\nNow, let‚Äôs look at the column calculated_host_listings_count. We‚Äôll look at the distribution.\n\nnyc['calculated_host_listings_count'].hist(bins = 50)\n\nSince this is very skewed, let‚Äôs look at what happens when we apply a log transformtion.\n\nnyc[\"calculated_host_listings_count\"].apply(np.log).hist(bins = 50)\n\nStill very skewed, so let‚Äôs try the log log transformation\n\nnyc[\"calculated_host_listings_count\"].apply(log_log).hist(bins = 50)\n\nStill skewed, and we can tell that the data becoming very discrete is going to be an issue. This might not be the best choice, but let‚Äôs just stick with applying a log transformation.\n\nnyc[\"log_calculated_host_listings_count\"] = nyc[\"calculated_host_listings_count\"].apply(np.log)\n\nnyc.drop(\"calculated_host_listings_count\", axis = 1, inplace = True)\n\nnyc.describe(include = \"all\")"
  },
  {
    "objectID": "04-DATA/lab-DATA.html#room-type",
    "href": "04-DATA/lab-DATA.html#room-type",
    "title": "Task 1: NYC Data",
    "section": "Room Type",
    "text": "Room Type\nWe‚Äôll look at room type next.\n\nnyc['room_type'].value_counts()\n\n##‚úè Q10: Reasoning for OHE\nWe‚Äôre going to apply one-hot encoding to this feature as it is. Why are we choosing to do the OHE immediately instead of doing additional cleaning on this feature first?\n[your answer here]\nLet‚Äôs go ahead and OHE room_type\n\nroom_type_dummies = pd.get_dummies(nyc[\"room_type\"], prefix = \"room_type\")\n\nnyc = nyc.join(room_type_dummies)\nnyc.drop('room_type', axis = 1, inplace = True)\n\nnyc.describe(include = \"all\")"
  },
  {
    "objectID": "04-DATA/lab-DATA.html#data-cleaning-relates-to-model-performance",
    "href": "04-DATA/lab-DATA.html#data-cleaning-relates-to-model-performance",
    "title": "Task 1: NYC Data",
    "section": "Data cleaning relates to model performance",
    "text": "Data cleaning relates to model performance\nNow that we have gone through our features and made some transformations, we would typically train a model to see how well it performs. Remember, data cleaning is the most impactful thing you can do to improve your model performance.\nThe following steps would create a model to predict the price of an AirBNB rental based off our transformed feature set.\nSteps: * Selecting our features and target. * Split the data into testing and training sets. * Fit selected predictive model to the training data. * Check appropriate evaluation metric for our training data and for our testing data. * Plot predicted values again true target values, to visualize how the selected model is performing.\nThis last step means that we will need to apply transformations to the parameters, that correspond to the transformations we did on the features of the dataset.\nAn example for implementation with our data set: * Use selected model trained above to make a predict for log_price. * ‚ÄúUndo‚Äù the log transformation, and return the predicted price. * Compare your predictions to the actual prices for the records we pulled out.\n##‚úè Q11: Next Steps Even after some data cleaning, suppose that we are not getting very good performance. What are two things you could try next?\nOption1:\nOption2:"
  },
  {
    "objectID": "04-DATA/lab-DATA.html#finding-inspiration-in-others",
    "href": "04-DATA/lab-DATA.html#finding-inspiration-in-others",
    "title": "Task 1: NYC Data",
    "section": "Finding inspiration in others",
    "text": "Finding inspiration in others\nThe best way to learn how to clean data is to see what other people do!\nFor this last task, scroll down and select one of the notebooks someone has published on the Kaggle site (linked in Task 1). Note: you may have to dig a bit to see one you like.\nOnce you have a notebook selected, read through it and answer the following questions.\n##‚úè Q12: Url of notebook used:\n[your answer here]\n##‚úè Q13: EDA\n\nDescribe something new this workbook has done with EDA.\nWhat is one reason this may be a good choice for our data.\nWhat is one reason that this may not be a good choice for our data.\nHow does this compare to what we‚Äôve done so far with the data?\n\n[your answer here]\n##üíª Q14: EDA plot Now, try to replicate this EDA step in the code chunk below. You can copy-paste from the author‚Äôs work, but you may need to change a few things so it can work on our dataframe nyc\n\n#your code here\n\n##‚úè Q15: Data Cleaning\n\nDescribe something new this workbook has done with data cleaning.\nWhat is one reason this may be a good choice for our data.\nWhat is one reason that this may not be a good choice for our data.\nHow does this compare to what we‚Äôve done so far with the data?\n\n[your answer here]\n##üíª Q16: Data cleaning step Now, try to replicate this data cleaning step in the code chunk below. You can copy-paste from the author‚Äôs work, but you may need to change a few things so it can work on our dataframe nyc\n\n#your code here\n\n#Task 6: Reflection\nTake a moment to reflect on the assingment\n##‚úè Q17: Reflection\nWhat did you like about it? What could be improved? Your answers will not affect your overall grade. This feedback will be used to improve future programming assignments.\n#Grading\nFor each of the following accomplishments, there is a breakdown of points which total to 20. The fraction of points earned out of 20 will be multiplied by 5 to get your final score (e.g.¬†17 points earned will be 17/20 * 5 ‚Üí 4.25) * (1pt) Task1 q1: Correctly filters dataset * (2pt) Task1 q2: Gives 1-2 sectence descriptions for each feature * (2pt) Task2 q3: Discussed reasons for why all the listed features were dropped * (1pt) Task2 q4: Discussed reasons for why all the listed features were kept * (1pt) Task3 q5: Correctly implemented a log function * (1pt) Task3 q6: Identified the reason for adding 1 to each value * (1pt) Task3 q7: Correctly implemented a log_log with add-one function * (1pt) Task4 q8: Correclt implemented a filtering function for neighborhoods * (1pt) Task4 q9: Replicated the OHE code for neighborhoods * (1pt) Task4 q10: Discusses why we can do OHE without cleaning * (2pt) Task5 q11: Gives two scenarios to improve model performance for our airbnb dataset. Must include specific examples from the data. * (1pt) Task5 q12: Url of inspiration is included * (1pt) Task5 q13: Discusses new EDA from inspiration notebook * (1pt) Task5 q14: Replicates (successfully) the EDA plot * (1pt) Task5 q15: Discusses new data cleaning from inspiration notebook * (1pt) Task5 q16: Replicates (successfully) the data cleaning * (1pt) Task6 q17: Thoughtfully reflected on the assignment"
  },
  {
    "objectID": "05-TREE/lab-TREE.html",
    "href": "05-TREE/lab-TREE.html",
    "title": "Task 1: Evaluation for multiple random testing-training splits",
    "section": "",
    "text": "Name:\nWho you worked with:"
  },
  {
    "objectID": "05-TREE/lab-TREE.html#objectives",
    "href": "05-TREE/lab-TREE.html#objectives",
    "title": "Task 1: Evaluation for multiple random testing-training splits",
    "section": "Objectives",
    "text": "Objectives\nThe goals of this project are to: - Implement multiple tree-based models - Tune models to find the best combination of hyperparameters"
  },
  {
    "objectID": "05-TREE/lab-TREE.html#overview",
    "href": "05-TREE/lab-TREE.html#overview",
    "title": "Task 1: Evaluation for multiple random testing-training splits",
    "section": "Overview",
    "text": "Overview\nFor this programming assignment, you will be working with the Wisconsin Breast Cancer dataset. This dataset contains various measurements of the size and shape of tumors, along with the diagnosis of the tumor as benign or malignant. Further information on this dataset can be found here: https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic). Our goal is to train a tree-based model that can use the measurements of a tumor to diagnosis it as benign or malignant."
  },
  {
    "objectID": "05-TREE/lab-TREE.html#schedule",
    "href": "05-TREE/lab-TREE.html#schedule",
    "title": "Task 1: Evaluation for multiple random testing-training splits",
    "section": "Schedule",
    "text": "Schedule\nHere is the suggested schedule for working on this project: - Weekend: Read through project instructions, run code for Task 0. - Tuesday: Complete Tasks 1-2. - Wednesday: Complete Tasks 3. - Thursday: Complete Task 4.\nThis project is due on Thursday, 3/20, by 11:59pm.\n#Task 0: Data\nWe start by loading the dataset into a pandas dataframe.\n\nimport pandas as pd\n\ncancer_df = pd.read_csv(\"https://github.com/lynn0032/MLCamp2021/raw/main/breast_cancer.csv\")\n\nNext, we look at a summary of the data. Note that not every column is shown in this summary.\n\ncancer_df.describe(include = \"all\")\n\nWe can use shape to check the size of the data. Here, we see that we have 569 samples (for 569 tumors), and 33 attributes.\n\ncancer_df.shape\n\nNormally, we‚Äôd do exploratory data analysis here to understand the distribution of the dataset. For this programming assignment, however, we‚Äôre going to focus on evaluation for a classifier, so we will neglect to do that here.\nThere are two columns that we won‚Äôt use for prediction, so we drop those columns.\n\ncancer_df.drop([\"id\", \"Unnamed: 32\"], axis=1, inplace = True)\n\ncancer_df.describe(include=\"all\")\n\nNext, we separate the dataset into features and target. Remember that the diagnosis is the target, and it takes values ‚ÄúM‚Äù (for malignant) and ‚ÄúB‚Äù for benign.\n\nfeatures = list(cancer_df.columns)\nfeatures.remove(\"diagnosis\")\n\nX = cancer_df[features].values\ny = cancer_df[\"diagnosis\"].values"
  },
  {
    "objectID": "05-TREE/lab-TREE.html#tuning-max_depth",
    "href": "05-TREE/lab-TREE.html#tuning-max_depth",
    "title": "Task 1: Evaluation for multiple random testing-training splits",
    "section": "Tuning max_depth",
    "text": "Tuning max_depth\nNow, we will use the evaluate function to tune the parameter max_depth, with the default value for min_samples_leaf (which is 1).\nThe code is provided for you, but you will need to interpret the results.\n\ntest_accuracies = []\ntrain_accuracies = []\n\nfor max_depth in range(1,20):\n  test_acc, train_acc = evaluate(max_depth, 1)\n  test_accuracies.append(test_acc)\n  train_accuracies.append(train_acc)\n\n  print(\"Max depth\", max_depth)\n  print(\"\\tTesting Accuracy:\", test_acc)\n  print(\"\\tTraining Accuracy:\", train_acc)\n\n\nimport matplotlib.pyplot as plt\n\ntest = plt.plot(range(1,20), test_accuracies, label = \"Testing\")\ntrain = plt.plot(range(1,20), train_accuracies, label = \"Training\")\nplt.ylim(0, 1)      # to \"zoom in\", you can delete this line\nplt.xlabel('max_depth')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\n\n##‚úè Q4: describe max_depth results.\nBe sure to discuss overfitting vs.¬†underfitting, as well as which value for max_depth you think is best and why.\nyour answer here"
  },
  {
    "objectID": "05-TREE/lab-TREE.html#tuning-min_samples_leaf",
    "href": "05-TREE/lab-TREE.html#tuning-min_samples_leaf",
    "title": "Task 1: Evaluation for multiple random testing-training splits",
    "section": "Tuning min_samples_leaf",
    "text": "Tuning min_samples_leaf\nNext, we will use the evaluate function to tune the parameter min_samples_leaf, with the default value for max_depth (which is None). The code is provided for you, but you will need to interpret the results.\n\ntest_accuracies = []\ntrain_accuracies = []\n\nfor min_samples_leaf in range(1,100):\n  test_acc, train_acc = evaluate(None, min_samples_leaf)\n  test_accuracies.append(test_acc)\n  train_accuracies.append(train_acc)\n\n  print(\"Min samples leaf\", min_samples_leaf)\n  print(\"\\tTesting Accuracy:\", test_acc)\n  print(\"\\tTraining Accuracy:\", train_acc)\n\n\nimport matplotlib.pyplot as plt\n\ntest = plt.plot(range(1,100), test_accuracies, label = \"Testing\")\ntrain = plt.plot(range(1,100), train_accuracies, label = \"Training\")\nplt.ylim(0, 1)      # to \"zoom in\", you can delete this line\nplt.xlabel('min_samples_leaf')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\n\n##‚úèQ5: Describe min_samples_leaf results\nBe sure to discuss overfitting vs.¬†underfitting, as well as which value for min_samples_leaf you think is best and why.\nyour answer here\n##üíªQ6: Best Combination\nWhen tuning parameters, it typically takes more effort than just tuning parameters individually. Really, we want the best combination of parameters, so we want to explore them together.\nOne way to do this is with a grid search, where we explore a grid of possible parameter values. In the cell below, you will conduct a grid search to find the best combination of max_depth and min_samples_leaf.\n\nUsing nested for loops, use the function evaluate to test all combinations of max_depth and min_samples_leaf, where max_depth ranges from 1 through 10, and min_samples_leaf ranges from 1 through 50 (can you see how I chose these ranges from the results for the individual parameters above?).\nRemember that the function evaluate returns both the testing accuracy and training accuracy. For simplicity, we will just look for the parameters with the best testing accuracy.\nFor the parameters with the best testing accuracy, store their values in the tuple parameters (with max_depth first, followed by min_samples_leaf). Store their testing accuracy and training accuracy (as found by by evaluate), in the tuple best_metrics.\n\nThe grid search should take a couple of minutes to run. Because it can be time consuming, it is not always possible to run a grid search for big datasets with complex models.\n\nfrom tables.array import Leaf\nbest_metrics = (0,0)\nparameters = (0,0)\n\n# your code here\n\n\nprint(\"Best max_depth:\", parameters[0])\nprint(\"Best min_samples_leaf:\", parameters[1])\nprint(\"Testing Accuracy:\", best_metrics[0])\nprint(\"Training Accuracy:\", best_metrics[1])\n\n\n# TEST CELL - DO NOT CHANGE - RUN THIS CELL TO CHECK YOUR WORK\n# Note: passing this cell doesn't guarantee your code is correct or that you will get full credit,\n# but should be used to help you check your work\nassert(best_metrics == (0.9315013196708584, 0.9578214767688452))\nprint(\"Tests passed, but be sure to test your own code as well!\")\n\n#Task 4: Final Model + Reflection\n##Training a final model\nNow that we have identified the optimal parameter values, we will train a final model on all of the data, using those parameters values. The idea is that this final model would be used to diagnose tumors.\nAlthough we were able to find the optimal parameter values for decision trees, there are other machine learning models that can have even better performance on this dataset. Unfortunately, we don‚Äôt have time to cover more in this course.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=parameters[0], min_samples_leaf=parameters[1], random_state = 0)\nclf.fit(X, y)\n\nNext, we visualize the decision tree that we produced, so that we can see how it makes decisions.\n\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 10))\nplot_tree(clf, feature_names = features, class_names = [\"Benign\", \"Malignant\"])\nplt.show()\n\n##‚úèQ7: New record 1\nSuppose we encounter a new tumor, with values: * area_se = 35.021 * concave_points_worst = 0.100 * concavity_worst = .238 * radius_worst = 17.106 * texture_mean = 15.987 * texture_worst = 26.832\nHow would the decision tree model classify this tumor? Explain your answer by describing the path this datapoint takes through the tree.\nyour answer here\n##‚úèQ8: New record 2\nSuppose we encounter a new tumor, with values: * area_se = 39.542 * concave_points_worst = 0.154 * concavity_worst = .221 * radius_worst = 16.234 * texture_mean = 16.785 * texture_worst = 25.979\nHow would the decision tree model classify this tumor? Explain your answer by describing the path this datapoint takes through the tree.\nyour answer here\n##‚úè Q9: Reflection\nWhat did you like about it? What could be improved? Your answers will not affect your overall grade. This feedback will be used to improve future programming assignments.\n#Grading\nFor each of the following accomplishments, there is a breakdown of points which total to 20. The fraction of points earned out of 20 will be multiplied by 5 to get your final score (e.g.¬†17 points earned will be 17/20 * 5 ‚Üí 4.25) * (1pt) Task1 Q1: Uses correct cross validation * (1pt) Task1 Q1: Trains and fits the data on a decision tree * (1pt) Task1 Q1: Predicts on correct set * (1pt) Task1 Q1: Finds all three evaluation metrics and appends to list * (2pt) Task1 Q2: Correctly interprets accuracy and recall results * (1pt) Task2 Q3: Uses stratified fold correctly * (1pt) Task2 Q3: Finds test and training sets * (1pt) Task2 Q3: Trains and fits a decision tree * (1pt) Task2 Q3: Makes predictions * (1pt) Task3 Q4: Describes max_depth results including over/under fitting and best value * (1pt) Task3 Q5: Describes min_samples_leaf results including over/under fitting and best value * (2pt) Task3 Q6 : Correctly used for loops to loop through hyperparameters * (1pt) Task3 Q6 : Using evaluate function from Task 2 * (2pt) Task3 Q6 : Updates best model appropriately * (1pt) Task4 Q7: Correctly classifies new tumor using decision tree path * (1pt) Task4 Q8: Correctly classifies new tumor using decision tree path * (1pt) Task4 Q9: Thoughtfully reflected on the assignment"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Stuff about homework\n\n\nThis homework goes over looking at summary tables for a dataset, explaining statistical terms to non-tech folks, exploring statistics, and interpretting statistical tests.\n\n\n\nThis homework goes over model selection in our 5-step ML workflow, as well as evaluation metrics for both classification and regression.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#overview",
    "href": "homework.html#overview",
    "title": "Homework",
    "section": "",
    "text": "Stuff about homework\n\n\nThis homework goes over looking at summary tables for a dataset, explaining statistical terms to non-tech folks, exploring statistics, and interpretting statistical tests.\n\n\n\nThis homework goes over model selection in our 5-step ML workflow, as well as evaluation metrics for both classification and regression.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This is the homepage for CSCI 200 - Introduction to Machine Learning taught by Prof.¬†Kim Mandery in January 2026 at St.¬†Olaf College. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here .",
    "crumbs": [
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Machine Learning",
    "section": "",
    "text": "This is the homepage for CSCI 200 - Introduction to Machine Learning taught by Prof.¬†Kim Mandery in January 2026 at St.¬†Olaf College. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here .",
    "crumbs": [
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "course_information/syllabus.html",
    "href": "course_information/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "CSCI 200B Interim 2026 Special Topics: Machine Learning Morning Sessions: 10:40-12:40 in RNS 203 Afternoon Sessions: 1-3 in RNS 203\n\n\nMission: This is a welcoming, inclusive, encouraging, and failure-tolerant class.  Instructor: Kim Mandery  E-mail: mander1@stolaf.edu  Office: RMS 407\nTextbook: Introduction to Statistical Learning with Python (ISL-P). This book is free and online.\n\n\n\nIt has become increasingly common to use machine learning algorithms to analyze data, draw conclusions, and build models, without direct human instruction. These algorithms have been used in a wide variety of applications, including Netflix recommendations, predicting healthcare outcomes, criminal justice, and many more. In this course, we‚Äôll explore several common machine learning algorithms, learning how they work, and applying them to real datasets. We will cover the strengths and limitations of machine learning algorithms. We will also explore real-world applications of machine learning, and discuss the ethical and societal consequences of the use of these algorithms.\n\n\n\nThrough the lens of machine learning, we will:  - develop and interpret applications of algorithms to domain use-cases - develop working software that satisfies coding best practices - work effectively, both individually and in teams - communicate information effectively to both technical and non-technical audiences\nCourse Schedule: The schedule for this course is available here. This document is likely to be updated throughout the semester, and will be announced in class if it is.\nActive Learning and Engagement: I expect you to take an active role in class each day, participating positively, wholeheartedly, and respectfully in class/group discussions and other activities. You may need sick/mental health days. You are responsible for learning the material on the day you missed. If you are experiencing issues causing you to miss classes often, please speak with me as this may be evident of a possible failing grade. You can also visit your class dean as they are a great resource."
  },
  {
    "objectID": "course_information/syllabus.html#general-information",
    "href": "course_information/syllabus.html#general-information",
    "title": "Syllabus",
    "section": "",
    "text": "Mission: This is a welcoming, inclusive, encouraging, and failure-tolerant class.  Instructor: Kim Mandery  E-mail: mander1@stolaf.edu  Office: RMS 407\nTextbook: Introduction to Statistical Learning with Python (ISL-P). This book is free and online."
  },
  {
    "objectID": "course_information/syllabus.html#course-description",
    "href": "course_information/syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "It has become increasingly common to use machine learning algorithms to analyze data, draw conclusions, and build models, without direct human instruction. These algorithms have been used in a wide variety of applications, including Netflix recommendations, predicting healthcare outcomes, criminal justice, and many more. In this course, we‚Äôll explore several common machine learning algorithms, learning how they work, and applying them to real datasets. We will cover the strengths and limitations of machine learning algorithms. We will also explore real-world applications of machine learning, and discuss the ethical and societal consequences of the use of these algorithms."
  },
  {
    "objectID": "course_information/syllabus.html#course-goals",
    "href": "course_information/syllabus.html#course-goals",
    "title": "Syllabus",
    "section": "",
    "text": "Through the lens of machine learning, we will:  - develop and interpret applications of algorithms to domain use-cases - develop working software that satisfies coding best practices - work effectively, both individually and in teams - communicate information effectively to both technical and non-technical audiences\nCourse Schedule: The schedule for this course is available here. This document is likely to be updated throughout the semester, and will be announced in class if it is.\nActive Learning and Engagement: I expect you to take an active role in class each day, participating positively, wholeheartedly, and respectfully in class/group discussions and other activities. You may need sick/mental health days. You are responsible for learning the material on the day you missed. If you are experiencing issues causing you to miss classes often, please speak with me as this may be evident of a possible failing grade. You can also visit your class dean as they are a great resource."
  },
  {
    "objectID": "course_information/syllabus.html#readings-and-reflections",
    "href": "course_information/syllabus.html#readings-and-reflections",
    "title": "Syllabus",
    "section": "Readings and Reflections",
    "text": "Readings and Reflections\n(3pt * 10 ‚Üí 30 pt): Weekly reading assignments are due at 11:59pm (midnight) on the days indicated in the course schedule (typically Sundays). These readings consist of textbook passages from Introduction to Statistical Learning (ISL-P), as well as articles on ethics and/or domain use-cases. The reflections will be posted on Moodle and are graded based off completion. These will be used to gauge the difficulty of the material and guide certain aspects of the lecture. Late work: Submissions submitted after the deadline will receive no credit."
  },
  {
    "objectID": "course_information/syllabus.html#homework-assignments",
    "href": "course_information/syllabus.html#homework-assignments",
    "title": "Syllabus",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n(5pt * 9 ‚Üí 45 pt): Weekly homework assignments are due at the start of class on the days indicated in the course schedule (typically Mondays). These homework assignments consist of exercises to complete by hand (rather than programming), to reinforce your understanding of the concepts. Collaboration on homework is encouraged, and can be beneficial. However, remember that simply sharing answers is considered academic dishonesty, and is counterproductive, as it will not help you learn the material. Late work: Submissions one day late will receive a minus 1 point late penalty; submissions between one day and one week late will receive a minus 3 point late penalty. Submissions more than one week late will receive no credit."
  },
  {
    "objectID": "course_information/syllabus.html#lab-assignments",
    "href": "course_information/syllabus.html#lab-assignments",
    "title": "Syllabus",
    "section": "Lab Assignments",
    "text": "Lab Assignments\n(5pt * 7 ‚Üí 35 pt): We will have labs associated with every module. These are due before we take the corresponding module quiz. See the course schedule for more details. These assignments consist of programming exercises to practice applying the techniques we cover. Typically, these assignments will be completed by filling in missing code into Jupyter notebooks using Google Colab, and then submitting your completed notebook. Each exercise will be graded all-or-nothing, with no partial credit. Late work: Submissions one day late will receive a minus 1 point late penalty; submissions between one day and one week late will receive a minus 3 point late penalty. Submissions more than one week late will receive no credit."
  },
  {
    "objectID": "course_information/syllabus.html#semester-project",
    "href": "course_information/syllabus.html#semester-project",
    "title": "Syllabus",
    "section": "Semester Project",
    "text": "Semester Project\n(60 pts): Towards the end of the course, you will complete a final project where you apply the concepts covered in the course to a topic of your choice. The final project may be completed individually or in a group of at most three students. It will be divided into the following components (a more detailed rubric will be available later on in the semester):  ‚Ä¢ Plan, Implementation, and Submission. You will submit a plan for your project using the machine learning workflow, and implement the workflow using coding best practices. Your final code workbook will be submitted to Moodle prior to your presentation day. ‚Ä¢ Peer Evaluations and Workdays. We will have several project workdays in class where you will be asked to critique group projects in small groups, as well as give constructive feedback of all projects during our two presentation days. After you present your own project, you will complete a self reflection to assess what you did well, what you could improve upon, and the most important learning you discovered along the way. ‚Ä¢ Presentation Day. You will give a five-minute presentation on your project to the class. Late Work: Late penalties for each component will be included in the instructions for that component. Late submissions for some components may not receive credit."
  },
  {
    "objectID": "course_information/syllabus.html#standards-based-quizzes",
    "href": "course_information/syllabus.html#standards-based-quizzes",
    "title": "Syllabus",
    "section": "Standards based Quizzes",
    "text": "Standards based Quizzes\n(8pt * 9 ‚Üí 72 pt): There will be five (5) quizzes or mini-exams over the course of the semester that will be assessed using standards-based grading. Each quiz will contain between 2-4 standards, covering in total 16 standards. You will be given three attempts to pass each standard. The first attempt will be during a quiz as indicated in the course schedule. There will be two retake days for any second attempts. All final attempts for standards will take place during our final exam time. The standards are graded on an 8-point scale and binned into four categories: P - proficient (8) R - small revision needed S - not satisfactory (4) I - incomplete (0) Revisions must be handed in during class on the days indicated on the schedule. Standards with an R that hand in a fully correct solution will be bumped up to a P. Otherwise it will get bumped down to an S."
  },
  {
    "objectID": "course_information/syllabus.html#extra-credit",
    "href": "course_information/syllabus.html#extra-credit",
    "title": "Syllabus",
    "section": "Extra Credit",
    "text": "Extra Credit\nYou can earn 2pt of extra credit for every CS-related event you attend (up to three events total). You must fill out the EC form on Moodle to earn credit for the event. An additional 1pt will be added to any selfie taken during the event with either the presenter, poster, or other student(s) attending (you must get permission of all folks in the selfie beforehand). ## Final Exam The final exam is during the assigned period: Friday, Jan 30 from 1:00 - 3:00 P.M. Any final attempt on standards will take place during our final time. If you successfully passed each standard, you do not have to attend the final exam. Any student with S or I for three or more standards will be ineligible for a final grade greater than a B- in the course.\n\nFinal Grade Scale\n97 ‚â§ A+ ‚â§ 100 87 ‚â§ B+ ‚â§ 90 77 ‚â§ C+ ‚â§ 80 67 ‚â§ D+ ‚â§ 70 93 ‚â§ A ‚â§ 97 83 ‚â§ B ‚â§ 87 73 ‚â§ C ‚â§ 77 63 ‚â§ D ‚â§ 67 90 ‚â§ A- ‚â§ 93 80 ‚â§ B- ‚â§ 83 70 ‚â§ C- ‚â§ 73 60 ‚â§ D- ‚â§ 63\nExplanations of each letter grade range can be found at https://catalog.stolaf.edu/academic-regulations-procedures/grades/"
  },
  {
    "objectID": "course_information/syllabus.html#syllabus-changes",
    "href": "course_information/syllabus.html#syllabus-changes",
    "title": "Syllabus",
    "section": "Syllabus Changes",
    "text": "Syllabus Changes\nAll course information provided in this syllabus is subject to change and/or elimination. Changes will only be made when the instructor feels they are in the best interest of the class. It is the responsibility of every student to attend class, check e-mail, and communicate with the instructor to be informed and understand these changes. Most importantly HAVE FUN ‚Äì I am really looking forward to this semester!"
  },
  {
    "objectID": "course_information/syllabus.html#workflow",
    "href": "course_information/syllabus.html#workflow",
    "title": "Syllabus",
    "section": "Workflow",
    "text": "Workflow\ndefinition and examples of a broad variety of machine learning tasks: supervised learning (classification,regression), unsupervised learning (clustering); ability to identify, construct, and critique ML workflows."
  },
  {
    "objectID": "course_information/syllabus.html#eda",
    "href": "course_information/syllabus.html#eda",
    "title": "Syllabus",
    "section": "EDA",
    "text": "EDA\nunderstanding of statistical measures, distributions, and tests; exploration of data to guide workflow design and model selection."
  },
  {
    "objectID": "course_information/syllabus.html#data",
    "href": "course_information/syllabus.html#data",
    "title": "Syllabus",
    "section": "Data",
    "text": "Data\ndata preprocessing (importance and pitfalls); handling missing values (imputing, flag-as-missing, implications); encoding categorical variables and real-valued data; normalization and standardization."
  },
  {
    "objectID": "course_information/syllabus.html#selection",
    "href": "course_information/syllabus.html#selection",
    "title": "Syllabus",
    "section": "Selection",
    "text": "Selection\nimportance of understanding what your model is actually doing, where its pitfalls or shortcomings are, and the implications of its decisions; no free lunch (no one learner can solve all problems); representational design decisions have consequences; sources of error and undecidability in machine learning."
  },
  {
    "objectID": "course_information/syllabus.html#algorithms",
    "href": "course_information/syllabus.html#algorithms",
    "title": "Syllabus",
    "section": "Algorithms",
    "text": "Algorithms\nfundamentals of understanding how common ML algorithms work (including but not limited to: clustering, tree-based, regression-based, ensemble, neural nets, deep learning); ability to explain algorithms and their associated terms to a non-technical audience (including but not limited to: objective function, gradient descent, regularization, entropy)"
  },
  {
    "objectID": "course_information/syllabus.html#training",
    "href": "course_information/syllabus.html#training",
    "title": "Syllabus",
    "section": "Training",
    "text": "Training\nseparation of train, validation, and test sets; tuning the parameters of a machine learning model with a validation set; cross validation; overfitting problem / controlling solution complexity (regularization, pruning ‚Äì intuition only); the bias(underfitting) - variance (overfitting) tradeoff."
  },
  {
    "objectID": "course_information/syllabus.html#evaluation",
    "href": "course_information/syllabus.html#evaluation",
    "title": "Syllabus",
    "section": "Evaluation",
    "text": "Evaluation\nperformance metrics for classifiers; estimation of test performance on held-out data; other metrics for classification (e.g., error, precision, recall); performance metrics for regressors; confusion matrix;"
  },
  {
    "objectID": "course_information/syllabus.html#ethics",
    "href": "course_information/syllabus.html#ethics",
    "title": "Syllabus",
    "section": "Ethics",
    "text": "Ethics\nfocus on real data, scenarios, and case studies; bias present in datasets, algorithms, evaluation; privacy; fairness; ethical matrix"
  },
  {
    "objectID": "course_information/syllabus.html#inclusivity-and-community",
    "href": "course_information/syllabus.html#inclusivity-and-community",
    "title": "Syllabus",
    "section": "Inclusivity and Community",
    "text": "Inclusivity and Community\nIn keeping with St.¬†Olaf College‚Äôs mission statement, this class strives to be an inclusive and antiracist learning community, respecting and promoting those of differing backgrounds and beliefs. As a community, we will be to be respectful to all citizens in this class, regardless of race, ethnicity, religion, gender, or sexual orientation. This course affirms people of all gender expressions and gender identities. If you prefer to be called a different name or pronoun than what is on the class roster, please let me know."
  },
  {
    "objectID": "course_information/syllabus.html#illness-and-community-standards",
    "href": "course_information/syllabus.html#illness-and-community-standards",
    "title": "Syllabus",
    "section": "Illness and Community Standards",
    "text": "Illness and Community Standards\nOut of respect for our learning community, if you are experiencing any symptoms of an illness, do not come to class or my office. Please contact me before class time to let me know of your absence. ## Cell Phone Policy and Classroom Atmosphere You are expected to contribute to a positive classroom atmosphere, which includes arriving on time, not being disruptive, being respectful, actively involving yourself in class, and silencing and putting away cell phones. There may come a time where we use technology in class ‚Äì during this time you can use a laptop, tablet, or your phone if you do not have the other options available to you. At any other times, however, I do not want to see your phone out. I‚Äôll ask you to put them away if they show up. ## Mental and Physical Health I greatly value your experience in this class, and it is my duty to facilitate a safe, caring, and productive learning environment. I recognize that you may experience a range of emotional, physical, and/or psychological issues, both in and out of the classroom, that may distract you from your learning. If you are experiencing such issues, please do not hesitate to see me‚Äì I am here to listen. We can also discuss what further resources might be available to you. ## Academic Accommodations I am committed to supporting the learning of all students in my class. If you have already registered with Disability and Access (DAC) and have your letter of accommodations, please meet with me as soon as possible to discuss, plan, and implement your accommodations in the course. If you have or think you have a disability (learning, sensory, physical, chronic health, mental health or attentional), please contact Disability and Access staff at 507-786-3288 or by visiting https://wp.stolaf.edu/academic-support/dac. ## Multilingual Students I am committed to making course content accessible to all students. If English is not your first language and this causes you concern about the course, please speak with me. Students who would like extra support with writing or speaking in English can also contact the language specialist (berryag@stolaf.edu) in CAAS. ## Managing stress, anxiety, and other issues I greatly value your experience in this class, and it is my duty to facilitate a safe, caring, and productive learning environment. I recognize that you may experience a range of emotional, physical, and/or psychological issues, both in and out of the classroom, that may distract you from your learning. If you are experiencing such issues, please do not hesitate to come see me ‚Äì I am here to listen. We can also discuss what further resources might be available to you. ## Plagiarism & Academic Integrity Plagiarism, the unacknowledged appropriation of another person‚Äôs words or ideas, is a serious academic offense. It is imperative that you hand in work that is your own, and that cites or gives credit to others whenever you draw from their work. This includes citing prompts when using genAI tools or an internet search. Please see St.Olaf‚Äôs statements on academic integrity and plagiarism at: https://wp.stolaf.edu/thebook/academic/integrity/. See also the description of St.Olaf‚Äôs honor system at: https://wp.stolaf.edu/honorcouncil/. ## Communication Expectations: ‚Ä¢ Instructor/Student Expectations: I check my email frequently, and attempt to respond to questions within 24 hours. It should not be expected that I check my email after 9 PM, on Saturdays, or regularly during breaks. ‚Ä¢ E-mail Etiquette: If you have multiple questions, fewer emails with more information per email is preferred. Please include the following when sending an email: course number with section number, a description of your question(s) within the subject line, a greeting and your instructors preferred name (mine is Kim), and a sign-off using your preferred name (this is what you want others to refer to you in class by). Add screenshots of your work for clarity. ## The St.¬†Olaf Honor System The St.¬†Olaf Honor System has been in place at St.¬†Olaf College since 1911. All tests, quizzes, and examinations of any kind are taken under the St.¬†Olaf Honor System. Each student is responsible for adhering to all principles of the Honor System regardless of the individual circumstances associated with their assessment environment."
  },
  {
    "objectID": "01-EDA/rr-EDA.html",
    "href": "01-EDA/rr-EDA.html",
    "title": "EDA readings",
    "section": "",
    "text": "EDA readings\nRead through the following articles:\nStats distributions describes how by only knowing statistical properties we may be making inaccurate assumptions of what our data actually looks like visually.\nImplications behind visualizations (excusing the explicit language) has loads of examples of how biases can creep into our data insights. This article shows how we can initially interpret visualizations, and perhaps how to start questioning if the visualization is telling the story we think it is.\nUsing LLMs to storyboard goes through the process of how a data-driven story is created at The Pudding, but using an LLM. This article is a great way to understand how data-driven stories are put together, and how good (or not so good) LLMs are at replicating tasks associated with the design and implementation process.\nggplot vs plotnine compares the graphing power of R‚Äôs ggplot to Python‚Äôs plotnine (R wrapper for ggplot). Useful for those who have experience with creating visualizations in R, or for those who are unimpressed by matplotlib‚Äôs capabilities. Only read if you have a background using R. The following cheatsheets may be helpful for your reference: data storytelling, data visualizations, matplotlib, plotly, seaborn."
  },
  {
    "objectID": "01-EDA/hw-EDA.html",
    "href": "01-EDA/hw-EDA.html",
    "title": "Homework 1: Exploring Data",
    "section": "",
    "text": "Due: start of class on the day indicated on the syllabus  Complete the following exercises. Remember to explain your answers.\n\n\n\nFor this exercise, you will use the titanic dataset \\(\\href{https://www.kaggle.com/competitions/titanic}{(see:https://www.kaggle.com/competitions/titanic)}\\). You may have to sign-in using your Google account to view the data. Use the train.csv dataset on the Data tab to inform your answers below. You can also find the csv here\n\nGive an example of a record. What does it represent? \nSuppose you are using this dataset to train a model that will use a passenger class, sex, age, and fare to predict whether or not they will survive. \n\nIs this a classification task or a regression task? Explain your answer. \nWhat are the features? Explain your answer. \nWhat are the targets? Explain your answer. \nWe call a model a null model if it predicts all observations (regardless of any features selected) as the target variable. For this dataset, the null model is a model that would label all records in our data as survived. What would be the accuracy of this null model‚Äôs predictions? \n\nWould you expect a model using passenger class, sex, age, and fare to predict whether or not a passenger would survive to give a better accuracy than the null model? Explain why or why not. \n\n\n\n\n\n\nIn your own (non-technical) words, explain what each of the following summary statistics measures: \n\nmean:\nmedian:\nstandard deviation:\nmode:\n\n\n\n\n\n\nYou are given the following sets of five numbers, each with a mean and median of 3. \\[\n\\text{SetA: } \\{1,2,3,4,5\\} \\qquad\n\\text{SetB: } \\{3,3,3,3,3\\} \\qquad\n\\text{SetC: } \\{1,1,3,5,5\\}\n\\] For each of the examples below, give a brief explanation for each of the choices made.\n\nWhat is true about the sum of all numbers for each set? Which measure tells us why this is occurring? \n\nOrder the sets from smallest standard deviation to largest. Explain (non-technical, no math) why this is.\n\nCreate a set where the mean is 3, but the median is 4. Keep all numbers in the set between 0 and 5. \n\nDo part c again, but where the median is 0. Why must this set contain numbers higher than 5? \n\n\n\n\n\n\nFor this exercise, you will use the titanic dataset to understand the relationships between different variables and draw meaningful conclusions using statistical tests. Your task is to use the given information to understand what each test is doing. You will then fill in the missing information for selecting a feature pair for each statistical test, forming hypotheses, and interpreting the results.\n\nt-test\n\nFeatures used:\nWhat the test is doing: It‚Äôs checking if the average age of survivors differs significantly from the average age of non-survivors\nNull Hypothesis (\\(H_0\\)):\nAlternative Hypothesis (\\(H_1\\)):\nHow to interpret this test:\nWhat to do if test is significant: If the result is significant, you could explore further by examining the age distribution for each group (survivors vs.¬†non-survivors). You could create visualizations to check if the distribution is skewed or if there are specific age ranges that were more likely to survive.\n\nchi-square test\n\nFeatures used:\nWhat the test is doing:\nNull Hypothesis (\\(H_0\\)):\nAlternative Hypothesis (\\(H_1\\)): There is a significant association between passenger class and survival status\nHow to interpret this test:\nWhat to do if test is significant:\n\ncorrelation test\n\nFeatures used: age and fare\nWhat the test is doing:\nNull Hypothesis (\\(H_0\\)):\nAlternative Hypothesis (\\(H_1\\)):\nHow to interpret this test: If the p-value is small, you reject the null hypothesis and conclude that there is a significant linear relationship between age and fare. If the correlation coefficient is close to 1 or -1, the relationship is strong, while a coefficient near 0 indicates no significant relationship.\nWhat to do if test is significant:\n\n\n\n\n\n\n\nGive an example of a set of five numbers with a mean of 3. Demonstrate that your setof numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of five numbers with a mean of 3, and a very small standard deviation. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of five numbers with a mean of 3, and a very large standard deviation. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of five numbers with a mean of 3 and a median of 4. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of five numbers with a mean of 3 and a median of 0. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of 10 numbers with a mean of 5 and a symmetric distribution. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics and making a dot plot.\nGive an example of a set of 10 numbers with a mean of 5 and a left-skewed distribution.Demonstrate that your set of numbers has this property, by computing the relevantsummary statistics and making a dot plot.\nGive an example of a set of 10 numbers with a mean of 5 and a right-skewed distribution. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics and making a dot plot."
  },
  {
    "objectID": "01-EDA/hw-EDA.html#looking-at-data",
    "href": "01-EDA/hw-EDA.html#looking-at-data",
    "title": "Homework 1: Exploring Data",
    "section": "",
    "text": "For this exercise, you will use the titanic dataset \\(\\href{https://www.kaggle.com/competitions/titanic}{(see:https://www.kaggle.com/competitions/titanic)}\\). You may have to sign-in using your Google account to view the data. Use the train.csv dataset on the Data tab to inform your answers below. You can also find the csv here\n\nGive an example of a record. What does it represent? \nSuppose you are using this dataset to train a model that will use a passenger class, sex, age, and fare to predict whether or not they will survive. \n\nIs this a classification task or a regression task? Explain your answer. \nWhat are the features? Explain your answer. \nWhat are the targets? Explain your answer. \nWe call a model a null model if it predicts all observations (regardless of any features selected) as the target variable. For this dataset, the null model is a model that would label all records in our data as survived. What would be the accuracy of this null model‚Äôs predictions? \n\nWould you expect a model using passenger class, sex, age, and fare to predict whether or not a passenger would survive to give a better accuracy than the null model? Explain why or why not."
  },
  {
    "objectID": "01-EDA/hw-EDA.html#explaining-statistics",
    "href": "01-EDA/hw-EDA.html#explaining-statistics",
    "title": "Homework 1: Exploring Data",
    "section": "",
    "text": "In your own (non-technical) words, explain what each of the following summary statistics measures: \n\nmean:\nmedian:\nstandard deviation:\nmode:"
  },
  {
    "objectID": "01-EDA/hw-EDA.html#exploring-statistics",
    "href": "01-EDA/hw-EDA.html#exploring-statistics",
    "title": "Homework 1: Exploring Data",
    "section": "",
    "text": "You are given the following sets of five numbers, each with a mean and median of 3. \\[\n\\text{SetA: } \\{1,2,3,4,5\\} \\qquad\n\\text{SetB: } \\{3,3,3,3,3\\} \\qquad\n\\text{SetC: } \\{1,1,3,5,5\\}\n\\] For each of the examples below, give a brief explanation for each of the choices made.\n\nWhat is true about the sum of all numbers for each set? Which measure tells us why this is occurring? \n\nOrder the sets from smallest standard deviation to largest. Explain (non-technical, no math) why this is.\n\nCreate a set where the mean is 3, but the median is 4. Keep all numbers in the set between 0 and 5. \n\nDo part c again, but where the median is 0. Why must this set contain numbers higher than 5?"
  },
  {
    "objectID": "01-EDA/hw-EDA.html#interpretting-statistical-tests",
    "href": "01-EDA/hw-EDA.html#interpretting-statistical-tests",
    "title": "Homework 1: Exploring Data",
    "section": "",
    "text": "For this exercise, you will use the titanic dataset to understand the relationships between different variables and draw meaningful conclusions using statistical tests. Your task is to use the given information to understand what each test is doing. You will then fill in the missing information for selecting a feature pair for each statistical test, forming hypotheses, and interpreting the results.\n\nt-test\n\nFeatures used:\nWhat the test is doing: It‚Äôs checking if the average age of survivors differs significantly from the average age of non-survivors\nNull Hypothesis (\\(H_0\\)):\nAlternative Hypothesis (\\(H_1\\)):\nHow to interpret this test:\nWhat to do if test is significant: If the result is significant, you could explore further by examining the age distribution for each group (survivors vs.¬†non-survivors). You could create visualizations to check if the distribution is skewed or if there are specific age ranges that were more likely to survive.\n\nchi-square test\n\nFeatures used:\nWhat the test is doing:\nNull Hypothesis (\\(H_0\\)):\nAlternative Hypothesis (\\(H_1\\)): There is a significant association between passenger class and survival status\nHow to interpret this test:\nWhat to do if test is significant:\n\ncorrelation test\n\nFeatures used: age and fare\nWhat the test is doing:\nNull Hypothesis (\\(H_0\\)):\nAlternative Hypothesis (\\(H_1\\)):\nHow to interpret this test: If the p-value is small, you reject the null hypothesis and conclude that there is a significant linear relationship between age and fare. If the correlation coefficient is close to 1 or -1, the relationship is strong, while a coefficient near 0 indicates no significant relationship.\nWhat to do if test is significant:"
  },
  {
    "objectID": "01-EDA/hw-EDA.html#additional-problems-not-assigned",
    "href": "01-EDA/hw-EDA.html#additional-problems-not-assigned",
    "title": "Homework 1: Exploring Data",
    "section": "",
    "text": "Give an example of a set of five numbers with a mean of 3. Demonstrate that your setof numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of five numbers with a mean of 3, and a very small standard deviation. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of five numbers with a mean of 3, and a very large standard deviation. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of five numbers with a mean of 3 and a median of 4. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of five numbers with a mean of 3 and a median of 0. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics.\nGive an example of a set of 10 numbers with a mean of 5 and a symmetric distribution. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics and making a dot plot.\nGive an example of a set of 10 numbers with a mean of 5 and a left-skewed distribution.Demonstrate that your set of numbers has this property, by computing the relevantsummary statistics and making a dot plot.\nGive an example of a set of 10 numbers with a mean of 5 and a right-skewed distribution. Demonstrate that your set of numbers has this property, by computing the relevant summary statistics and making a dot plot."
  },
  {
    "objectID": "03-CLS/rr-CLS.html",
    "href": "03-CLS/rr-CLS.html",
    "title": "CLS readings",
    "section": "",
    "text": "CLS readings\nISL-P: Unsupervised Learning section 12.1; PCA section 12.2; Clustering Models section 12.3\nRead the following articles: Decision making with AI (few years old) gives real-world examples of the ethical dilemmas of ML\n[Recommender system tutorial] (https://www.alldatascience.com/recommender-systems/simple-recipe-recommender-system-with-scikit-surprise/)(won‚Äôt cover in class but interesting unsupervised learning model) which you can get a better idea of how it works by writing out the 5-step ML workflow and fill in the steps you see being taken in the tutorial."
  },
  {
    "objectID": "03-CLS/lab-CLS.html",
    "href": "03-CLS/lab-CLS.html",
    "title": "Task 0: Data Exploration",
    "section": "",
    "text": "Name:\nWho you worked with:"
  },
  {
    "objectID": "03-CLS/lab-CLS.html#objectives",
    "href": "03-CLS/lab-CLS.html#objectives",
    "title": "Task 0: Data Exploration",
    "section": "Objectives",
    "text": "Objectives\nThe goals of this project are to: - Perform EDA, PCA, and visualize the data - Implement K-means clustering - Evaluate the clustering results against the true labels - Thoughtfully interpret and discuss the results"
  },
  {
    "objectID": "03-CLS/lab-CLS.html#overview",
    "href": "03-CLS/lab-CLS.html#overview",
    "title": "Task 0: Data Exploration",
    "section": "Overview",
    "text": "Overview\nIn this assignment, you will explore the Fashion MNIST dataset, which contains grayscale images of 10 different clothing items. You will focus on applying unsupervised learning techniques, specifically K-means clustering, to see if the algorithm can naturally identify patterns that correspond to different clothing categories. You will also critically evaluate the performance of clustering against the ground truth labels and reflect on the limitations of the algorithm."
  },
  {
    "objectID": "03-CLS/lab-CLS.html#schedule",
    "href": "03-CLS/lab-CLS.html#schedule",
    "title": "Task 0: Data Exploration",
    "section": "Schedule",
    "text": "Schedule\nHere is the suggested schedule for working on this project: - Weekend: Read through project instructions, complete Task 0. - Tuesday: Complete Tasks 1-2. - Wednesday: Complete Tasks 3-4. - Thursday: Complete Task 5.\nThis project is due on Thursday, 3/6, by 11:59pm."
  },
  {
    "objectID": "03-CLS/lab-CLS.html#explore-the-data",
    "href": "03-CLS/lab-CLS.html#explore-the-data",
    "title": "Task 0: Data Exploration",
    "section": "Explore the data",
    "text": "Explore the data\nLet‚Äôs explore the format of the dataset before training the model. The following shows there are 60,000 images, with each image represented as 28 x 28 pixels:\n\nX.shape\n\nLikewise, there are 60,000 labels:\n\nlen(y)\n\n###üíª Question1: Is The Data Balanced?\nLet‚Äôs take a look at how many of each type of clothing article we have.\nBelow is the dataframe of our target.\nAdd a new line of code that takes y_df and finds the counts of each class.\nIs our dataset balanced?\n\ny_df = pd.DataFrame(y)\n#add code here\n\n###‚úè Question2: Image Flattening\nOur images are flattened from a 28√ó28 image to 784 features. What spatial information might be lost in this process? How could this impact our clustering results?\n##PCA\nIt would take a very long time to generate pairplots for 700+ features. Instead, we‚Äôll use principal component analysis (PCA) for dimensionality reduction, so that we can visualize a projection of the data. Here, we reduce the data to a few dimensions.\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=9)\npca.fit(X)\nprojection = pca.transform(X)\nprojection_df = pd.DataFrame(projection)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure()\nsns.pairplot(projection_df)\nplt.show()\n\n###‚úè Question3: PCA plots How do the different shapes of these plots (such as nugget-shaped, normal, or multi-modal) help us understand the characteristics of clothing categories? For example, do they show variations in style within a category, or do they highlight differences between categories?"
  },
  {
    "objectID": "03-CLS/lab-CLS.html#task-1-closest-centroid",
    "href": "03-CLS/lab-CLS.html#task-1-closest-centroid",
    "title": "Task 0: Data Exploration",
    "section": "Task 1: Closest Centroid",
    "text": "Task 1: Closest Centroid\nFirst, you‚Äôll implement a function that takes an array of data points and an array of centroids, and returns an array giving the index of the closest centroid to each data point. Note that the shapes should be:\n\ndata has shape \\((N, D)\\), where \\(N\\) is the number of datapoints and \\(D\\) is the dimensionality of each datapoint.\ncentroids has shape \\((k, D)\\), where \\(k\\) is the number of centroids, and \\(D\\) is the dimensionality of each datapoint.\nclosest_centroids (the return value) has shape \\((N,)\\), where \\(N\\) is the number of datapoints.\n\nThe code below has been outlined in such a way that only basic programming logic needs to be used. You may find that using numpy methods helpful when calculating the Euclidean.\nHint: since we have high-dimensional data you can use np.sum((point - centroid) ** 2) to find the sum of squared differences when you do the distance calculation.\n\nimport numpy as np\n\ndef closest_centroid(data, centroids):\n    # init to store the closest centroid index for each data point\n    closest_centroids = []\n\n    # looping through each data point\n    for point in data:\n        min_distance = float('inf')  #init min distance to huge number like infinity\n        closest_centroid_index = -1\n\n        # counter for centroid index\n        i = 0\n\n        # looping through each centroid to compute the distance to the data point\n\n            # calculate the Euclidean distance\n\n\n            # if the distance is smaller than the minimum (min_distance) found so far, update the closest centroid (closest_centroid_index)\n\n\n            # increment the counter by 1\n\n        # appending the index of the closest centroid to the result list\n        closest_centroids.append(closest_centroid_index)\n\n    # return the list of closest centroid indices\n    return np.array(closest_centroids)\n\nThe code chunks below will test your functions. If you run both and no errors occur, your function works as expected.\n\n# testing your function\ndata = np.array([[-2,2], [-1, 2], [-1,1], [1,1], [1,2], [2,2]])\ncentroids = np.array([[-3,3], [3,3]])\nassert(np.array_equal(closest_centroid(data, centroids), np.array([0,0,0,1,1,1])))\n\n\ndata = train_data[:10]  # First 10 images\ncentroids = train_data[121:123]  # Images at index 121 and 123 as centroids\nclosest_centroid(data, centroids)\n#assert(np.array_equal(closest_centroid(data, centroids), np.array([1, 1, 0, 0, 0, 1, 0, 1, 0, 0])))\n\n###‚úè Question4: Dimensionality and Distances\nIn high-dimensional spaces like our 784-dimensional images, how does distance calculation become problematic? (This is related to what we call the ‚Äúcurse of dimensionality‚Äù)\n###‚úè Question5: Computational Costs\nIf we apply this function to all 60,000 images, it will be computationally expensive. How might you modify the approach to make it more efficient for large datasets? For example, we may consider only a subset of the data rather than all 60k images.\n###‚úè Question6: Dealing with Ties\nWhat would happen if two centroids were equally distant from a data point? How does your function handle this case, and is this approach appropriate?"
  },
  {
    "objectID": "03-CLS/lab-CLS.html#task-2-recompute-centroids",
    "href": "03-CLS/lab-CLS.html#task-2-recompute-centroids",
    "title": "Task 0: Data Exploration",
    "section": "Task 2: Recompute Centroids",
    "text": "Task 2: Recompute Centroids\nNext, you‚Äôll define a function that recomputes centroids once each data point has been assigned to a cluster. This function takes an array of datapoints and an array giving the cluster assignments. The index of each centroid should correspond to its cluster number. Note that the shapes should be:\n\ndata has shape \\((N, D)\\), where \\(N\\) is the number of datapoints and \\(D\\) is the dimensionality of each datapoint.\nlabels has shape \\((N,)\\), where \\(N\\) is the number of datapoints.\ncentroids (the return value) has shape \\((k, D)\\), where \\(k\\) is the number of centroids, and \\(D\\) is the dimensionality of each datapoint.\n\n\nimport numpy as np\n\ndef compute_centroids(data, labels):\n    # getting the number of clusters\n    k = np.max(labels) + 1 #adding one since we start counting at 0 not 1\n\n    # init to store the new centroids\n    new_centroids = []\n\n    # looping through each cluster\n    for i in range(k):\n        # get the data points(data) assigned to cluster i (check if labels are equal to i)\n        cluster_points = #\n\n        # compute the mean (use np.mean) of the data points in the cluster (cluster_points)\n        new_centroid = #\n\n        # append the new centroid to our new centroid list\n        new_centroids.append(new_centroid)\n\n    # convert new_centroids into a numpy array and return\n    return #\n\nThe code chunks below will test your functions. If you run both and no errors occur, your function works as expected.\n\n# testing your function\ndata = np.array([[-2,2], [-1, 2], [-1,1], [1,1], [1,2], [2,2]])\nlabels = np.array([0,0,0,0,1,1])\nassert(np.array_equal(compute_centroids(data, labels), np.array([[-.75, 1.5], [1.5, 2]])))\n\n\ndata = train_data[:10]  # Take the first 10 images for testing\nlabels = np.array([1, 0, 1, 1, 1, 0, 1, 0, 1, 1])  # Example labels (clusters)\nassert(np.array_equal(compute_centroids(data, labels)[0][5:7], np.array([0.6666667 , 0.33333334], dtype='float32')))\n\n###‚úè Question7: Centroid as an Image\nA centroid is the mean of all points in a cluster. For image data, what does this ‚Äúaverage image‚Äù actually represent visually? Would it still look like a recognizable piece of clothing?\n###‚úè Question8: Mean vs Median\nThe mean minimizes the sum of squared distances to all points. What if we used the median instead? How might this change our clusters and when might this be beneficial?"
  },
  {
    "objectID": "03-CLS/lab-CLS.html#task-3-implement-k-means",
    "href": "03-CLS/lab-CLS.html#task-3-implement-k-means",
    "title": "Task 0: Data Exploration",
    "section": "Task 3: Implement \\(k\\)-Means",
    "text": "Task 3: Implement \\(k\\)-Means\nNow, that you‚Äôve seen how the various components of the algorithm are created, we‚Äôre going to switch gears and create a model.\nUsually when we have a ton of data, we have methods to make it easier on our algorithms (and machines) to create the clusters. For our purposes, we‚Äôre going to take only three of the types of clothing and do k-means on this subset of data. This will help us as we eventually want to also investigate the clusters to see what is going on.\nWith large data, that also means that the process of choosing the actual best k take a bit of time. Instead, we‚Äôre going to use some domain knowledge (we‚Äôre choosing 3 articles of clothing in our subset) and use that to pick our k value.\n\ndataset = np.column_stack((X, y))\narticles = [y_names.index(\"Coat\"), y_names.index(\"Bag\"), y_names.index(\"Sneaker\")]\nsubset = dataset[np.isin(dataset[:, -1], articles)]\nX_subset = subset[:, :-1]\ny_subset = subset[:,-1:]\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nkmeans = KMeans(n_clusters=3, random_state = 42)\nkmeans.fit(X_subset)\nsilhouette_score(X_subset, kmeans.labels_)\n\n###‚úè Question9: Subset Article Choices Our subset currently cointains Coat, Bag, and Sneaker. What if instead we had selected only footwear items: Sandal, Sneaker, Ankle boot? What clustering challenges might arise when items are similar? How might this affect our silhouette score compared to clustering with an unrelated subset?\n###‚úè Question10: Silhouette Score\nThe silhouette score measures how well-separated clusters are. What would a perfect silhouette score be, and what does our current score suggest about our clusters?"
  },
  {
    "objectID": "03-CLS/lab-CLS.html#finding-particular-images",
    "href": "03-CLS/lab-CLS.html#finding-particular-images",
    "title": "Task 0: Data Exploration",
    "section": "Finding particular images",
    "text": "Finding particular images\nYou can further invesitgate by using the function below find_pic which will take the class label (items from row_0 in the matrix above) and the cluster label (items from col_0 in the matrix above) and return the first image that meets that criteria.\n\ndef find_pic(article_type, cluster_label):\n  \"\"\"This function takes in y label (article_type) and the cluster it belongs\n  to (cluster_label) and will show that image\"\"\"\n  valid_indices = np.where((y_subset.flatten() == article_type) & (kmeans.labels_ == cluster_label))[0]\n  chosen_index = np.random.choice(valid_indices)\n  print(chosen_index)\n  plt.figure(figsize=(3, 3))\n  plt.imshow(X_subset[chosen_index].reshape(28, 28), cmap='gray')\n  plt.title(f'Label: {article_type}, Cluster: {cluster_label}')\n  plt.show()\n\n\nfind_pic(4,0)\n\n\nfind_pic(4,1)\n\n\nfind_pic(4,2)\n\n###‚úè Question11: Labels\nWhat does the row values 4, 7, 8 mean in the context of our data?\n###üíª Question12: Identifying Clothing\nIs the model generally grouping items according to their true class? Which clothing article type seems easiest for the algorithm to identify? Which is most confused? You can use the find_pic function above if that is helpful.\n###üíª Question13: Visual Inspection\nConsider the shapes of our articles of clothing. What visual features might cause the algorithm to group certain articles together despite having different labels? Use the function above to find at least two pieces of evidence in the data to support your claim. Add your code below.\n###‚úè Question14: Clusters\nFor each cluster: * which of the labels appear in the cluster? * is there a label that occurs significantly more frequently than the others?\nWhere does the algorithm have difficulty? Why do you think this is happening?\n#Reflection\nTake a moment to reflect on the assingment\n##‚úè Question 15: Reflection\nWhat did you like about it? What could be improved? Your answers will not affect your overall grade. This feedback will be used to improve future programming assignments.\n#Grading For each of the following accomplishments, there is a breakdown of points which total to 21. The fraction of points earned out of 21 will be multiplied by 5 to get your final score (e.g.¬†17 points earned will be 17/21 * 5 ‚Üí 4.05) * (1pt) Task0 q1: Identified counts and discussed if balanced. * (1pt) Task0 q2: Identified at least one concern of flattening images * (1pt) Task0 q3: Discussed distributions of features * (2pt) Task1: Function closest_centroid runs as expected * (1pt) Task1 q4: Correctly states why dimensions matter when calculating distances * (1pt) Task1 q5: At least one approach to save on computational costs is discussed. * (1pt) Task1 q6: Discusses ties and offers a solution * (2pt) Task2: Function recompute_centroid runs as expected * (1pt) Task2 q7: Correctly explains what ‚Äúaverage image‚Äù means * (1pt) Task2 q8: Discusses mean vs median appropriately * (1pt) Task3 q9: Discusses subsets and their influence on the accuracy of the model * (1pt) Task3 q10: Interprets the silhouette score * (1pt) Task4 q11: Indentifies what each label means in the context of the dataset * (2pt) Task4 q12: Interprets the confusion matrix for articles of clothing (rows) * (1pt) Task4 q13: Used the function find_pic to support the claims * (2pt) Task4 q14: Interprets the confusion matrix for clusters (columns) * (1pt) Task5 q15: You have reflected on the assignment"
  },
  {
    "objectID": "01-EDA/lab-EDA.html",
    "href": "01-EDA/lab-EDA.html",
    "title": "Task 2: Critique the analysis",
    "section": "",
    "text": "Name:\nWho you worked with:\n##Objectives The goals of this project are to: * Understand the process of exploratory data analysis (EDA), statistical tests, and visualizations. * Develop the ability to critique and improve code generated by a language model (like ChatGPT or DeepSeek). * Create two publication-style visualizations that tell a story.\n##Overview This project is designed to reinforce some of the concepts we have learned while exploring data. For Task 1, you are to generate Python code to analyze and summarize the Titanic dataset given a prompt for a genAI model. For Task 2, you will critique the generated program by identifying three areas of improvement. For Task 3, you will implement the improvements you identified. For Task 4, you will create two publication-style visualizations that tell a story. For Task 5, you will reflect on this assignment.\n##Schedule Here is the suggested schedule for working on this project: * By Thursday, 2/13, read through the project instructions. * By Sunday, 2/16, complete Tasks 1-2 of the project, and start Task 3 of the project. * By Tuesday, 2/18, complete Task 3 of the project, and start Task 4. * By Wednesday, 2/19, complete Tasks 4-5 and check your solutions against the rubric (included at the end of these instructions), and submit your files through moodle.\nThis project is due on Thursday, 2/20, by 11:59pm.\n#Task 1: Generate an analysis\nUse the following prompt in ChatGPT to generate Python code for analyzing the Titanic dataset:\nmodel used:"
  },
  {
    "objectID": "01-EDA/lab-EDA.html#part-1-generate-code",
    "href": "01-EDA/lab-EDA.html#part-1-generate-code",
    "title": "Task 2: Critique the analysis",
    "section": "Part 1: Generate Code",
    "text": "Part 1: Generate Code\nAdd the output to a code chunk below.\nIf your code does not initially run, you may want to comment some of it out and return to it during Task2."
  },
  {
    "objectID": "01-EDA/lab-EDA.html#part-2-outline-analysis",
    "href": "01-EDA/lab-EDA.html#part-2-outline-analysis",
    "title": "Task 2: Critique the analysis",
    "section": "Part 2: Outline Analysis",
    "text": "Part 2: Outline Analysis\nWith the program that was generated, fill in the methods taken for each of the points below (keep it general). Some examples have been filled in for you.\nOnce you have listed these out, add a quick explaination for why this step was done (i.e.¬†what did it accomplish? what do you know about the data now?)\nSteps taken below:\n\nBasic statistics:\n\n\nhead\ndescribe\nisnull + sum\n\n\nVisualizations:\n\n\nsns.boxplot\n\n\nStatistical tests:\n\n\nstats.ttest_ind"
  },
  {
    "objectID": "01-EDA/lab-EDA.html#part-1-questions-to-consider",
    "href": "01-EDA/lab-EDA.html#part-1-questions-to-consider",
    "title": "Task 2: Critique the analysis",
    "section": "Part 1: Questions to consider",
    "text": "Part 1: Questions to consider\nSome initial questions to consider are listed below. Add three more questions you may want to consider when looking over EDA, visualizations, and code in general.\nExample questions: * Is the choice of statistical test appropriate for the analysis? * Are there interpretations of statistical tests beyond p-value? * Are the visualizations clear, informative, and easy to interpret? * Are there any redundancies or areas where the code could be simplified?\nAdditional questions to consider:"
  },
  {
    "objectID": "01-EDA/lab-EDA.html#part-2-improvements",
    "href": "01-EDA/lab-EDA.html#part-2-improvements",
    "title": "Task 2: Critique the analysis",
    "section": "Part 2: Improvements",
    "text": "Part 2: Improvements\nUsing all the questions above, list out three improvements that you would like to implement.\nFor each improvement, describe when this would occur (e.g.¬†visualizations, statistical tests), why you would like to implement it, and what additional information is gained by doing so (i.e.¬†what was the purpose of doing it?)\nImprovements:"
  },
  {
    "objectID": "01-EDA/lab-EDA.html#part-1-updated-prompt",
    "href": "01-EDA/lab-EDA.html#part-1-updated-prompt",
    "title": "Task 2: Critique the analysis",
    "section": "Part 1: Updated Prompt",
    "text": "Part 1: Updated Prompt\nYou may find that you will have to iteratively update the prompt to get the exact output you‚Äôre looking for. Once you have a final prompt that implements all of the improvements, add it to the text box below"
  },
  {
    "objectID": "01-EDA/lab-EDA.html#part-2-updated-code",
    "href": "01-EDA/lab-EDA.html#part-2-updated-code",
    "title": "Task 2: Critique the analysis",
    "section": "Part 2: Updated Code",
    "text": "Part 2: Updated Code\nNow add the outputted code generated from the updated prompt below."
  },
  {
    "objectID": "01-EDA/lab-EDA.html#part-1-viz-1",
    "href": "01-EDA/lab-EDA.html#part-1-viz-1",
    "title": "Task 2: Critique the analysis",
    "section": "Part 1: Viz 1",
    "text": "Part 1: Viz 1"
  },
  {
    "objectID": "01-EDA/lab-EDA.html#part-2-viz-2",
    "href": "01-EDA/lab-EDA.html#part-2-viz-2",
    "title": "Task 2: Critique the analysis",
    "section": "Part 2: Viz 2",
    "text": "Part 2: Viz 2"
  },
  {
    "objectID": "01-EDA/EDA1.html#this-week",
    "href": "01-EDA/EDA1.html#this-week",
    "title": "EDA1: Exploring Data",
    "section": "This Week",
    "text": "This Week"
  },
  {
    "objectID": "01-EDA/EDA1.html#warm-up-describe-this-data",
    "href": "01-EDA/EDA1.html#warm-up-describe-this-data",
    "title": "EDA1: Exploring Data",
    "section": "Warm-up: Describe this Data",
    "text": "Warm-up: Describe this Data\nImagine you‚Äôve just been this random sample of a dataset:\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n205\nChinstrap\nDream\n50.7\n19.7\n203.0\n4050.0\nMale\n\n\n42\nAdelie\nDream\n36.0\n18.5\n186.0\n3100.0\nFemale\n\n\n127\nAdelie\nTorgersen\n41.5\n18.3\n195.0\n4300.0\nMale\n\n\n33\nAdelie\nDream\n40.9\n18.9\n184.0\n3900.0\nMale\n\n\n339\nGentoo\nBiscoe\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n253\nGentoo\nBiscoe\n59.6\n17.0\n230.0\n6050.0\nMale\n\n\n\n\n\n\n\nWe would like to unpack the stories that this data might tell. How many different ways can we describe or summarize this data‚Äîboth its overall shape and its finer details?"
  },
  {
    "objectID": "01-EDA/std-EDA.html",
    "href": "01-EDA/std-EDA.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Practice questions around EDA topic."
  },
  {
    "objectID": "course_information/schedule.html",
    "href": "course_information/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "dow\ndate\nprepare\ntopic\nmaterials\n\n\n\n\nM\nJan 5\nüìñsyllabus\nINTRO\nüë©üèª‚Äçüè´ INTRO üíª Lab: Intro ‚òëÔ∏è PC0\n\n\nT\nJan 6\nüìñreadings\nEDA\nüë©üèª‚Äçüè´ EDA1 EDA2 üìù HW:EDA\n\n\nW\nJan 7\nüììEDA study guide\nEDA\nüë©üèª‚Äçüè´ EDA3 üíª Lab:EDA ‚òëÔ∏è PC1\n\n\nTh\nJan 8\nüìñreadings\nEDA Quiz  EVL\nüë©üèª‚Äçüè´ EVL1 EVL2 üìù HW:EVL\n\n\nF\nJan 9\nüììEDA study guide\nEVL\nüë©üèª‚Äçüè´ EVL3 üíª Lab:EVL ‚òëÔ∏è PC2",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "course_information/schedule.html#week-1-jan-5---9",
    "href": "course_information/schedule.html#week-1-jan-5---9",
    "title": "Schedule",
    "section": "",
    "text": "dow\ndate\nprepare\ntopic\nmaterials\n\n\n\n\nM\nJan 5\nüìñsyllabus\nINTRO\nüë©üèª‚Äçüè´ INTRO üíª Lab: Intro ‚òëÔ∏è PC0\n\n\nT\nJan 6\nüìñreadings\nEDA\nüë©üèª‚Äçüè´ EDA1 EDA2 üìù HW:EDA\n\n\nW\nJan 7\nüììEDA study guide\nEDA\nüë©üèª‚Äçüè´ EDA3 üíª Lab:EDA ‚òëÔ∏è PC1\n\n\nTh\nJan 8\nüìñreadings\nEDA Quiz  EVL\nüë©üèª‚Äçüè´ EVL1 EVL2 üìù HW:EVL\n\n\nF\nJan 9\nüììEDA study guide\nEVL\nüë©üèª‚Äçüè´ EVL3 üíª Lab:EVL ‚òëÔ∏è PC2",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "course_information/schedule.html#week-2-jan-12---16",
    "href": "course_information/schedule.html#week-2-jan-12---16",
    "title": "Schedule",
    "section": "Week 2 (Jan 12 - 16)",
    "text": "Week 2 (Jan 12 - 16)\n\n\n\ndow\ndate\nprepare\ntopic\nmaterials\n\n\n\n\nM\nJan 12\nüìñreadings\nEVL Quiz  CLS\nüë©üèª‚Äçüè´ CLS1 CLS2 üìù HW:CLS\n\n\nT\nJan 13\nüììCLS study guide\nCLS  Quiz Retakes\nüë©üèª‚Äçüè´ CLS3 üíª Lab:CLS ‚òëÔ∏è PC3\n\n\nW\nJan 14\nüìñreadings\nCLS Quiz  DATA\nüë©üèª‚Äçüè´ DATA1 DATA2 üìù HW:DATA ‚òëÔ∏è PC4\n\n\nTh\nJan 15\nüììDATA study guide\nDATA\nüë©üèª‚Äçüè´ DATA3 üíª Lab:DATA ‚òëÔ∏è PC4\n\n\nF\nJan 16\nüìñreadings\nDATA Quiz  MOD\nüë©üèª‚Äçüè´ TREE1 TREE2 üìù HW:TREE",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "course_information/schedule.html#week-3-jan-19---23",
    "href": "course_information/schedule.html#week-3-jan-19---23",
    "title": "Schedule",
    "section": "Week 3 (Jan 19 - 23)",
    "text": "Week 3 (Jan 19 - 23)\n\n\n\ndow\ndate\nprepare\ntopic\nmaterials\n\n\n\n\nM\nJan 19\n\nMOD  Quiz Retakes\nüë©üèª‚Äçüè´ KNN üíª Lab:TREE ‚òëÔ∏è PC5\n\n\nT\nJan 20\n\nTREE Quiz  MOD\nüë©üèª‚Äçüè´ LIN1 LIN2 üìù HW:LIN\n\n\nW\nJan 21\n\nMOD\nüë©üèª‚Äçüè´ SVM  üíª Lab:MOD Part 1 ‚òëÔ∏è PC6\n\n\nTh\nJan 22\n\nLIN Quiz  ENS\nüë©üèª‚Äçüè´ ENS1 ENS2 üìù HW:ENS\n\n\nF\nJan 23\n\nENS  Quiz Retakes\nüë©üèª‚Äçüè´ ENS3 üíª Lab:MOD Part 2 ‚òëÔ∏è PC7",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "course_information/schedule.html#week-4-jan-26---30",
    "href": "course_information/schedule.html#week-4-jan-26---30",
    "title": "Schedule",
    "section": "Week 4 (Jan 26 - 30)",
    "text": "Week 4 (Jan 26 - 30)\n\n\n\ndow\ndate\nprepare\ntopic\nmaterials\n\n\n\n\nM\nJan 26\n\nENS Quiz  NN\nüë©üèª‚Äçüè´ NN1 NN2 üìù HW:NN\n\n\nT\nJan 27\n\nNN\nüë©üèª‚Äçüè´ NN3 üíª Lab:NN ‚òëÔ∏è PC8\n\n\nW\nJan 28\n\nNN Quiz  DL\nüë©üèª‚Äçüè´ DL1 üíª DL2\n\n\nTh\nJan 29\n\nDL Quiz  Last Day\n‚òëÔ∏è PC9\n\n\nF\nJan 30\n\nPresentations  Quiz Retakes",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Semester Project",
    "section": "",
    "text": "Semester Project\n\n\nOverview\n\n\nProject Check-ins\n\nPC0\nFork project site and deploy via GitPages\n\n\nPC1\nFind three data sets that you‚Äôre interested in. Include link to dataset.\n\n\nPC2\nSelect one dataset and do some background research. Make sure your data is accessible on a git repo.\n\n\nPC3\nDo EDA on your selected dataset.\n\n\nPC4\nWhat is the goal of the model you plan to build? Has something similar been done with this type of data?\n\n\nPC5\nClean up your data. Create good data visualizations.\n\n\nPC6\nPlan out your modeling workflow: what models would you like to use, what evaluation are you doing? Start building your presentation.\n\n\nPC7\nInitial modeling building. Continue building presentation.\n\n\nPC8\nDo intial model evaluation. How did your model fair?\n\n\nPC9\nFinal model building. Finalize presentation.\n\n\n\nSchedule\n\n\nRubric",
    "crumbs": [
      "Semester Project"
    ]
  },
  {
    "objectID": "07-MOD/lab-MOD.html",
    "href": "07-MOD/lab-MOD.html",
    "title": "Task 2: Outlining a workflow",
    "section": "",
    "text": "Name:\nWho you worked with:"
  },
  {
    "objectID": "07-MOD/lab-MOD.html#objectives",
    "href": "07-MOD/lab-MOD.html#objectives",
    "title": "Task 2: Outlining a workflow",
    "section": "Objectives",
    "text": "Objectives\nThe goals of this project are to:\n\nOutline and implement a full machine learning workflow.\nExplore data-driven decision-making through analysis of cookie recipes.\nApply techniques we‚Äôve seen from class, and be able to comfortably navigate documentation"
  },
  {
    "objectID": "07-MOD/lab-MOD.html#overview",
    "href": "07-MOD/lab-MOD.html#overview",
    "title": "Task 2: Outlining a workflow",
    "section": "Overview",
    "text": "Overview\nFor this programming assignment, you will come up with and implement a workflow to determine the attributes that make a cookie delicious. Based on historical data of 5K cookies regarding baked_temp, sugar_index among others, the authors of Cookie Monster collected the dataset we‚Äôd like to use. The authors started a workflow but didn‚Äôt get very far ‚Äî now it‚Äôs your turn to complete a proper ML workflow! Our goal is to expand upon the outlined workflow (see ReadMe in repo) in order to answer an interesting question about this data. The dataset we‚Äôll be using can be found by clicking the data folder and locating the dataset cookies.csv."
  },
  {
    "objectID": "07-MOD/lab-MOD.html#schedule",
    "href": "07-MOD/lab-MOD.html#schedule",
    "title": "Task 2: Outlining a workflow",
    "section": "Schedule",
    "text": "Schedule\nHere is the suggested schedule for working on this project: - Monday: Read through project instructions, run code for Task 0. - Tuesday: Complete Tasks 1-2. - Wednesday: Complete Tasks 3. - Thursday: Complete Task 4.\nThis project is due on Thursday, 4/17, by 11:59pm.\n#Task1: Cookie Data\nWe‚Äôd like to be able to bring our data in to try to understand it.\nLink to repo: link\n##üíª Q1: Read in data\nYour goal is to use the pandas method read_csv to read in the cookies.csv data from the repo. To find the url for this dataset, go to the data folder, click on cookies.csv, then click the button Raw on the right. You should be brought to a new window containing the dataset. Copy and paste the url into read_csv below (hint: url should begin with raw.githubusercontent.com).\nIf you are unsure how to use read_csv look at previous PAs/workbooks, or use the pandas doc.\n\nimport pandas as pd\ncookies = pd.read_csv(\"https://raw.githubusercontent.com/albertoabreu91/Cookies_project/refs/heads/master/data/cookies.csv\")\n\n\ncookies.head(5)\n\n\n    \n\n\n\n\n\n\nsugar to flour ratio\nsugar index\nbake temp\nchill time\ncalories\ndensity\npH\ngrams baking soda\nbake time\nquality\nbutter type\nweight\ndiameter\nmixins\ncrunch factor\naesthetic appeal\n\n\n\n\n0\n0.25\n9.5\n300\n15.0\n136.0\n0.99367\n8.10\n0.44\n12.1\n8\nmelted\n15.2\n7\nraisins\n1.30\n3\n\n\n1\n0.23\n3.3\n520\n34.0\n113.0\n0.99429\n8.16\n0.48\n8.4\n7\nmelted\n12.4\n7\nraisins\n1.71\n3\n\n\n2\n0.18\n1.9\n360\n33.0\n106.0\n0.98746\n8.21\n0.83\n14.0\n9\nmelted\n9.4\n7\nnuts, chocolate\n1.78\n3\n\n\n3\n0.18\n10.5\n490\n41.0\n124.0\n0.99630\n8.14\n0.35\n10.5\n7\nmelted\n12.2\n7\nchocolate\n1.59\n3\n\n\n4\n0.24\n2.4\n770\n6.0\n33.0\n0.99740\n8.09\n0.57\n9.4\n5\ncubed\n19.8\n7\nnuts, oats, chocolate\n1.30\n3\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n##‚úè Q2: Data Dictionary\nOnce you have the data, copy-paste the data dictionary from the repo below.\n##üíª Q3: Data observations\nFor each feature in the data dictionary, we would like to add 1-2 observations about its distribution (e.g., sugar_index is right-skewed, etc.). Add the code to do this below. Hint: this only requires one line, so what method gives us information about the distribution of features?\nThis step is to help organize and highlight the types of data cleaning that may be important later on. You can add your observations to the data dictionary above."
  },
  {
    "objectID": "07-MOD/lab-MOD.html#q16-data-driven-insights",
    "href": "07-MOD/lab-MOD.html#q16-data-driven-insights",
    "title": "Task 2: Outlining a workflow",
    "section": "Q16: Data-Driven Insights",
    "text": "Q16: Data-Driven Insights\nUse these guiding questions to articulate your key findings:\n\nWhich features mattered the most in predicting cookie quality?\nWas there anything unexpected in your EDA or model results?\nDid your modeling reveal any relationships you didn‚Äôt initially consider?\nWhat changes to the dataset (e.g., filtering, feature engineering) helped improve your results the most?\nIf someone asked you ‚Äúwhat makes a cookie great?‚Äù, how would you answer using evidence?\n\nWrite your three insights below. Be specific and support with evidence from your workflow.\n\n‚úèInsight 1\n\n\n‚úèInsight 2\n\n\n‚úèInsight 3\n##‚úè Q17: Framing a Data Story Good data stories go beyond charts. They interpret the findings and engage the audience. Who would care about this finding? What action or change does this insight suggest? What visual or informative result from your data best expresses this idea?\nRead through the following resources, and identify two strategies you found useful. Think of a way you could incorportate it into this analysis.\n\nCommunicating a Data Story (link)\nCreating Compelling Stories (link)\n\n\n\n‚úèStrategy 1\n\n\n‚úè Strategy 2\n##‚úè Q18: Data story pitch\nWrite a short (4-5 sentence) data story pitch as if you were sharing your results in one of the scenarios below.\nAn example pitch is given for each scenario. Use these examples to tailor to your specific results.\n\nScenario 1: you‚Äôre presenting to the CEO of a mid-sized cookie company looking to improve product quality and customer satisfaction. &gt; ‚ÄúOur model shows that cookies with lower baked temps and more sugar consistently score higher in quality. These two features accounted for approximately 72% of the variance. By reducing the baking temp slightly, your company could produce more satisfying cookies without changing core ingredients‚Äù\nScenario 2: you‚Äôre presenting to a group of recipe developers who are passionate about technique and ingredient optimization. &gt; ‚ÄúOur analysis revealed that using oat flour combined with low moisture content resulted in higher quality cookies. This effect was most pronounced when baking below 340¬∞F. This could inspire a new cookie line that emphasizes chewy, rich textures without added fats.‚Äù\nScenario 3: you‚Äôre demoing this project at a machine learning showcase where you‚Äôre evaluated on both your technical process and communication. &gt; ‚ÄúWe evaluated three models ‚Äî Logistic Regression, Random Forest, and KNN ‚Äî and found that Random Forest gave the highest accuracy at 86%. Key features included sugar_index and baked_temp, which showed strong non-linear relationships with cookie quality. Our pipeline is easily adaptable for predicting quality in other baked goods.‚Äù\n\n#Bringing it all together\nNow that you‚Äôve crafted your own pitch, let‚Äôs see how this process might look outside of the classroom.\nIn professional settings, data work usually starts not with a dataset‚Äîbut with a problem. A product manager, business analyst, or client might come to you and say something like: ‚ÄúWe‚Äôre seeing declining ratings for our cookies‚Äîcan you figure out why?‚Äù or ‚ÄúWe want to develop a low-cost but high-quality recipe. What tradeoffs can we make?‚Äù\nFrom there, your job becomes about framing the right question, identifying the most relevant data (which might not exist yet!!), and shaping a workflow around the decision-making needs of that audience. In contrast, your classroom workflow started with a dataset and built up to a pitch ‚Äî in practice, it often happens in reverse.\nIn real-world projects, you rarely get a perfectly clean dataset up front. Stakeholders might change their minds halfway through. You often need to balance accuracy with speed, interpretability, or budget. This means workflows are often messier: you may prototype a model quickly to explore feasibility before refining, or spend more time understanding what metrics truly matter to decision-makers (e.g., do they care more about accuracy, cost reduction, or customer experience?).\nApplying this type of workflow in the real world relies just as much‚Äîif not more‚Äîon your adaptability and communication skills as it does on your technical abilities. Success often depends on how well you can navigate shifting goals, collaborate with stakeholders, and translate insights into meaningful action.\n#‚úè Q19: Reflection\nWhat did you like about it? What could be improved? Your answers will not affect your overall grade. This feedback will be used to improve future programming assignments.\n#Grading\nFor each of the following accomplishments, there is a breakdown of points which total to 20. The fraction of points earned out of 20 will be multiplied by 5 to get your final score (e.g.¬†17 points earned will be 17/20 * 5 ‚Üí 4.25) * (1pt) Task1 Q1: Correctly pulls in cookies.csv * (1pt) Task1 Q2: Has data dictionary * (2pt) Task1 Q3: Expands upon data dictionary with EDA insights * (1pt) Task2 Q4: The project goal is clearly articulated and provides specific context for the workflow steps, including what is being predicted and why it matters. * (2pt) Task2 Q5-Q9: At least half of the workflow outline includes clear, relevant, and actionable steps that show an understanding of ML processes (e.g., specific methods for EDA, cleaning, or model choice). * (1pt) Task2 Q5-Q9: At least 75% of the workflow outline includes well-defined and appropriate steps that logically contribute to the overall goal and modeling approach. * (1pt) Task2 Q5-Q9: All parts of the workflow outline contain clearly stated, technically appropriate steps that are directly aligned with the project‚Äôs goal. * ( 1pt) Task2 Q7: The selected models are appropriate for the dataset and task, and the reasoning reflects a thoughtful connection between model characteristics (e.g., interpretability, ability to handle feature types, performance) and the data‚Äôs structure and/or prediction goals. * (1pt) Task3 Q10-14: At least half of the code demonstrates clear alignment with the outlined workflow, including appropriate use of methods for EDA, preprocessing, modeling, or evaluation * (1pt) Task3 Q10-14: At least 75% of the code demonstrates clear alignment with the outlined workflow, including appropriate use of methods for EDA, preprocessing, modeling, or evaluation * (1pt) Task3 Q10-14: All code is clearly written, technically sound, and thoughtfully implements each step of the workflow with appropriate use of sklearn tools and data handling. * (1pt) Task3 Q13: The pipeline is well-structured, includes necessary preprocessing/modeling steps, and is flexible enough to support model selection and tuning. * (1pt) Task3 Q15: The best-fit model is clearly interpreted, with reference to performance metrics and insights from feature importance or model behavior. * (1pt) Task4 Q16: At least two insights are clearly articulated, specific, and supported with evidence from EDA, modeling, or feature engineering steps. The response shows a good understanding of the data and connects analysis to the project question. * (1pt) Task4Q17: All three insights are clearly articulated, specific, and supported with evidence from EDA, modeling, or feature engineering steps. The response shows a good understanding of the data and connects analysis to the project question. * (1pt) Task4 Q17: Strategies for creating a compelling story are applicable * (1pt) Task4 Q18: Pitch is orignial, compelling, and reflective of the analysis * (1pt) Q19: Thoughtfully reflected on the assignment"
  },
  {
    "objectID": "04-DATA/rr-DATA.html",
    "href": "04-DATA/rr-DATA.html",
    "title": "DATA readings",
    "section": "",
    "text": "DATA readings\nThe following excerpts are from a data wrangling in Python resource. When reading keep an eye out for some of the methods used (some of which you‚Äôve likely seen already), and general ideas of how to begin working with data (it‚Äôs okay if you don‚Äôt understand every single example). We‚Äôll cover more specific applications in class.\nWorking with a Pandas DataFrame describes how to look at and access features and entries inside a pandas DataFrame.\nReading in data via read_csv aalks through different ways to grab data. We‚Äôll mainly pull csvs from a url (often via git repo), so skim this one.\nData exploring walks through some of the basics of EDA (some of this may look familiar to code from previous PAs).\nData summarization walks through how to do some summarization and transformations common in data cleaning.\nAdditional Pandas content\nCheatsheets: numpy, pandas, reshaping, text data"
  },
  {
    "objectID": "02-EVL/hw-EVL.html",
    "href": "02-EVL/hw-EVL.html",
    "title": "Homework 2: Evaluation and Model Selection",
    "section": "",
    "text": "Due: start of class on the day indicated on the syllabus  Complete the following exercises. Remember to explain your answers.\n\n\n\nWe‚Äôd like to identify models for some real-world scenarios.\n\n\n\n\nThe following questions ask you to choose three models for your workflow:\n\nBefore we select any models, we‚Äôd like to get a baseline model for comparison purposes. What is this baseline model called? Explain how it works (non-technical).\nWe‚Äôd like to pick a simple model for our workflow. Will this model have higher bias or higher variance? Explain why.\nWe‚Äôd like to pick a more complex model for our workflow. Will this model have higher bias or higher variance? Explain why.\n\n\n\n\n\n\nA music streaming service wants to group songs from its massive catalog into distinct genres based on their acoustic features, such as tempo, key, loudness, danceability, energy, and mood. The goal is to automatically identify patterns in the music and suggest personalized playlists or genres to users based on these patterns.\n\nWhat type of model approach should we take?\nWhat is our target variable?\nWhat is our feature set?\nGive an example of an algorithm we could use for our simple model:\nGive an example of an algorithm we could use for our more complicated model:\n\n\n\n\n\n\nA biologist is conducting an experiment to study the effect of various environmental factors on the growth of a specific type of plant. The goal is to predict the plant‚Äôs growth (measured in height) based on factors such as temperature, humidity, soil pH, light intensity, and water availability.\n\nWhat type of model approach should we take?\nWhat is our target variable?\nWhat is our feature set?\nGive an example of an algorithm we could use for our simple model:\nGive an example of an algorithm we could use for our more complicated model:\n\n\n\n\n\n\n\nSuppose we have trained a classifier that attempts to predict whether or not a person has COVID based off of their symptoms. When we compare the results predicted by our classifier to each person‚Äôs actual COVID status, we get the following results on the testing set. Here, 1 represents has COVID, while 0 represents does not have COVID.\n\n\\[\n\\begin{array}{c|c c}\n& y_{\\text{true}} = 1 & y_{\\text{true}} = 0 \\\\ \\hline\ny_{\\text{pred}} = 1 & 128 & 56 \\\\\ny_{\\text{pred}} = 0 & 72 & 285\n\\end{array}\n\\]\nAs an example for how to read this table, this means that there were 56 people who did not have COVID, that the classifier predicted had COVID.\n\nWhat is the accuracy of this classifier? Write a sentence explaining what this means to someone who is not in this course.\nWhat is the precision of this classifier? Write a sentence explaining what this means to someone who is not in this course.\nWhat is the recall of this classifier? Write a sentence explaining what this means to someone who is not in this course.\nWhat is the F1-score of this classifier?\nWhat do you think of the performance of this classifier? Consider the problem ‚Äî how easy or difficult is it to predict COVID status based on symptoms?\n\n\n\n\n\nSuppose we have trained a regression model that attempts to predict house prices based on their location and their characteristics. When we compare the houses‚Äô actual selling price with their predicted price, we get the following results on the (very small) testing set. \\[\n\\begin{array}{c|c c}\n\\text{actual price} (\\$) & 399,000 & 157,000 & 223,000 & 347,000\\\\ \\hline\n\\text{predicted price}(\\$)  & 410,000 & 150,000 & 225,000 & 350,000\n\\end{array}\n\\]\n\nWhat is the mean squared error for the model?\nWhat is the root mean squared error for the model?\nWhat is the mean absolute error for the model?"
  },
  {
    "objectID": "02-EVL/hw-EVL.html#model-selection-scenarios",
    "href": "02-EVL/hw-EVL.html#model-selection-scenarios",
    "title": "Homework 2: Evaluation and Model Selection",
    "section": "",
    "text": "We‚Äôd like to identify models for some real-world scenarios.\n\n\n\n\nThe following questions ask you to choose three models for your workflow:\n\nBefore we select any models, we‚Äôd like to get a baseline model for comparison purposes. What is this baseline model called? Explain how it works (non-technical).\nWe‚Äôd like to pick a simple model for our workflow. Will this model have higher bias or higher variance? Explain why.\nWe‚Äôd like to pick a more complex model for our workflow. Will this model have higher bias or higher variance? Explain why.\n\n\n\n\n\n\nA music streaming service wants to group songs from its massive catalog into distinct genres based on their acoustic features, such as tempo, key, loudness, danceability, energy, and mood. The goal is to automatically identify patterns in the music and suggest personalized playlists or genres to users based on these patterns.\n\nWhat type of model approach should we take?\nWhat is our target variable?\nWhat is our feature set?\nGive an example of an algorithm we could use for our simple model:\nGive an example of an algorithm we could use for our more complicated model:\n\n\n\n\n\n\nA biologist is conducting an experiment to study the effect of various environmental factors on the growth of a specific type of plant. The goal is to predict the plant‚Äôs growth (measured in height) based on factors such as temperature, humidity, soil pH, light intensity, and water availability.\n\nWhat type of model approach should we take?\nWhat is our target variable?\nWhat is our feature set?\nGive an example of an algorithm we could use for our simple model:\nGive an example of an algorithm we could use for our more complicated model:"
  },
  {
    "objectID": "02-EVL/hw-EVL.html#evaluation-metrics-classification",
    "href": "02-EVL/hw-EVL.html#evaluation-metrics-classification",
    "title": "Homework 2: Evaluation and Model Selection",
    "section": "",
    "text": "Suppose we have trained a classifier that attempts to predict whether or not a person has COVID based off of their symptoms. When we compare the results predicted by our classifier to each person‚Äôs actual COVID status, we get the following results on the testing set. Here, 1 represents has COVID, while 0 represents does not have COVID.\n\n\\[\n\\begin{array}{c|c c}\n& y_{\\text{true}} = 1 & y_{\\text{true}} = 0 \\\\ \\hline\ny_{\\text{pred}} = 1 & 128 & 56 \\\\\ny_{\\text{pred}} = 0 & 72 & 285\n\\end{array}\n\\]\nAs an example for how to read this table, this means that there were 56 people who did not have COVID, that the classifier predicted had COVID.\n\nWhat is the accuracy of this classifier? Write a sentence explaining what this means to someone who is not in this course.\nWhat is the precision of this classifier? Write a sentence explaining what this means to someone who is not in this course.\nWhat is the recall of this classifier? Write a sentence explaining what this means to someone who is not in this course.\nWhat is the F1-score of this classifier?\nWhat do you think of the performance of this classifier? Consider the problem ‚Äî how easy or difficult is it to predict COVID status based on symptoms?"
  },
  {
    "objectID": "02-EVL/hw-EVL.html#evaluation-metrics-regression",
    "href": "02-EVL/hw-EVL.html#evaluation-metrics-regression",
    "title": "Homework 2: Evaluation and Model Selection",
    "section": "",
    "text": "Suppose we have trained a regression model that attempts to predict house prices based on their location and their characteristics. When we compare the houses‚Äô actual selling price with their predicted price, we get the following results on the (very small) testing set. \\[\n\\begin{array}{c|c c}\n\\text{actual price} (\\$) & 399,000 & 157,000 & 223,000 & 347,000\\\\ \\hline\n\\text{predicted price}(\\$)  & 410,000 & 150,000 & 225,000 & 350,000\n\\end{array}\n\\]\n\nWhat is the mean squared error for the model?\nWhat is the root mean squared error for the model?\nWhat is the mean absolute error for the model?"
  },
  {
    "objectID": "02-EVL/rr-EVL.html",
    "href": "02-EVL/rr-EVL.html",
    "title": "EVL readings",
    "section": "",
    "text": "EVL readings\nISL-P: Intro to ML section 2.1; Model Accuracy section 2.2; Cross-validation section 5.1; Bootstrap section 5.2\nRead the following articles: Accountable algorithms describes principles for investigating algorithms and holding them accountable.\nAlgorithm guidance expands off of the accountable algorithms article to show some real-world examples. This is a bit long, so skim as needed. Pay attention to the real-world examples."
  }
]