[
  {
    "objectID": "02-EVL/rr-EVL.html",
    "href": "02-EVL/rr-EVL.html",
    "title": "EVL readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. These articles can be quite math-heavy. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\nIntro to ML section 2.1 \nModel Accuracy section 2.2 \nCross-validation section 5.1 \nBootstrap section 5.2\n\n\n\n\n\nAccountable algorithms describes principles for investigating algorithms and holding them accountable. \nAlgorithm guidance expands off of the accountable algorithms article to show some real-world examples. This is a bit long, so skim as needed. Pay attention to the real-world examples."
  },
  {
    "objectID": "02-EVL/rr-EVL.html#islp-selected-readings",
    "href": "02-EVL/rr-EVL.html#islp-selected-readings",
    "title": "EVL readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. These articles can be quite math-heavy. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\nIntro to ML section 2.1 \nModel Accuracy section 2.2 \nCross-validation section 5.1 \nBootstrap section 5.2"
  },
  {
    "objectID": "02-EVL/rr-EVL.html#related-articles",
    "href": "02-EVL/rr-EVL.html#related-articles",
    "title": "EVL readings",
    "section": "",
    "text": "Accountable algorithms describes principles for investigating algorithms and holding them accountable. \nAlgorithm guidance expands off of the accountable algorithms article to show some real-world examples. This is a bit long, so skim as needed. Pay attention to the real-world examples."
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Each lab will be completed with a lab partner. The group can decide which member is the recorder (copies the colab workbook and notetakes) and the researcher (searchs documentation, slide decks, class materials). The recorder should submit a clickable link to the Moodle submission by the next lecture.",
    "crumbs": [
      "Materials",
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#lab-intro",
    "href": "labs.html#lab-intro",
    "title": "Machine Learning",
    "section": "Lab: INTRO",
    "text": "Lab: INTRO\nIn this lab, we explore how to build our own Python dictionaries and functions to store and summarize data. Using a national policing dataset, we examine the impact of policing on Black Americans. Click to view Colab Notebook",
    "crumbs": [
      "Materials",
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#lab-eda",
    "href": "labs.html#lab-eda",
    "title": "Machine Learning",
    "section": "Lab: EDA",
    "text": "Lab: EDA\nIn this lab, we will explore how to critic and refine code outputted by a language model, and practice a bit of prompt engineering. We will be using the Titanic dataset to perform exploratory data analysis tasks, including calculating statistical measures, conducting statistical tests, and creating visualizations. Click to view Colab Notebook",
    "crumbs": [
      "Materials",
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#lab-evl",
    "href": "labs.html#lab-evl",
    "title": "Machine Learning",
    "section": "Lab: EVL",
    "text": "Lab: EVL\nIn this lab, we will explore the Wisconsin Breast Cancer dataset and focus on key aspects of model evaluation and resampling techniques. We will comment on the complexity of chosen models in a developed workflow, discuss resampling techniques, as well as the implications on various evaluation metrics. The lab finishes with auditing the algorithms using the ethical matrix framework discussed in class. Click to view Colab Notebook",
    "crumbs": [
      "Materials",
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#lab-cls",
    "href": "labs.html#lab-cls",
    "title": "Machine Learning",
    "section": "Lab: CLS",
    "text": "Lab: CLS\nIn this lab, we will be working with image data and exploring how decisions made in the modeling workflow impact results for our clustering models. There are a selection of open-ended questions (give your best guess on these) along with a few questions that require you to implement your own clustering functions. Click to view Colab Notebook",
    "crumbs": [
      "Materials",
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#lab-data",
    "href": "labs.html#lab-data",
    "title": "Machine Learning",
    "section": "Lab: DATA",
    "text": "Lab: DATA\nIn this lab, we will perform various data cleaning techniques and transformations on a rental housing dataset. The last task in this lab requires you to go out and view analyses that other coders have published using this dataset, bringing back at least one interesting code snippet you saw. Click to view Colab Notebook",
    "crumbs": [
      "Materials",
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#lab-tree",
    "href": "labs.html#lab-tree",
    "title": "Machine Learning",
    "section": "Lab: TREE",
    "text": "Lab: TREE\nIn this lab, we will train a tree-based model that can use the measurements of a tumor to diagnosis it as benign or malignant. Click to view Colab Notebook",
    "crumbs": [
      "Materials",
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#lab-models",
    "href": "labs.html#lab-models",
    "title": "Machine Learning",
    "section": "Lab: Models",
    "text": "Lab: Models\nFor this two-part lab, you will come up with (Part 1) and implement (Part 2) a workflow to determine the attributes that make a cookie delicious. Based on historical data of 5K cookies regarding baked_temp, sugar_index among others, the authors of Cookie Monster collected the dataset we’d like to use.\n\nPart 1\nClick to view Colab Notebook\n\n\nPart 2\nClick to view Colab Notebook",
    "crumbs": [
      "Materials",
      "Labs"
    ]
  },
  {
    "objectID": "04-DATA/rr-DATA.html",
    "href": "04-DATA/rr-DATA.html",
    "title": "DATA readings",
    "section": "",
    "text": "DATA readings\nFeature Engineering walks through various data preparation methods as well as PCA.\nThe following excerpts are from a data wrangling in Python resource. When reading keep an eye out for some of the methods used (some of which you’ve likely seen already), and general ideas of how to begin working with data (it’s okay if you don’t understand every single example). We’ll cover more specific applications in class.\nWorking with a Pandas DataFrame describes how to look at and access features and entries inside a pandas DataFrame.\nReading in data via read_csv aalks through different ways to grab data. We’ll mainly pull csvs from a url (often via git repo), so skim this one.\nData exploring walks through some of the basics of EDA (some of this may look familiar to code from previous PAs).\nData summarization walks through how to do some summarization and transformations common in data cleaning.\nAdditional Pandas content\nCheatsheets: numpy, pandas, reshaping, text data"
  },
  {
    "objectID": "05-TREE/WS-TREE.html",
    "href": "05-TREE/WS-TREE.html",
    "title": "Machine Learning",
    "section": "",
    "text": "The worksheet we will be using during lectures TREE1 and TREE 2 \nBelow is the solution for the worksheet"
  },
  {
    "objectID": "05-TREE/rr-TREE.html",
    "href": "05-TREE/rr-TREE.html",
    "title": "TREE readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. These articles can be quite math-heavy. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\n\n\nRegression Trees section 8.1.1 \nClassification Trees section 8.1.1 \nLinear vs Tree Models section 8.1.1 \nPros and Cons of Tree Models section 8.1.1 \n\n\n\n\n\nBagging section 18.2.1\nRandom Forests section 18.2.2\nBoosting section 18.2.3"
  },
  {
    "objectID": "05-TREE/rr-TREE.html#islp-selected-readings",
    "href": "05-TREE/rr-TREE.html#islp-selected-readings",
    "title": "TREE readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. These articles can be quite math-heavy. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\n\n\nRegression Trees section 8.1.1 \nClassification Trees section 8.1.1 \nLinear vs Tree Models section 8.1.1 \nPros and Cons of Tree Models section 8.1.1 \n\n\n\n\n\nBagging section 18.2.1\nRandom Forests section 18.2.2\nBoosting section 18.2.3"
  },
  {
    "objectID": "07-MOD/rr-MODELS.html",
    "href": "07-MOD/rr-MODELS.html",
    "title": "MODELS readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\n\n\nRegression Trees section 8.1.1 \nClassification Trees section 8.1.1 \nLinear vs Tree Models section 8.1.1 \nPros and Cons of Tree Models section 8.1.1 \n\n\n\n\n\nBagging section 18.2.1\nRandom Forests section 18.2.2\nBoosting section 18.2.3"
  },
  {
    "objectID": "07-MOD/rr-MODELS.html#islp-selected-readings",
    "href": "07-MOD/rr-MODELS.html#islp-selected-readings",
    "title": "MODELS readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\n\n\nRegression Trees section 8.1.1 \nClassification Trees section 8.1.1 \nLinear vs Tree Models section 8.1.1 \nPros and Cons of Tree Models section 8.1.1 \n\n\n\n\n\nBagging section 18.2.1\nRandom Forests section 18.2.2\nBoosting section 18.2.3"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nYou may choose to work on any machine learning project of your choice. This could be similar to the types of regression, classification, or clustering that we will cover in class, or could be on machine learning related to text (things like sentiment analysis, or text generation), or could be on machine learning related to images (things like image recognition, or image generation), or another area of machine learning altogether.\nYou must complete this project individually.\n\n\nRequirements\nThe first requirement for the project is that you have created a workflow plan for each section of the ML workflow process, and have code that implements that plan. You will want to create a colab workbook, and include this plan at the beginning of your code notebook.\nYour plan should include the following:\n\nWhat will be your goal in working on this project?\nWhat will be your source for data for the project? If this requires you to gather data, how will you go about this?\nWhat data cleaning, conversion, or preparation will you need to do to prepare your data set?\nWhat kinds of exploratory data analysis will you do?\nWhich machine learning algorithms will you use to train models? Why are you choosing these algorithms?\nHow will you attempt to optimize your models?\nHow will you analyze the accuracy of your models?\n\nYour workflow should have some form of data pre-processing, some kind of exploratory data analysis, training and testing of machine learning algorithms (including some parameter tuning), and evaluating the performance of your models. It is very important that your plan is complete and very detailed, in order to show a thorough understanding of the process of training a model using machine learning, in case you don’t have time to fully complete your project.\nThe second requirement for the project is that it use one of the following datasets. These datasets cover a variety of topics and have enough records to build substantial models. If you wish, you may choose your own data set (both the UCI repo or data.world are good places to start) that is not on this list, but you’ll need to check with your instructor first (look for data &gt;5k records with both numeric and categorial features)\n\nCardiotocography\nHuman Activity Recognition Using Smartphones\nLetter Recognition\nTurkiye Student Evaluation\nBank Marketing Data Set\nContraceptive Method Choice Data Set\nCrimes in Chicago\nDefault of Credit Card Clients Data Set\nOnline Shoppers Purchasing Intention Dataset\nSynthetic Financial Datasets for Fraud Detection (fake data set, but should still provide useful practice)\nRice Variety Recognition\nMice Protein Expression\n\n\n\nProject Check-ins\nThere are nine progress checks during the semester, each worth 4 points (see the schedule). To receive full credit for a progress check, you must submit work before the next lecture’s afternoon session.\n\nPC0\nRead over the project, taking a look at deadlines and deliverables. Then start to look at datasets you’re interested in. You can certainly use the ones given, but it is a requirement to find an outside datasource and post on the Forum under “Places to find additional data sources”\n\n\nPC1\nIdentify three data sets that you’re interested in. Include a link, a two sentence description, and the reasoning behind selection for each dataset. Post on the Forum under “Three datasets of interest” \n\n\nPC2\nCreate a Colab workbook and copy-paste the workflow plan above into a text box. Identify the dataset you’d like to use for your project, and fill out the project goal (first question). You may have to do some background research on your topic. Then fill out the next three questions.\nNext, make sure your data is accessible on a git repo somewhere so you can get started on the next PC. Depending on your project, this might be as simple as locating a git repo that has your data set, or it might require you to upload the dataset to your personal git repo.\nSubmit the link to your colab workbook to the Moodle submission.\n\n\nPC3\nPerform some exploratory data analysis using your workbook. Discuss the results, and what this tells you about your data, as well as your expectations for your results. This should include some data visualizations that go beyond a basic chart.\nSubmit a picture of a plot you made along with a quick relection to the Moodle submission\n\n\n\nPC4\nTime to do some more background research. By now, hopefully you have identified what the goal of the model you plan to build is. Your task is to see if something similar been done with this type of data. Do a brief write up of background information (one paragraph) making sure to link sources.\nYou should also continue to build out your workflow plan, adding more details as you learn new material in class.\n\n\nPC5\nPrepare your data set for training. This might include handling missing or categorical values, basic conversions, or more extensive conversions. Any choices that you make should be explained. Additionally, you may want to consider creating more visualizations using this pre-processed (or what we call “clean”) data.\n\n\nPC6\nFinish laying out your project workflow and begin building your presentation. Think about how to make the insights from your analysis, your model goal, or the background information engaging for a general audience. Use this checkpoint to explore what makes a story compelling and experiment with ways to bring that story to life in your presentation.\n\n\nPC7\nTrain models using machine learning algorithms. Although you should not use every algorithm that we’ve covered, you should use those that make sense for your data set. You should explain your choices, and perform parameter tuning in order to optimize their performance.\nAdd a slide to your presentation describing the algorithms and tuning you did, making sure to make it relatable for a general audience.\n\n\nPC8\nDiscussion of results. Compare the performance of your models, and discuss the results. Do you consider your results successful? Explain why or why not, and discuss how further improvements might be made.\nContinue working on your final presentation. You want a presentation that is digestible to a non-technical audidence and is interesting (visualizations, insights, models, data).\n\n\nPC9\nEvaluation of your plan. Discuss how closely you were able to follow your workflow. What unexpected issues did you encounter? What adjustments did you need to make? How would this affect planning a future machine learning project? Finalize your presentation, and submit your link along with a screenshot to Moodle.\n\n\n\nFinal Submission\nThe final submission evaluates the overall quality of your project and presentation, based on the materials submitted prior to the final exam. Submissions are due the night before the final.\nA detailed grading rubric TBD\n\n\nSmall group presentations\n(12 pts): During presentations, you will participate in three rounds of small-group presentations, presenting one-on-one with another project. Grading is based on the quality of your peer review during these rounds.\nThe peer review form is available here",
    "crumbs": [
      "Materials",
      "Project"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This is the homepage for CSCI 200 - Introduction to Machine Learning taught by Prof. Kim Mandery in January 2026 at St. Olaf College. All course materials will be posted on this site.\nWe will be using our Moodle page to submit assignments and host our class discussion forum\nClick here for the course syllabus or here for the course schedule\n\nArtwork: The Myth of Zhuangzi by Yeachin Tsai",
    "crumbs": [
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "course_information/syllabus.html",
    "href": "course_information/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "CSCI 200B Interim 2026 Special Topics: Machine Learning Morning Sessions: 10:40-12:40 in RNS 203 Afternoon Sessions: 1-3 in RNS 203\n\n\nMission: This is a welcoming, inclusive, encouraging, and failure-tolerant class.  Instructor: Kim Mandery  E-mail: mander1@stolaf.edu  Office: RMS 407\nTextbook: Introduction to Statistical Learning with Python (ISL-P). This book is free and available online, and is also hosted on this site. You can download your own pdf or access via your browser here\n\n\n\nIt has become increasingly common to use machine learning algorithms to analyze data, draw conclusions, and build models, without direct human instruction. These algorithms have been used in a wide variety of applications, including Netflix recommendations, predicting healthcare outcomes, criminal justice, and many more. In this course, we’ll explore several common machine learning algorithms, learning how they work, and applying them to real datasets. We will cover the strengths and limitations of machine learning algorithms. We will also explore real-world applications of machine learning, and discuss the ethical and societal consequences of the use of these algorithms.\n\n\n\nThrough the lens of machine learning, we will:\n\ndevelop and interpret applications of algorithms to domain use-cases\ndevelop working software that satisfies coding best practices\nwork effectively, both individually and in teams\ncommunicate information effectively to both technical and non-technical audiences\n\n\n\n\nThe schedule for this course is available by navigating to Course Information &gt; Schedule on the navigation window to the left. This page is likely to be updated throughout the semester, and will be announced in class if it is.\n\n\n\nI expect you to take an active role in class each day, participating positively, wholeheartedly, and respectfully in class/group discussions and other activities. You may need sick/mental health days. You are responsible for learning the material on the day you missed. If you are experiencing issues causing you to miss classes often, please speak with me as this may be evident of a possible failing grade. You can also visit your class dean as they are a great resource."
  },
  {
    "objectID": "course_information/syllabus.html#general-information",
    "href": "course_information/syllabus.html#general-information",
    "title": "Syllabus",
    "section": "",
    "text": "Mission: This is a welcoming, inclusive, encouraging, and failure-tolerant class.  Instructor: Kim Mandery  E-mail: mander1@stolaf.edu  Office: RMS 407\nTextbook: Introduction to Statistical Learning with Python (ISL-P). This book is free and available online, and is also hosted on this site. You can download your own pdf or access via your browser here"
  },
  {
    "objectID": "course_information/syllabus.html#course-description",
    "href": "course_information/syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "It has become increasingly common to use machine learning algorithms to analyze data, draw conclusions, and build models, without direct human instruction. These algorithms have been used in a wide variety of applications, including Netflix recommendations, predicting healthcare outcomes, criminal justice, and many more. In this course, we’ll explore several common machine learning algorithms, learning how they work, and applying them to real datasets. We will cover the strengths and limitations of machine learning algorithms. We will also explore real-world applications of machine learning, and discuss the ethical and societal consequences of the use of these algorithms."
  },
  {
    "objectID": "course_information/syllabus.html#course-goals",
    "href": "course_information/syllabus.html#course-goals",
    "title": "Syllabus",
    "section": "",
    "text": "Through the lens of machine learning, we will:\n\ndevelop and interpret applications of algorithms to domain use-cases\ndevelop working software that satisfies coding best practices\nwork effectively, both individually and in teams\ncommunicate information effectively to both technical and non-technical audiences"
  },
  {
    "objectID": "course_information/syllabus.html#course-schedule",
    "href": "course_information/syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "",
    "text": "The schedule for this course is available by navigating to Course Information &gt; Schedule on the navigation window to the left. This page is likely to be updated throughout the semester, and will be announced in class if it is."
  },
  {
    "objectID": "course_information/syllabus.html#active-learning-and-engagement",
    "href": "course_information/syllabus.html#active-learning-and-engagement",
    "title": "Syllabus",
    "section": "",
    "text": "I expect you to take an active role in class each day, participating positively, wholeheartedly, and respectfully in class/group discussions and other activities. You may need sick/mental health days. You are responsible for learning the material on the day you missed. If you are experiencing issues causing you to miss classes often, please speak with me as this may be evident of a possible failing grade. You can also visit your class dean as they are a great resource."
  },
  {
    "objectID": "course_information/syllabus.html#readings",
    "href": "course_information/syllabus.html#readings",
    "title": "Syllabus",
    "section": "Readings",
    "text": "Readings\n(optional): Each module has selected readings related to the content covered in class, consisting of textbook passages from Introduction to Statistical Learning (ISL-P), as well as articles on ethics and/or domain use-cases. In a typical semester-long course, these readings would be required. In our four-week course, these readings are optional. If you find that you have questions on a particular topic, check out the readings to find related information."
  },
  {
    "objectID": "course_information/syllabus.html#article-reviews",
    "href": "course_information/syllabus.html#article-reviews",
    "title": "Syllabus",
    "section": "Article Reviews",
    "text": "Article Reviews\n(5pt * 3 → 15 pt): In place of readings, you will be required to find three (3) articles related to machine learning, and complete a review on each. The article reviews are due before the final exam, though it is recommended to finish them as early as possible. Read more about article reviews here."
  },
  {
    "objectID": "course_information/syllabus.html#homework-assignments",
    "href": "course_information/syllabus.html#homework-assignments",
    "title": "Syllabus",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n(5pt * 8 → 40 pt): We will have a homework assignment for each module that we cover, which will be due at the start of class of the following class. These homework assignments consist of exercises to complete by hand (rather than programming), to reinforce your understanding of the concepts. Collaboration on homework is encouraged, though you are responsible for understanding the computations as you will be assessed on similar questions during your quizzes.  Late work: Due to the short turn-around on turning homework in and taking the module quiz, no late work will be accepted."
  },
  {
    "objectID": "course_information/syllabus.html#lab-assignments",
    "href": "course_information/syllabus.html#lab-assignments",
    "title": "Syllabus",
    "section": "Lab Assignments",
    "text": "Lab Assignments\n(5pt * 9 → 45 pt): We will have labs associated with every module and you are encouraged to work on labs with one other person. Only one partner needs to submit the lab for the group. Labs are due before we take the corresponding quiz. These assignments consist of programming exercises to practice applying the techniques we cover. Typically, these assignments will be completed by filling in missing code into Jupyter notebooks using Google Colab, and then submitting your completed notebook. Each exercise will be graded all-or-nothing, with no partial credit. Late work: Due to the short turn-around on turning labs in and taking the module quiz, no late work will be accepted."
  },
  {
    "objectID": "course_information/syllabus.html#semester-project",
    "href": "course_information/syllabus.html#semester-project",
    "title": "Syllabus",
    "section": "Semester Project",
    "text": "Semester Project\n(80 pts): Over the entirty of this course, you will complete a personal project where you apply the concepts covered in the course to a topic of your choice. You can find more in-depth information regarding the project on the project page\nThe project will be divided into the following components:\n\nProgress Checks (4pt * 9 → 36 pt): There are nine progress checks during the semester, each worth 4 points (see the schedule). To receive full credit for a progress check, you must check in with your instructor before leaving class on the day of the check.\nFinal Submission (30 pts): The final submission evaluates the overall quality of your project and presentation, based on the materials submitted prior to the final exam. Submissions are due the night before finals. A detailed grading rubric will be available on the project page.\nSmall group presentations (9 pts): During presentations, you will participate in three rounds of small-group presentations, presenting one-on-one with another project. Grading is based on the quality of your peer review during these rounds. The peer review form will be available on the project page.\nReflection (5 pts): Complete the reflection form up on the project page. This form asks you what went well, what could be improved upon, ect.\n\nLate Work: No late work will be accepted."
  },
  {
    "objectID": "course_information/syllabus.html#standards-based-quizzes",
    "href": "course_information/syllabus.html#standards-based-quizzes",
    "title": "Syllabus",
    "section": "Standards based Quizzes",
    "text": "Standards based Quizzes\n(8pt * 9 → 72 pt): Each module will have a quiz that will be assessed using standards-based grading. Each quiz will contain two (2) questions called standards. There will be a study guide for each quiz that indicates which standards are being assessed. The full list of standards is in the section ML Core Standards found on this page.\nYou will be given three attempts to pass each standard. The first attempt will be during the module quiz as indicated in the course schedule. The second attempt will be during the retake day associated with each module (see schedule). All final attempts for standards will take place during our final exam time after presentations.\nThe standards are graded on an 8-point scale and binned into four categories:\n\nP proficient (8 pt)\nR small revision needed\nN not satisfactory (4 pt)\nI incomplete (0 pt)\n\nRevisions must be handed in before your next quiz. Standards with an R have a fully correct solution handed in will be bumped up to a P. Otherwise it will get bumped down to an N.\nAny student with N or I for three or more standards at the end of the course will be ineligible for a final grade greater than a B- in the course."
  },
  {
    "objectID": "course_information/syllabus.html#extra-credit",
    "href": "course_information/syllabus.html#extra-credit",
    "title": "Syllabus",
    "section": "Extra Credit",
    "text": "Extra Credit\nSome projects have additional questions you can answer for extra credit."
  },
  {
    "objectID": "course_information/syllabus.html#final-exam",
    "href": "course_information/syllabus.html#final-exam",
    "title": "Syllabus",
    "section": "Final Exam",
    "text": "Final Exam\nThe final exam is during the assigned period:\nFriday, Jan 30 from 10:40 - 3:00 P.M.\nAny final attempt on standards will take place during the second half of our final time. If you successfully passed each standard, you do not have to attend the second half of our final exam.\n\nFinal Grade Scale\n97 ≤ A+ ≤ 100 87 ≤ B+ ≤ 90 77 ≤ C+ ≤ 80 67 ≤ D+ ≤ 70  93 ≤ A ≤ 97 83 ≤ B ≤ 87 73 ≤ C ≤ 77 63 ≤ D ≤ 67  90 ≤ A- ≤ 93 80 ≤ B- ≤ 83 70 ≤ C- ≤ 73 60 ≤ D- ≤ 63 \nExplanations of each letter grade range can be found at https://catalog.stolaf.edu/academic-regulations-procedures/grades/"
  },
  {
    "objectID": "course_information/syllabus.html#syllabus-changes",
    "href": "course_information/syllabus.html#syllabus-changes",
    "title": "Syllabus",
    "section": "Syllabus Changes",
    "text": "Syllabus Changes\nAll course information provided in this syllabus is subject to change and/or elimination. Changes will only be made when the instructor feels they are in the best interest of the class. It is the responsibility of every student to attend class, check e-mail, and communicate with the instructor to be informed and understand these changes.\nMost importantly HAVE FUN – I am really looking forward to this semester!"
  },
  {
    "objectID": "course_information/syllabus.html#workflow",
    "href": "course_information/syllabus.html#workflow",
    "title": "Syllabus",
    "section": "Workflow",
    "text": "Workflow\n\ndefinition and examples of a broad variety of machine learning tasks:\n\nsupervised learning (classification, regression)\nunsupervised learning (clustering);\n\nability to identify, construct, and critique ML workflows\n\nStandards include: FLOW, FLOW2, FLOW-LIN, ALG-CLS"
  },
  {
    "objectID": "course_information/syllabus.html#eda",
    "href": "course_information/syllabus.html#eda",
    "title": "Syllabus",
    "section": "EDA",
    "text": "EDA\n\nunderstanding of statistical measures, distributions, and tests\nexploration of data to guide workflow design and model selection\n\nStandards include: EDA-STAT"
  },
  {
    "objectID": "course_information/syllabus.html#data",
    "href": "course_information/syllabus.html#data",
    "title": "Syllabus",
    "section": "Data",
    "text": "Data\n\ndata preprocessing (importance and pitfalls)\nhandling missing values (imputing, flag-as-missing, implications)\nencoding categorical variables and real-valued data\nnormalization and standardization.\n\nStandards include: DATA-PREP"
  },
  {
    "objectID": "course_information/syllabus.html#selection",
    "href": "course_information/syllabus.html#selection",
    "title": "Syllabus",
    "section": "Selection",
    "text": "Selection\n\nimportance of understanding what your model is actually doing, where its pitfalls or shortcomings are, and the implications of its decisions\nno free lunch (no one learner can solve all problems)\nrepresentational design decisions have consequences\nsources of error and undecidability in machine learning\n\nStandards include: FLOW-LIN, MOD-SEL"
  },
  {
    "objectID": "course_information/syllabus.html#algorithms",
    "href": "course_information/syllabus.html#algorithms",
    "title": "Syllabus",
    "section": "Algorithms",
    "text": "Algorithms\n\nfundamentals of understanding how common ML algorithms work (including but not limited to):\n\nclustering\ntree-based\nregression-based\nensemble\nneural nets\ndeep learning\n\nability to explain algorithms and their associated terms to a non-technical audience (including but not limited to):\n\nobjective function\ngradient descent\nregularization\nentropy\n\n\nStandards: ALG-CLS, ALG-TREE, ALG-LIN, ALG-ESB, ALG-NN, ALG-DL"
  },
  {
    "objectID": "course_information/syllabus.html#training",
    "href": "course_information/syllabus.html#training",
    "title": "Syllabus",
    "section": "Training",
    "text": "Training\n\nseparation of train, validation, and test sets\ntuning the parameters of a machine learning model with a validation set\ncross validation\noverfitting problem / controlling solution complexity (regularization, pruning–intuition only)\nthe bias(underfitting) - variance (overfitting) tradeoff Standards: TRAIN, TRAIN-TREE"
  },
  {
    "objectID": "course_information/syllabus.html#evaluation",
    "href": "course_information/syllabus.html#evaluation",
    "title": "Syllabus",
    "section": "Evaluation",
    "text": "Evaluation\nperformance metrics for classifiers; estimation of test performance on held-out data; other metrics for classification (e.g., error, precision, recall); performance metrics for regressors; confusion matrix;\nStandards: EVAL, EVAL-CLS"
  },
  {
    "objectID": "course_information/syllabus.html#ethics",
    "href": "course_information/syllabus.html#ethics",
    "title": "Syllabus",
    "section": "Ethics",
    "text": "Ethics\n\nfocus on real data, scenarios, and case studies\nbias present in datasets, algorithms, evaluation\nprivacy\nfairness\nethical matrix"
  },
  {
    "objectID": "course_information/syllabus.html#inclusivity-and-community",
    "href": "course_information/syllabus.html#inclusivity-and-community",
    "title": "Syllabus",
    "section": "Inclusivity and Community",
    "text": "Inclusivity and Community\nIn keeping with St. Olaf College’s mission statement, this class strives to be an inclusive and antiracist learning community, respecting and promoting those of differing backgrounds and beliefs. As a community, we will be to be respectful to all citizens in this class, regardless of race, ethnicity, religion, gender, or sexual orientation. This course affirms people of all gender expressions and gender identities. If you prefer to be called a different name or pronoun than what is on the class roster, please let me know."
  },
  {
    "objectID": "course_information/syllabus.html#illness-and-community-standards",
    "href": "course_information/syllabus.html#illness-and-community-standards",
    "title": "Syllabus",
    "section": "Illness and Community Standards",
    "text": "Illness and Community Standards\nOut of respect for our learning community, if you are experiencing any symptoms of an illness, do not come to class or my office. Please contact me before class time to let me know of your absence."
  },
  {
    "objectID": "course_information/syllabus.html#cell-phone-policy-and-classroom-atmosphere",
    "href": "course_information/syllabus.html#cell-phone-policy-and-classroom-atmosphere",
    "title": "Syllabus",
    "section": "Cell Phone Policy and Classroom Atmosphere",
    "text": "Cell Phone Policy and Classroom Atmosphere\nYou are expected to contribute to a positive classroom atmosphere, which includes arriving on time, not being disruptive, being respectful, actively involving yourself in class, and silencing and putting away cell phones. There may come a time where we use technology in class – during this time you can use a laptop, tablet, or your phone if you do not have the other options available to you. At any other times, however, I do not want to see your phone out. I’ll ask you to put them away if they show up."
  },
  {
    "objectID": "course_information/syllabus.html#mental-and-physical-health",
    "href": "course_information/syllabus.html#mental-and-physical-health",
    "title": "Syllabus",
    "section": "Mental and Physical Health",
    "text": "Mental and Physical Health\nI greatly value your experience in this class, and it is my duty to facilitate a safe, caring, and productive learning environment. I recognize that you may experience a range of emotional, physical, and/or psychological issues, both in and out of the classroom, that may distract you from your learning. If you are experiencing such issues, please do not hesitate to see me– I am here to listen. We can also discuss what further resources might be available to you."
  },
  {
    "objectID": "course_information/syllabus.html#academic-accommodations",
    "href": "course_information/syllabus.html#academic-accommodations",
    "title": "Syllabus",
    "section": "Academic Accommodations",
    "text": "Academic Accommodations\nI am committed to supporting the learning of all students in my class. If you have already registered with Disability and Access (DAC) and have your letter of accommodations, please meet with me as soon as possible to discuss, plan, and implement your accommodations in the course. If you have or think you have a disability (learning, sensory, physical, chronic health, mental health or attentional), please contact Disability and Access staff at 507-786-3288 or by visiting https://wp.stolaf.edu/academic-support/dac."
  },
  {
    "objectID": "course_information/syllabus.html#multilingual-students",
    "href": "course_information/syllabus.html#multilingual-students",
    "title": "Syllabus",
    "section": "Multilingual Students",
    "text": "Multilingual Students\nI am committed to making course content accessible to all students. If English is not your first language and this causes you concern about the course, please speak with me. Students who would like extra support with writing or speaking in English can also contact the language specialist in CAAS."
  },
  {
    "objectID": "course_information/syllabus.html#managing-stress-anxiety-and-other-issues",
    "href": "course_information/syllabus.html#managing-stress-anxiety-and-other-issues",
    "title": "Syllabus",
    "section": "Managing stress, anxiety, and other issues",
    "text": "Managing stress, anxiety, and other issues\nI greatly value your experience in this class, and it is my duty to facilitate a safe, caring, and productive learning environment. I recognize that you may experience a range of emotional, physical, and/or psychological issues, both in and out of the classroom, that may distract you from your learning. If you are experiencing such issues, please do not hesitate to come see me – I am here to listen. We can also discuss what further resources might be available to you."
  },
  {
    "objectID": "course_information/syllabus.html#plagiarism-academic-integrity",
    "href": "course_information/syllabus.html#plagiarism-academic-integrity",
    "title": "Syllabus",
    "section": "Plagiarism & Academic Integrity",
    "text": "Plagiarism & Academic Integrity\nPlagiarism, the unacknowledged appropriation of another person’s words or ideas, is a serious academic offense. It is imperative that you hand in work that is your own, and that cites or gives credit to others whenever you draw from their work. This includes citing prompts when using genAI tools or an internet search. Please see St.Olaf’s statements on academic integrity and plagiarism at: https://wp.stolaf.edu/thebook/academic/integrity/. See also the description of St.Olaf’s honor system at: https://wp.stolaf.edu/honorcouncil/."
  },
  {
    "objectID": "course_information/syllabus.html#communication-expectations",
    "href": "course_information/syllabus.html#communication-expectations",
    "title": "Syllabus",
    "section": "Communication Expectations:",
    "text": "Communication Expectations:\n\nInstructor/Student Expectations: I check my email frequently, and attempt to respond to questions within 24 hours. It should not be expected that I check my email after 9 PM, on Saturdays, or regularly during breaks.\nE-mail Etiquette: If you have multiple questions, fewer emails with more information per email is preferred. Please include the following when sending an email: course number with section number, a description of your question(s) within the subject line, a greeting and your instructors preferred name (mine is Kim), and a sign-off using your preferred name (this is what you want others to refer to you in class by). Add screenshots of your work for clarity."
  },
  {
    "objectID": "course_information/syllabus.html#the-st.-olaf-honor-system",
    "href": "course_information/syllabus.html#the-st.-olaf-honor-system",
    "title": "Syllabus",
    "section": "The St. Olaf Honor System",
    "text": "The St. Olaf Honor System\nThe St. Olaf Honor System has been in place at St. Olaf College since 1911. All tests, quizzes, and examinations of any kind are taken under the St. Olaf Honor System. Each student is responsible for adhering to all principles of the Honor System regardless of the individual circumstances associated with their assessment environment."
  },
  {
    "objectID": "course_information/syllabus.html#computer-science-program-attendance-policy-jterm-2026",
    "href": "course_information/syllabus.html#computer-science-program-attendance-policy-jterm-2026",
    "title": "Syllabus",
    "section": "Computer Science Program Attendance Policy Jterm 2026:",
    "text": "Computer Science Program Attendance Policy Jterm 2026:\nAttending class is essential for learning the content of the course, as well as practicing communication and collaboration. Missing class hinders your ability to achieve these important learning outcomes. That said, you understandably may need to occasionally miss class due to illness or a scheduling conflict. In these situations, I do not want to judge which absences should count as “excused” or “unexcused.” So, all absences will be treated equally, since they all result in missing important class time. Additionally, arriving late disrupts not only your own learning but also your classmates’ ability to engage fully in class activities. Any tardiness over 5 minutes (subject to instructor adjustment) will also be counted as an absence. During Jterm, each session (morning and afternoon) will have attendance taken, so missing an entire day of class equates to two absences.\nTo reflect the importance of attendance in this course: (1) If you have accrued two absences without contacting me in advance, I’ll notify your academic advisor and the Dean’s Office by filing an Early Alert Form to make sure you’re getting the support you need. (2) If you have accrued four absences (whether or not you contacted me in advance), you must meet with me to discuss your standing in the course and what you need to do to get back on track for successfully completing the class. (3) If you have accrued nine absences (whether or not you contacted me in advance), you’ll automatically fail the class. Nine absences means missing nearly a quarter of all class sessions, indicating that you have not legitimately completed a student-centered course that requires class engagement. If you do need to miss class, I strongly encourage you to check in with another student on what you missed, and attend office hours to discuss any questions that you have."
  },
  {
    "objectID": "01-EDA/std-EDA.html",
    "href": "01-EDA/std-EDA.html",
    "title": "Machine Learning",
    "section": "",
    "text": "The following standards will be assessed on the EDA quiz:"
  },
  {
    "objectID": "01-EDA/std-EDA.html#flow",
    "href": "01-EDA/std-EDA.html#flow",
    "title": "Machine Learning",
    "section": "FLOW",
    "text": "FLOW\nYou should be able to:\n\nDefine learning approaches (supervised vs. unsupervised learning) and learning tasks (regression, classification, and clustering).\nInterpret a dataset summary table.\nDescribe a model’s goal and explain who might benefit from the model.\nDescribe the five steps ML workflow and give a few examples of tasks for each step"
  },
  {
    "objectID": "01-EDA/std-EDA.html#eda-stat",
    "href": "01-EDA/std-EDA.html#eda-stat",
    "title": "Machine Learning",
    "section": "EDA-STAT",
    "text": "EDA-STAT\nYou should be able to:\n\nUnderstand and describe statistical measures, distributions, and tests\nRead and interpret charts, including identifying key features (skew, balance, shape) and chart types (histogram, boxplot, scatterplot)\nIdentify different feature types (numeric, discrete, continuous, ordinal, categorical, binary, or none).\nExplain what factors to consider when deciding whether a feature type is appropriate to include in a machine learning model."
  },
  {
    "objectID": "01-EDA/std-EDA.html#study-tips",
    "href": "01-EDA/std-EDA.html#study-tips",
    "title": "Machine Learning",
    "section": "Study Tips",
    "text": "Study Tips\nReview the lecture slides, workbooks, homework assignments, and the associated lab.\nQuiz questions are similar (though not identical) to the types of questions covered in these materials. You should be able to use appropriate terminology and apply the concepts covered in class to a variety of new scenarios.\nThe primary goal of each quiz is to give you an opportunity to apply concepts with clear and well-explained reasoning.\nQuizzes are also a valuable opportunity to receive feedback on your reasoning skills."
  },
  {
    "objectID": "06-LIN/rr-LIN.html",
    "href": "06-LIN/rr-LIN.html",
    "title": "LIN readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\nOverview chapter 3 \nSimple Linear Regression section 3.1 \nMultiple Linear Regression section 3.2 \nConsiderations section 3.3"
  },
  {
    "objectID": "06-LIN/rr-LIN.html#islp-selected-readings",
    "href": "06-LIN/rr-LIN.html#islp-selected-readings",
    "title": "LIN readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\nOverview chapter 3 \nSimple Linear Regression section 3.1 \nMultiple Linear Regression section 3.2 \nConsiderations section 3.3"
  },
  {
    "objectID": "06-LIN/std-LIN.html",
    "href": "06-LIN/std-LIN.html",
    "title": "Machine Learning",
    "section": "",
    "text": "The following standards will be assessed on the LIN quiz:"
  },
  {
    "objectID": "06-LIN/std-LIN.html#alg-lin",
    "href": "06-LIN/std-LIN.html#alg-lin",
    "title": "Machine Learning",
    "section": "ALG-LIN",
    "text": "ALG-LIN\nYou should be able to:\n\nExplain the process of the linear regression algorithm\nDefine the following terms: objective function, cost function, gradient descent, learning rate\nInterpret gradients and give suggestions for possible model improvement"
  },
  {
    "objectID": "06-LIN/std-LIN.html#flow-lin",
    "href": "06-LIN/std-LIN.html#flow-lin",
    "title": "Machine Learning",
    "section": "FLOW-LIN",
    "text": "FLOW-LIN\nYou should be able to:\n\nAnalyze and critique a given five-step workflow using linear models"
  },
  {
    "objectID": "06-LIN/std-LIN.html#study-tips",
    "href": "06-LIN/std-LIN.html#study-tips",
    "title": "Machine Learning",
    "section": "Study Tips",
    "text": "Study Tips\nReview the lecture slides, workbooks, homework assignments, and the associated lab.\nQuiz questions are similar (though not identical) to the types of questions covered in these materials. You should be able to use appropriate terminology and apply the concepts covered in class to a variety of new scenarios.\nThe primary goal of each quiz is to give you an opportunity to apply concepts with clear and well-explained reasoning.\nQuizzes are also a valuable opportunity to receive feedback on your reasoning skills."
  },
  {
    "objectID": "03-CLS/rr-CLS.html",
    "href": "03-CLS/rr-CLS.html",
    "title": "CLS readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. These articles can be quite math-heavy. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\nUnsupervised Learning section 12.1 \nPCA section 12.2 \nClustering Models section 12.4\n\n\n\n\n\nDecision making with AI (few years old) gives real-world examples of the ethical dilemmas of ML \nRecommender system tutorial(won’t cover in class but interesting unsupervised learning model) which you can get a better idea of how it works by writing out the 5-step ML workflow and fill in the steps you see being taken in the tutorial."
  },
  {
    "objectID": "03-CLS/rr-CLS.html#islp-selected-readings",
    "href": "03-CLS/rr-CLS.html#islp-selected-readings",
    "title": "CLS readings",
    "section": "",
    "text": "Dive deeper in the content we’ll be covering. These articles can be quite math-heavy. If anything is unclear, don’t hesitate to reach out to your instructor. Aim to grasp the big-picture ideas so you can explain each topic to a non-technical audience. \n\nUnsupervised Learning section 12.1 \nPCA section 12.2 \nClustering Models section 12.4"
  },
  {
    "objectID": "03-CLS/rr-CLS.html#related-articles",
    "href": "03-CLS/rr-CLS.html#related-articles",
    "title": "CLS readings",
    "section": "",
    "text": "Decision making with AI (few years old) gives real-world examples of the ethical dilemmas of ML \nRecommender system tutorial(won’t cover in class but interesting unsupervised learning model) which you can get a better idea of how it works by writing out the 5-step ML workflow and fill in the steps you see being taken in the tutorial."
  },
  {
    "objectID": "03-CLS/hw-CLS.html",
    "href": "03-CLS/hw-CLS.html",
    "title": "Machine Learning",
    "section": "",
    "text": "For the following calculations, you can reuse the functions you build in your in-class workbooks."
  },
  {
    "objectID": "03-CLS/std-CLS.html",
    "href": "03-CLS/std-CLS.html",
    "title": "Machine Learning",
    "section": "",
    "text": "The following standards will be assessed on the CLS quiz:"
  },
  {
    "objectID": "03-CLS/std-CLS.html#alg-cls",
    "href": "03-CLS/std-CLS.html#alg-cls",
    "title": "Machine Learning",
    "section": "ALG-CLS",
    "text": "ALG-CLS\nYou should be able to:\n\nDefine the unsupervised learning approach and identify it’s associated algorithms\nExplain these algorithms and their associated terms to a non-technical audience\nIdentify when to use various unsupervised algorithms\nDefine intra vs inter cluster distances and it’s relevance to the k-means clustering algorithm"
  },
  {
    "objectID": "03-CLS/std-CLS.html#evl-cls",
    "href": "03-CLS/std-CLS.html#evl-cls",
    "title": "Machine Learning",
    "section": "EVL-CLS",
    "text": "EVL-CLS\nYou should be able to:\n\nInterpret clustering results and identify which clusters are related to ground truths\nCalculate and interpret silhouette coefficients\nIdentify good choices for cluster size \\(k\\)"
  },
  {
    "objectID": "03-CLS/std-CLS.html#study-tips",
    "href": "03-CLS/std-CLS.html#study-tips",
    "title": "Machine Learning",
    "section": "Study Tips",
    "text": "Study Tips\nReview the lecture slides, workbooks, homework assignments, and the associated lab.\nQuiz questions are similar (though not identical) to the types of questions covered in these materials. You should be able to use appropriate terminology and apply the concepts covered in class to a variety of new scenarios.\nThe primary goal of each quiz is to give you an opportunity to apply concepts with clear and well-explained reasoning.\nQuizzes are also a valuable opportunity to receive feedback on your reasoning skills."
  },
  {
    "objectID": "03-CLS/std-CLS.html#formulas-given",
    "href": "03-CLS/std-CLS.html#formulas-given",
    "title": "Machine Learning",
    "section": "Formulas given",
    "text": "Formulas given\n\\(\\text{Silhouette coefficient} = \\frac{b-a}{max(a,b)}\\) \\(\\text{Euclidean distance} = \\sqrt{(x_2 - x_1)^2+ (y_2 - y_1)^2}\\)"
  },
  {
    "objectID": "01-EDA/stats_resource.html#setup",
    "href": "01-EDA/stats_resource.html#setup",
    "title": "Statistics for Machine Learning",
    "section": "Setup",
    "text": "Setup\n\n# Grab the libraries we want to use\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Clear out pesky warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better visibility\nplt.style.use('ggplot')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n\n\n\nLibraries loaded successfully!\nNumPy version: 2.3.4\nPandas version: 2.3.3\n\n\nYou can update the style to your liking by looking at this matplotlib style guide."
  },
  {
    "objectID": "01-EDA/stats_resource.html#introduction",
    "href": "01-EDA/stats_resource.html#introduction",
    "title": "Statistics for Machine Learning",
    "section": "Introduction",
    "text": "Introduction\n\n\nKey Topics\n\nExploratory Data Analysis\nData & Sampling Distributions\nStatistical Inference\nPractical Applications\n\n\nWhy This Matters\n\nData science merges statistics, CS, and domain knowledge\nUnderstanding variability is crucial\nModern tools enable practical analysis"
  },
  {
    "objectID": "01-EDA/stats_resource.html#data-types",
    "href": "01-EDA/stats_resource.html#data-types",
    "title": "Statistics for Machine Learning",
    "section": "Data Types",
    "text": "Data Types\n\n\nNumeric Data\n\nContinuous: Any value\n\n25.2, 24, 29.131414\nUseful for precise measurements and calculations\n\nDiscrete: Integer values\n\n25, 24, 29\nUseful for counts\n\n\n\nCategorical Data\n\nCategorical: Fixed set of values\n\n'Dog', 'Cat', 'Bat'\ntypical for factors/ bins / categories\n\nBinary: Two categories\n\n0, 1 or True, False\nUseful for indicators\n\nOrdinal: Ordered categories\n\n'low', 'medium', 'high'\ncategories more similar to neighbor categories"
  },
  {
    "objectID": "01-EDA/stats_resource.html#data-types-in-python",
    "href": "01-EDA/stats_resource.html#data-types-in-python",
    "title": "Statistics for Machine Learning",
    "section": "Data Types in Python",
    "text": "Data Types in Python\n\n# Create sample data\ndata = {\n    'age': [25, 30, 35, 40],  # Continuous\n    'visits': [3, 5, 2, 7],  # Discrete\n    'category': ['A', 'B', 'A', 'C'],  # Categorical\n    'is_member': [True, False, True, True],  # Binary\n    'rating': ['low', 'medium', 'high', 'medium']  # Ordinal\n}\n\ndf = pd.DataFrame(data)\n\n\n\n\nSince this is a small dataset, we can display the entire thing:\n   age  visits category  is_member  rating\n0   25       3        A       True     low\n1   30       5        B      False  medium\n2   35       2        A       True    high\n3   40       7        C       True  medium\n\nIf this was a larger dataset, use `df.head()` to see the first couple of entries\n\nWe can also see the data type for each feature (column) using `df.dtypes`:\nage           int64\nvisits        int64\ncategory     object\nis_member      bool\nrating       object\ndtype: object"
  },
  {
    "objectID": "01-EDA/stats_resource.html#tabular-data-structure",
    "href": "01-EDA/stats_resource.html#tabular-data-structure",
    "title": "Statistics for Machine Learning",
    "section": "Tabular Data Structure",
    "text": "Tabular Data Structure\nKey Terms\n\nData Frame: Basic structure for analysis\nFeature: A column (attribute, predictor, variable)\nRecord: A row (case, observation, sample)\nOutcome: Target variable to predict"
  },
  {
    "objectID": "01-EDA/stats_resource.html#estimates-of-location",
    "href": "01-EDA/stats_resource.html#estimates-of-location",
    "title": "Statistics for Machine Learning",
    "section": "Estimates of Location",
    "text": "Estimates of Location\n\nMean: Average value which is sensitive to outliers (extreme values) \\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nMedian: Middle value (50th percentile) which is more robust to outliers \\[\n\\text{Median} =\n\\begin{cases}\nx_{\\frac{n+1}{2}}, & n \\text{ odd} \\\\[2mm]\n\\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}, & n \\text{ even}\n\\end{cases}\n\\]\nTrimmed Mean: Average after removing extremes\n\\[\n\\bar{x}_{\\text{trim}} = \\frac{1}{n - 2k} \\sum_{i=k+1}^{n-k} x_{(i)}\n\\] where (k) values are removed from each end."
  },
  {
    "objectID": "01-EDA/stats_resource.html#computing-location-estimates",
    "href": "01-EDA/stats_resource.html#computing-location-estimates",
    "title": "Statistics for Machine Learning",
    "section": "Computing Location Estimates",
    "text": "Computing Location Estimates\n\n# Sample data: state populations (in millions)\npopulation = np.array([4.78, 0.71, 6.39, 2.92, 37.25, \n                       5.03, 3.57, 0.90])\n\nmean_pop = np.mean(population)\n\nmedian_pop = np.median(population)\n\ndef trim_mean(data, trim_percent):\n    n = len(data)\n    k = int(n * trim_percent)\n    sorted_data = np.sort(data)\n    return np.mean(sorted_data[k:n-k])\n\ntrimmed_mean = trim_mean(population, 0.1) #trimming 10%\n\n\n\n\nPopulation data: [ 4.78  0.71  6.39  2.92 37.25  5.03  3.57  0.9 ]\n\nMean: 7.69M\nMedian: 4.17M\nTrimmed Mean (10%): 7.69M\n\nNote: Mean &gt; Trimmed Mean &gt; Median due to outliers"
  },
  {
    "objectID": "01-EDA/stats_resource.html#weighted-mean",
    "href": "01-EDA/stats_resource.html#weighted-mean",
    "title": "Statistics for Machine Learning",
    "section": "Weighted Mean",
    "text": "Weighted Mean\n\n# Murder rates and populations by state\nmurder_rate = np.array([5.7, 5.6, 4.7, 5.6, 4.4])\npopulation = np.array([4.78, 0.71, 6.39, 2.92, 37.25])\nstates = ['AL', 'AK', 'AZ', 'AR', 'CA']\n\n# Simple mean (treats all states equally)\nsimple_mean = np.mean(murder_rate)\n\n# Weighted mean (accounts for population)\nweighted_mean = np.average(murder_rate, weights=population)\n\n\n\n\nState Data:\n  AL: Rate=5.7, Pop=4.78M\n  AK: Rate=5.6, Pop=0.71M\n  AZ: Rate=4.7, Pop=6.39M\n  AR: Rate=5.6, Pop=2.92M\n  CA: Rate=4.4, Pop=37.25M\n\nSimple Mean Murder Rate: 5.20\nWeighted Mean Murder Rate: 4.64\n\nWeighted mean is lower because CA (largest state) has lowest rate"
  },
  {
    "objectID": "01-EDA/stats_resource.html#estimates-of-variability",
    "href": "01-EDA/stats_resource.html#estimates-of-variability",
    "title": "Statistics for Machine Learning",
    "section": "Estimates of Variability",
    "text": "Estimates of Variability\n\nRange: Difference between maximum and minimum\n\\[\n\\text{Range} = x_{\\text{max}} - x_{\\text{min}}\n\\]\nInterquartile Range (IQR): Difference between 75th and 25th percentile\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\nVariance: Average squared deviation from mean\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\nStandard Deviation: Square root of variance\nMean Absolute Deviation (MAD): Average absolute deviation\n\\[\n\\text{MAD} = \\frac{1}{n} \\sum_{i=1}^{n} \\big| x_i - \\bar{x} \\big|\n\\]"
  },
  {
    "objectID": "01-EDA/stats_resource.html#computing-variability",
    "href": "01-EDA/stats_resource.html#computing-variability",
    "title": "Statistics for Machine Learning",
    "section": "Computing Variability",
    "text": "Computing Variability\n\ndata = np.array([1, 4, 4, 7, 10, 15])\n\ndata_range = np.max(data) - np.min(data)\nq75, q25 = np.percentile(data, [75, 25])\niqr = q75 - q25\nvariance = np.var(data, ddof=1)  # ddof=1 for sample variance\nstd_dev = np.std(data, ddof=1)\nmad = np.mean(np.abs(data - np.mean(data)))\n\n\n\n\nData: [ 1  4  4  7 10 15]\n\nMeasures of Variability:\n  Variance: 25.37\n  Standard Deviation: 5.04\n  Mean Absolute Deviation: 3.83\n  Range: 14.00\n  IQR (Q75 - Q25): 5.25"
  },
  {
    "objectID": "01-EDA/stats_resource.html#interpreting-variability",
    "href": "01-EDA/stats_resource.html#interpreting-variability",
    "title": "Statistics for Machine Learning",
    "section": "Interpreting Variability",
    "text": "Interpreting Variability\nSo what did all those values even mean?\nThe numbers in our dataset are spread out, but not too wildly.\nThe range of 14 tells us how far apart the smallest and largest numbers are.\nLooking closer, the middle half of the numbers (the IQR of 6) are much closer together, which means a few unusually high or low numbers are stretching the range.\nMeasures like variance (36.80) and standard deviation (6.07) give a sense of how much the numbers tend to differ from the average, while the mean absolute deviation (4.83) shows a typical “typical difference” in a way that isn’t affected as much by extreme values.\nAltogether, this tells us that while there are a few numbers that are far from the rest, most of the data is relatively clustered near the middle."
  },
  {
    "objectID": "01-EDA/stats_resource.html#percentiles-and-boxplots",
    "href": "01-EDA/stats_resource.html#percentiles-and-boxplots",
    "title": "Statistics for Machine Learning",
    "section": "Percentiles and Boxplots",
    "text": "Percentiles and Boxplots\n\nnp.random.seed(42)\ndata = np.random.exponential(scale=2, size=100)\npercentiles = np.percentile(data, [5, 25, 50, 75, 95])\n\n\n\nPercentiles (5th, 25th, 50th, 75th, 95th):\n  5th percentile: 0.09\n  25th percentile: 0.43\n  50th percentile: 1.25\n  75th percentile: 2.62\n  95th percentile: 5.95\n\n\n\n\nShow/Hide Code\nfig, ax = plt.subplots(figsize=(10, 3))\nbp = ax.boxplot(data, vert=False, widths=0.5)\nax.set_xlabel('Value')\nax.set_title('Boxplot of Exponential Distribution')\nax.grid(alpha=0.3)\n\n# Add percentile markers\nfor p, val in zip([25, 50, 75], [percentiles[1], percentiles[2], percentiles[3]]):\n    ax.axvline(val, color='red', linestyle='--', alpha=0.5, linewidth=1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "01-EDA/stats_resource.html#frequency-tables-and-histograms",
    "href": "01-EDA/stats_resource.html#frequency-tables-and-histograms",
    "title": "Statistics for Machine Learning",
    "section": "Frequency Tables and Histograms",
    "text": "Frequency Tables and Histograms\n\nnp.random.seed(42)\ndata = np.random.normal(100, 15, 1000)\n\n\n\nShow/Hide Code\nfig, ax = plt.subplots(figsize=(10, 3.5))\ncounts, bins, patches = ax.hist(data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\nax.set_xlabel('Value')\nax.set_ylabel('Frequency')\nax.set_title('Histogram of Normal Distribution (μ=100, σ=15)')\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency Table (first 5 bins):\n  [51.4, 54.9): 1 observations\n  [54.9, 58.5): 0 observations\n  [58.5, 62.0): 3 observations\n  [62.0, 65.6): 3 observations\n  [65.6, 69.1): 6 observations\n\nTotal observations: 1000\nMean: 100.29, Std: 14.69"
  },
  {
    "objectID": "01-EDA/stats_resource.html#density-plots",
    "href": "01-EDA/stats_resource.html#density-plots",
    "title": "Statistics for Machine Learning",
    "section": "Density Plots",
    "text": "Density Plots\n\n\nShow/Hide Code\n# Kernel density estimation (simple implementation)\ndef kde(data, x_eval, bandwidth=0.5):\n    \"\"\"Simple Gaussian kernel density estimation\"\"\"\n    n = len(data)\n    density = np.zeros_like(x_eval, dtype=float)\n    for xi in data:\n        kernel = np.exp(-0.5 * ((x_eval - xi) / bandwidth) ** 2)\n        kernel /= (bandwidth * np.sqrt(2 * np.pi))\n        density += kernel\n    return density / n\n\n# Generate data\nnp.random.seed(42)\ndata = np.random.normal(100, 15, 1000)\n\n# Generate density estimate\nxs = np.linspace(data.min(), data.max(), 200)\ndensity = kde(data, xs, bandwidth=3)\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.hist(data, bins=30, density=True, alpha=0.5, \n        edgecolor='black', label='Histogram', color='lightblue')\nax.plot(xs, density, 'r-', linewidth=2, label='Density Estimate')\nax.set_xlabel('Value')\nax.set_ylabel('Density')\nax.set_title('Density Plot: Smoothed Distribution')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nDensity estimation smooths the histogram. Bandwidth parameter controls smoothness"
  },
  {
    "objectID": "01-EDA/stats_resource.html#correlation",
    "href": "01-EDA/stats_resource.html#correlation",
    "title": "Statistics for Machine Learning",
    "section": "Correlation",
    "text": "Correlation\nPearson Correlation Coefficient\n\nMeasures linear association between variables\nRanges from -1 (perfect negative) to +1 (perfect positive)\n0 indicates no linear correlation\n\n\\[r = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y})}{(N-1)s_x s_y}\\]"
  },
  {
    "objectID": "01-EDA/stats_resource.html#computing-correlation",
    "href": "01-EDA/stats_resource.html#computing-correlation",
    "title": "Statistics for Machine Learning",
    "section": "Computing Correlation",
    "text": "Computing Correlation\n\n# Generate correlated data\nnp.random.seed(42)\nx = np.random.normal(0, 1, 100)\ny = 2 * x + np.random.normal(0, 0.5, 100)\n\n# Correlation coefficient\ncorrelation = np.corrcoef(x, y)[0, 1]\nprint(f\"Correlation between x and y: {correlation:.3f}\")\nprint(f\"Interpretation: Strong positive linear relationship\")\n\n# Correlation matrix for multiple variables\nz = x + y  # Create third variable\ndata_matrix = np.column_stack([x, y, z])\ncorr_matrix = np.corrcoef(data_matrix.T)\n\nprint(\"\\nCorrelation Matrix:\")\nprint(\"      x      y      z\")\nfor i, row in enumerate(corr_matrix):\n    label = ['x', 'y', 'z'][i]\n    print(f\"{label}: {row[0]:6.3f} {row[1]:6.3f} {row[2]:6.3f}\")\n\nprint(\"\\nNote: Correlation ranges from -1 (perfect negative) to +1 (perfect positive)\")\n\nCorrelation between x and y: 0.965\nInterpretation: Strong positive linear relationship\n\nCorrelation Matrix:\n      x      y      z\nx:  1.000  0.965  0.985\ny:  0.965  1.000  0.996\nz:  0.985  0.996  1.000\n\nNote: Correlation ranges from -1 (perfect negative) to +1 (perfect positive)"
  },
  {
    "objectID": "01-EDA/stats_resource.html#scatterplot-with-correlation",
    "href": "01-EDA/stats_resource.html#scatterplot-with-correlation",
    "title": "Statistics for Machine Learning",
    "section": "Scatterplot with Correlation",
    "text": "Scatterplot with Correlation\n\nnp.random.seed(42)\nx = np.random.normal(0, 1, 100)\ny = 2 * x + np.random.normal(0, 0.5, 100)\ncorrelation = np.corrcoef(x, y)[0, 1]\n\nfig, ax = plt.subplots(figsize=(8, 3.5))\nax.scatter(x, y, alpha=0.6, s=50, color='steelblue', edgecolors='black', linewidth=0.5)\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('Y', fontsize=12)\nax.set_title(f'Scatterplot with Correlation (r = {correlation:.3f})', fontsize=14)\nax.grid(alpha=0.3)\n\n# Add regression line\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nx_line = np.linspace(x.min(), x.max(), 100)\nax.plot(x_line, p(x_line), \"r-\", linewidth=2, alpha=0.8, label='Best Fit Line')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Slope of regression line: {z[0]:.3f}\")\nprint(f\"Intercept: {z[1]:.3f}\")\n\n\n\n\n\n\n\n\nSlope of regression line: 1.928\nIntercept: 0.004"
  },
  {
    "objectID": "01-EDA/stats_resource.html#hexagonal-binning",
    "href": "01-EDA/stats_resource.html#hexagonal-binning",
    "title": "Statistics for Machine Learning",
    "section": "Hexagonal Binning",
    "text": "Hexagonal Binning\n\n# Large dataset example\nnp.random.seed(42)\nn = 10000\nx = np.random.normal(0, 1, n)\ny = 2 * x + np.random.normal(0, 1, n)\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nhb = ax.hexbin(x, y, gridsize=30, cmap='Blues', mincnt=1, edgecolors='black', linewidths=0.2)\ncb = plt.colorbar(hb, ax=ax, label='Count')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title(f'Hexagonal Binning for Large Datasets (n={n:,})')\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total data points: {n:,}\")\nprint(f\"Correlation: {np.corrcoef(x, y)[0,1]:.3f}\")\nprint(f\"\\nHexagonal binning aggregates dense point clouds into bins\")\nprint(f\"Useful when scatterplots become too dense to interpret\")\n\n\n\n\n\n\n\n\nTotal data points: 10,000\nCorrelation: 0.894\n\nHexagonal binning aggregates dense point clouds into bins\nUseful when scatterplots become too dense to interpret"
  },
  {
    "objectID": "01-EDA/stats_resource.html#categorical-data-bar-charts",
    "href": "01-EDA/stats_resource.html#categorical-data-bar-charts",
    "title": "Statistics for Machine Learning",
    "section": "Categorical Data: Bar Charts",
    "text": "Categorical Data: Bar Charts\n\n# Flight delay causes\ncauses = ['Carrier', 'ATC', 'Weather', 'Security', 'Inbound']\npercentages = [23.02, 30.40, 4.03, 0.12, 42.43]\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nbars = ax.bar(causes, percentages, color='steelblue', edgecolor='black', alpha=0.8)\nax.set_ylabel('Percentage (%)', fontsize=12)\nax.set_title('Airline Delays by Cause at DFW Airport', fontsize=14)\nax.grid(axis='y', alpha=0.3)\n\n# Add percentage labels on bars\nfor bar, pct in zip(bars, percentages):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{pct:.1f}%', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Delay Causes Summary:\")\nfor cause, pct in zip(causes, percentages):\n    print(f\"  {cause:12s}: {pct:5.2f}%\")\nprint(f\"\\nTotal: {sum(percentages):.2f}%\")\n\n\n\n\n\n\n\n\nDelay Causes Summary:\n  Carrier     : 23.02%\n  ATC         : 30.40%\n  Weather     :  4.03%\n  Security    :  0.12%\n  Inbound     : 42.43%\n\nTotal: 100.00%"
  },
  {
    "objectID": "01-EDA/stats_resource.html#contingency-tables",
    "href": "01-EDA/stats_resource.html#contingency-tables",
    "title": "Statistics for Machine Learning",
    "section": "Contingency Tables",
    "text": "Contingency Tables\n\n# Create sample loan data\nnp.random.seed(42)\ngrades = np.random.choice(['A', 'B', 'C', 'D'], size=1000, \n                          p=[0.2, 0.3, 0.3, 0.2])\nstatus = np.random.choice(['Paid', 'Current', 'Late', 'Charged Off'],\n                          size=1000, p=[0.25, 0.65, 0.05, 0.05])\n\n# Create contingency table\nct = pd.crosstab(grades, status, margins=True)\nprint(\"Contingency Table (Counts):\")\nprint(ct)\n\n# With row percentages\nct_pct = pd.crosstab(grades, status, normalize='index') * 100\nprint(\"\\n\\nRow Percentages (by Grade):\")\nprint(ct_pct.round(1))\n\nprint(\"\\n\\nInterpretation:\")\nprint(\"Each cell shows percentage of loans in that grade with that status\")\nprint(\"Example: What % of Grade A loans are Current, Paid, Late, etc.?\")\n\nContingency Table (Counts):\ncol_0  Charged Off  Current  Late  Paid   All\nrow_0                                        \nA               12      143     9    61   225\nB               14      174    14    76   278\nC               20      196    10    72   298\nD               13      132     5    49   199\nAll             59      645    38   258  1000\n\n\nRow Percentages (by Grade):\ncol_0  Charged Off  Current  Late  Paid\nrow_0                                  \nA              5.3     63.6   4.0  27.1\nB              5.0     62.6   5.0  27.3\nC              6.7     65.8   3.4  24.2\nD              6.5     66.3   2.5  24.6\n\n\nInterpretation:\nEach cell shows percentage of loans in that grade with that status\nExample: What % of Grade A loans are Current, Paid, Late, etc.?"
  },
  {
    "objectID": "01-EDA/stats_resource.html#violin-plots",
    "href": "01-EDA/stats_resource.html#violin-plots",
    "title": "Statistics for Machine Learning",
    "section": "Violin Plots",
    "text": "Violin Plots\n\n# Compare distributions across groups\nnp.random.seed(42)\ngroup_a = np.random.normal(10, 2, 100)\ngroup_b = np.random.normal(12, 3, 100)\ngroup_c = np.random.normal(11, 1.5, 100)\n\ndata_violin = [group_a, group_b, group_c]\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nparts = ax.violinplot(data_violin, positions=[1, 2, 3], \n                      showmeans=True, showmedians=True)\nax.set_xticks([1, 2, 3])\nax.set_xticklabels(['Group A', 'Group B', 'Group C'])\nax.set_ylabel('Value', fontsize=12)\nax.set_title('Violin Plot: Comparing Distribution Shapes', fontsize=14)\nax.grid(alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprint(\"Summary Statistics by Group:\")\nfor name, data in [('A', group_a), ('B', group_b), ('C', group_c)]:\n    print(f\"  Group {name}: Mean={np.mean(data):.2f}, Std={np.std(data, ddof=1):.2f}\")\nprint(\"\\nViolin width shows density at each value level\")\n\n\n\n\n\n\n\n\nSummary Statistics by Group:\n  Group A: Mean=9.79, Std=1.82\n  Group B: Mean=12.07, Std=2.86\n  Group C: Mean=11.10, Std=1.63\n\nViolin width shows density at each value level"
  },
  {
    "objectID": "01-EDA/stats_resource.html#random-sampling-concepts",
    "href": "01-EDA/stats_resource.html#random-sampling-concepts",
    "title": "Statistics for Machine Learning",
    "section": "Random Sampling Concepts",
    "text": "Random Sampling Concepts\nKey Terms\n\nPopulation: The complete dataset (real or theoretical)\nSample: A subset drawn from the population\nRandom Sampling: Each element has equal chance of selection\nSample Bias: Sample systematically differs from population\nStratified Sampling: Random sampling from defined subgroups"
  },
  {
    "objectID": "01-EDA/stats_resource.html#random-sampling-in-python",
    "href": "01-EDA/stats_resource.html#random-sampling-in-python",
    "title": "Statistics for Machine Learning",
    "section": "Random Sampling in Python",
    "text": "Random Sampling in Python\n\n# Create a population\nnp.random.seed(42)\npopulation = np.random.normal(100, 15, 10000)\n\n# Simple random sample\nsample_size = 100\nsimple_sample = np.random.choice(population, size=sample_size, \n                                 replace=False)\n\nprint(f\"Population size: {len(population):,}\")\nprint(f\"Sample size: {sample_size}\")\nprint(f\"\\nPopulation Mean: {np.mean(population):.2f}\")\nprint(f\"Population Std Dev: {np.std(population, ddof=1):.2f}\")\nprint(f\"\\nSample Mean: {np.mean(simple_sample):.2f}\")\nprint(f\"Sample Std Dev: {np.std(simple_sample, ddof=1):.2f}\")\n\n# Show difference\ndiff = abs(np.mean(simple_sample) - np.mean(population))\nprint(f\"\\nDifference in means: {diff:.2f}\")\nprint(f\"Sampling error (expected): ~{15/np.sqrt(sample_size):.2f}\")\n\nPopulation size: 10,000\nSample size: 100\n\nPopulation Mean: 99.97\nPopulation Std Dev: 15.05\n\nSample Mean: 101.75\nSample Std Dev: 16.06\n\nDifference in means: 1.78\nSampling error (expected): ~1.50"
  },
  {
    "objectID": "01-EDA/stats_resource.html#stratified-sampling",
    "href": "01-EDA/stats_resource.html#stratified-sampling",
    "title": "Statistics for Machine Learning",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\n\n# Create stratified population (3 distinct groups)\nnp.random.seed(42)\nstratum_a = np.random.normal(100, 10, 7000)  # 70% of population\nstratum_b = np.random.normal(120, 15, 2000)  # 20% of population\nstratum_c = np.random.normal(90, 12, 1000)   # 10% of population\n\nfull_population = np.concatenate([stratum_a, stratum_b, stratum_c])\ntrue_mean = np.mean(full_population)\n\n# Sample from each stratum proportionally\nn = 300\nsample_a = np.random.choice(stratum_a, size=int(n*0.7))\nsample_b = np.random.choice(stratum_b, size=int(n*0.2))\nsample_c = np.random.choice(stratum_c, size=int(n*0.1))\n\nstratified_sample = np.concatenate([sample_a, sample_b, sample_c])\n\n# Compare with simple random sample\nsimple_sample = np.random.choice(full_population, size=n)\n\nprint(f\"True Population Mean: {true_mean:.2f}\")\nprint(f\"Stratified Sample Mean: {np.mean(stratified_sample):.2f}\")\nprint(f\"Simple Random Sample Mean: {np.mean(simple_sample):.2f}\")\nprint(f\"\\nStratified sampling ensures representation from all groups\")\n\nTrue Population Mean: 103.00\nStratified Sample Mean: 102.56\nSimple Random Sample Mean: 101.53\n\nStratified sampling ensures representation from all groups"
  },
  {
    "objectID": "01-EDA/stats_resource.html#sampling-distribution",
    "href": "01-EDA/stats_resource.html#sampling-distribution",
    "title": "Statistics for Machine Learning",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\nThe sampling distribution is the distribution of a sample statistic over many samples.\nKey Insight: Even if individual data isn’t normally distributed, sample means tend toward normality (Central Limit Theorem)"
  },
  {
    "objectID": "01-EDA/stats_resource.html#demonstrating-sampling-distribution",
    "href": "01-EDA/stats_resource.html#demonstrating-sampling-distribution",
    "title": "Statistics for Machine Learning",
    "section": "Demonstrating Sampling Distribution",
    "text": "Demonstrating Sampling Distribution\n\n# Draw many samples and compute means\nnp.random.seed(42)\npopulation = np.random.exponential(scale=2, size=10000)\nsample_means = [np.mean(np.random.choice(population, 30)) \n                for _ in range(1000)]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\n\naxes[0].hist(population, bins=50, edgecolor='black', alpha=0.7, color='lightcoral')\naxes[0].set_title('Population Distribution (Exponential)', fontsize=12)\naxes[0].set_xlabel('Value')\naxes[0].set_ylabel('Frequency')\n\naxes[1].hist(sample_means, bins=30, edgecolor='black', alpha=0.7, color='lightblue')\naxes[1].set_title('Sampling Distribution of Mean (n=30, 1000 samples)', fontsize=12)\naxes[1].set_xlabel('Sample Mean')\naxes[1].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Population mean: {np.mean(population):.3f}\")\nprint(f\"Mean of sample means: {np.mean(sample_means):.3f}\")\nprint(f\"Std dev of sample means: {np.std(sample_means):.3f}\")\nprint(f\"\\nNote: Even though population is skewed, sampling distribution is normal!\")\n\n\n\n\n\n\n\n\nPopulation mean: 1.955\nMean of sample means: 1.968\nStd dev of sample means: 0.343\n\nNote: Even though population is skewed, sampling distribution is normal!"
  },
  {
    "objectID": "01-EDA/stats_resource.html#central-limit-theorem",
    "href": "01-EDA/stats_resource.html#central-limit-theorem",
    "title": "Statistics for Machine Learning",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT states: As sample size increases, the sampling distribution of the mean approaches a normal distribution, regardless of the population’s distribution.\nRequirements: - Large enough sample size (typically n &gt; 30) - Population not too heavily skewed"
  },
  {
    "objectID": "01-EDA/stats_resource.html#standard-error",
    "href": "01-EDA/stats_resource.html#standard-error",
    "title": "Statistics for Machine Learning",
    "section": "Standard Error",
    "text": "Standard Error\nStandard Error (SE): The standard deviation of a sampling distribution\n\\[SE = \\frac{s}{\\sqrt{n}}\\]\nWhere: - s = sample standard deviation - n = sample size\nSquare-root of n rule: To reduce SE by half, need 4× the sample size"
  },
  {
    "objectID": "01-EDA/stats_resource.html#computing-standard-error",
    "href": "01-EDA/stats_resource.html#computing-standard-error",
    "title": "Statistics for Machine Learning",
    "section": "Computing Standard Error",
    "text": "Computing Standard Error\n\n# Generate sample\nnp.random.seed(42)\nsample = np.random.normal(100, 15, 50)\n\n# Calculate standard error (formula-based)\nse_formula = np.std(sample, ddof=1) / np.sqrt(len(sample))\n\nprint(f\"Sample size: {len(sample)}\")\nprint(f\"Sample Mean: {np.mean(sample):.2f}\")\nprint(f\"Sample Std Dev: {np.std(sample, ddof=1):.2f}\")\nprint(f\"Standard Error (formula): {se_formula:.2f}\")\n\n# Verify with multiple samples (empirical approach)\nsample_means = [np.mean(np.random.normal(100, 15, 50)) \n                for _ in range(1000)]\nempirical_se = np.std(sample_means)\n\nprint(f\"\\nEmpirical SE (from 1000 samples): {empirical_se:.2f}\")\nprint(f\"\\nThey match! SE = σ / √n = 15 / √50 ≈ {15/np.sqrt(50):.2f}\")\n\n# Demonstrate square-root of n rule\nprint(f\"\\nSquare-root of n rule:\")\nprint(f\"  To halve SE, need 4× sample size: {se_formula:.2f} → {se_formula/2:.2f}\")\nprint(f\"  New sample size needed: {50 * 4} (was 50)\")\n\nSample size: 50\nSample Mean: 96.62\nSample Std Dev: 14.01\nStandard Error (formula): 1.98\n\nEmpirical SE (from 1000 samples): 2.15\n\nThey match! SE = σ / √n = 15 / √50 ≈ 2.12\n\nSquare-root of n rule:\n  To halve SE, need 4× sample size: 1.98 → 0.99\n  New sample size needed: 200 (was 50)"
  },
  {
    "objectID": "01-EDA/stats_resource.html#the-bootstrap",
    "href": "01-EDA/stats_resource.html#the-bootstrap",
    "title": "Statistics for Machine Learning",
    "section": "The Bootstrap",
    "text": "The Bootstrap\nBootstrap Resampling: Sample WITH replacement from observed data to estimate sampling distribution\nAlgorithm: 1. Draw n samples with replacement from original data 2. Calculate statistic of interest 3. Repeat B times (e.g., 1000) 4. Use distribution of statistics for inference"
  },
  {
    "objectID": "01-EDA/stats_resource.html#bootstrap-implementation",
    "href": "01-EDA/stats_resource.html#bootstrap-implementation",
    "title": "Statistics for Machine Learning",
    "section": "Bootstrap Implementation",
    "text": "Bootstrap Implementation\n\n# Original sample\nnp.random.seed(42)\noriginal_sample = np.random.normal(100, 15, 50)\n\n# Bootstrap resampling\nn_bootstrap = 1000\nbootstrap_means = []\n\nfor _ in range(n_bootstrap):\n    resample = np.random.choice(original_sample, \n                                size=len(original_sample), \n                                replace=True)  # WITH replacement!\n    bootstrap_means.append(np.mean(resample))\n\nbootstrap_means = np.array(bootstrap_means)\n\n# Plot results\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.hist(bootstrap_means, bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\nax.axvline(np.mean(original_sample), color='red', \n           linestyle='--', linewidth=2, label='Original Sample Mean')\nax.set_xlabel('Bootstrap Mean')\nax.set_ylabel('Frequency')\nax.set_title(f'Bootstrap Distribution ({n_bootstrap} resamples)')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original Sample Mean: {np.mean(original_sample):.2f}\")\nprint(f\"Bootstrap SE: {np.std(bootstrap_means):.2f}\")\nprint(f\"Formula SE: {np.std(original_sample, ddof=1)/np.sqrt(50):.2f}\")\n\n\n\n\n\n\n\n\nOriginal Sample Mean: 96.62\nBootstrap SE: 2.00\nFormula SE: 1.98"
  },
  {
    "objectID": "01-EDA/stats_resource.html#confidence-intervals",
    "href": "01-EDA/stats_resource.html#confidence-intervals",
    "title": "Statistics for Machine Learning",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nConfidence Interval: A range that likely contains the true population parameter\n90% CI from Bootstrap: - Take the 5th and 95th percentiles of bootstrap distribution - Interpretation: If we repeated this process many times, 90% of intervals would contain the true parameter"
  },
  {
    "objectID": "01-EDA/stats_resource.html#computing-confidence-intervals",
    "href": "01-EDA/stats_resource.html#computing-confidence-intervals",
    "title": "Statistics for Machine Learning",
    "section": "Computing Confidence Intervals",
    "text": "Computing Confidence Intervals\n\n# Use bootstrap distribution from previous slide\nnp.random.seed(42)\noriginal_sample = np.random.normal(100, 15, 50)\nbootstrap_means = [np.mean(np.random.choice(original_sample, \n                           len(original_sample), replace=True))\n                   for _ in range(1000)]\n\n# 90% Confidence interval\nci_90_lower = np.percentile(bootstrap_means, 5)\nci_90_upper = np.percentile(bootstrap_means, 95)\n\n# 95% Confidence interval\nci_95_lower = np.percentile(bootstrap_means, 2.5)\nci_95_upper = np.percentile(bootstrap_means, 97.5)\n\nprint(f\"Original Sample Mean: {np.mean(original_sample):.2f}\")\nprint(f\"\\n90% Confidence Interval: ({ci_90_lower:.2f}, {ci_90_upper:.2f})\")\nprint(f\"  Width: {ci_90_upper - ci_90_lower:.2f}\")\nprint(f\"\\n95% Confidence Interval: ({ci_95_lower:.2f}, {ci_95_upper:.2f})\")\nprint(f\"  Width: {ci_95_upper - ci_95_lower:.2f}\")\n\nprint(f\"\\nInterpretation:\")\nprint(f\"We are 95% confident the true population mean lies between\")\nprint(f\"{ci_95_lower:.2f} and {ci_95_upper:.2f}\")\nprint(f\"\\nNote: Higher confidence → wider interval\")\n\nOriginal Sample Mean: 96.62\n\n90% Confidence Interval: (93.43, 99.96)\n  Width: 6.52\n\n95% Confidence Interval: (92.73, 100.68)\n  Width: 7.95\n\nInterpretation:\nWe are 95% confident the true population mean lies between\n92.73 and 100.68\n\nNote: Higher confidence → wider interval"
  },
  {
    "objectID": "01-EDA/stats_resource.html#normal-distribution",
    "href": "01-EDA/stats_resource.html#normal-distribution",
    "title": "Statistics for Machine Learning",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n# Normal PDF function\ndef normal_pdf(x, mu=0, sigma=1):\n    \"\"\"Calculate normal probability density function\"\"\"\n    return (1 / (sigma * np.sqrt(2 * np.pi))) * \\\n           np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n\n# Generate normal distribution\nx = np.linspace(-4, 4, 1000)\ny = normal_pdf(x, 0, 1)\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.plot(x, y, 'b-', linewidth=2, label='Standard Normal')\nax.fill_between(x, y, where=(x &gt;= -1) & (x &lt;= 1), \n                alpha=0.3, label='68% (±1 SD)', color='blue')\nax.fill_between(x, y, where=((x &gt;= -2) & (x &lt; -1)) | ((x &gt; 1) & (x &lt;= 2)), \n                alpha=0.2, label='95% (±2 SD)', color='green')\nax.set_xlabel('Z-score (Standard Deviations)')\nax.set_ylabel('Density')\nax.set_title('Standard Normal Distribution')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Standard Normal Properties:\")\nprint(\"  68% of data within ±1 SD\")\nprint(\"  95% of data within ±2 SD\")\nprint(\"  99.7% of data within ±3 SD\")\n\n\n\n\n\n\n\n\nStandard Normal Properties:\n  68% of data within ±1 SD\n  95% of data within ±2 SD\n  99.7% of data within ±3 SD"
  },
  {
    "objectID": "01-EDA/stats_resource.html#q-q-plots",
    "href": "01-EDA/stats_resource.html#q-q-plots",
    "title": "Statistics for Machine Learning",
    "section": "Q-Q Plots",
    "text": "Q-Q Plots\n\ndef qq_plot(data, ax, title):\n    \"\"\"Create Q-Q plot manually\"\"\"\n    standardized = (data - np.mean(data)) / np.std(data, ddof=1)\n    standardized = np.sort(standardized)\n    n = len(standardized)\n    theoretical = np.array([np.percentile(np.random.standard_normal(10000), \n                            100 * (i - 0.5) / n) for i in range(1, n + 1)])\n    ax.scatter(theoretical, standardized, alpha=0.6, s=30)\n    ax.plot([-3, 3], [-3, 3], 'r--', linewidth=2, label='Perfect Normal')\n    ax.set_xlabel('Theoretical Quantiles')\n    ax.set_ylabel('Sample Quantiles')\n    ax.set_title(title)\n    ax.grid(alpha=0.3)\n    ax.legend()\n\nnp.random.seed(42)\nnormal_data = np.random.normal(0, 1, 100)\nexp_data = np.random.exponential(1, 100)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\nqq_plot(normal_data, axes[0], 'Q-Q Plot: Normal Data')\nqq_plot(exp_data, axes[1], 'Q-Q Plot: Exponential Data')\nplt.tight_layout()\nplt.show()\n\nprint(\"Interpretation:\")\nprint(\"  Left: Points on line → data is normally distributed\")\nprint(\"  Right: Points deviate → data is NOT normally distributed\")\n\n\n\n\n\n\n\n\nInterpretation:\n  Left: Points on line → data is normally distributed\n  Right: Points deviate → data is NOT normally distributed"
  },
  {
    "objectID": "01-EDA/stats_resource.html#binomial-distribution",
    "href": "01-EDA/stats_resource.html#binomial-distribution",
    "title": "Statistics for Machine Learning",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nBinomial: Distribution of number of successes in n independent trials\nParameters: - n: number of trials - p: probability of success\nExample: 10 coin flips, probability of getting 6 heads"
  },
  {
    "objectID": "01-EDA/stats_resource.html#binomial-distribution-in-python",
    "href": "01-EDA/stats_resource.html#binomial-distribution-in-python",
    "title": "Statistics for Machine Learning",
    "section": "Binomial Distribution in Python",
    "text": "Binomial Distribution in Python\n\ndef binomial_pmf(k, n, p):\n    \"\"\"Calculate binomial probability mass function\"\"\"\n    def factorial(x):\n        if x &lt;= 1: return 1\n        return x * factorial(x - 1)\n    def comb(n, k):\n        return factorial(n) // (factorial(k) * factorial(n - k))\n    return comb(n, k) * (p ** k) * ((1 - p) ** (n - k))\n\nn = 20  # trials\np = 0.3  # probability of success\n\nx = np.arange(0, n+1)\npmf = np.array([binomial_pmf(k, n, p) for k in x])\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.bar(x, pmf, edgecolor='black', alpha=0.7, color='steelblue')\nax.set_xlabel('Number of Successes')\nax.set_ylabel('Probability')\nax.set_title(f'Binomial Distribution (n={n} trials, p={p} success probability)')\nax.grid(alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprob_5 = binomial_pmf(5, n, p)\nprob_le_5 = sum([binomial_pmf(k, n, p) for k in range(6)])\nprint(f\"P(exactly 5 successes) = {prob_5:.4f}\")\nprint(f\"P(5 or fewer successes) = {prob_le_5:.4f}\")\nprint(f\"Expected value (mean) = n×p = {n*p:.1f}\")\n\n\n\n\n\n\n\n\nP(exactly 5 successes) = 0.1789\nP(5 or fewer successes) = 0.4164\nExpected value (mean) = n×p = 6.0"
  },
  {
    "objectID": "01-EDA/stats_resource.html#poisson-distribution",
    "href": "01-EDA/stats_resource.html#poisson-distribution",
    "title": "Statistics for Machine Learning",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nPoisson: Distribution of events occurring at a constant rate\nParameter: λ (lambda) = average rate\nExamples: - Website visits per hour - Customer calls per minute - Defects per unit"
  },
  {
    "objectID": "01-EDA/stats_resource.html#poisson-distribution-in-python",
    "href": "01-EDA/stats_resource.html#poisson-distribution-in-python",
    "title": "Statistics for Machine Learning",
    "section": "Poisson Distribution in Python",
    "text": "Poisson Distribution in Python\n\nimport math\n\ndef poisson_pmf(k, lam):\n    \"\"\"Calculate Poisson probability mass function\"\"\"\n    return (lam ** k) * np.exp(-lam) / math.factorial(k)\n\nlambda_rate = 3  # average events per interval\n\nx = np.arange(0, 15)\npmf = np.array([poisson_pmf(k, lambda_rate) for k in x])\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.bar(x, pmf, edgecolor='black', alpha=0.7, color='coral')\nax.set_xlabel('Number of Events')\nax.set_ylabel('Probability')\nax.set_title(f'Poisson Distribution (λ={lambda_rate} events/interval)')\nax.grid(alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n# Generate random Poisson data (simulation)\nnp.random.seed(42)\npoisson_data = np.random.poisson(lambda_rate, 100)\nprint(f\"Theoretical mean (λ): {lambda_rate}\")\nprint(f\"Simulated mean: {np.mean(poisson_data):.2f}\")\nprint(f\"Theoretical variance (also λ): {lambda_rate}\")\nprint(f\"Simulated variance: {np.var(poisson_data, ddof=1):.2f}\")\nprint(f\"\\nUse for: arrivals, calls, defects per time/space unit\")\n\n\n\n\n\n\n\n\nTheoretical mean (λ): 3\nSimulated mean: 2.78\nTheoretical variance (also λ): 3\nSimulated variance: 3.14\n\nUse for: arrivals, calls, defects per time/space unit"
  },
  {
    "objectID": "01-EDA/stats_resource.html#exponential-distribution",
    "href": "01-EDA/stats_resource.html#exponential-distribution",
    "title": "Statistics for Machine Learning",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nExponential: Time between Poisson events\nParameter: λ (rate parameter)\nExample: Time between customer arrivals"
  },
  {
    "objectID": "01-EDA/stats_resource.html#exponential-distribution-in-python",
    "href": "01-EDA/stats_resource.html#exponential-distribution-in-python",
    "title": "Statistics for Machine Learning",
    "section": "Exponential Distribution in Python",
    "text": "Exponential Distribution in Python\n\ndef exponential_pdf(x, lam):\n    \"\"\"Calculate exponential probability density function\"\"\"\n    return lam * np.exp(-lam * x)\n\nrate = 0.5  # events per unit time\nscale = 1/rate  # mean time between events\n\nx = np.linspace(0, 10, 1000)\npdf = exponential_pdf(x, rate)\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.plot(x, pdf, 'b-', linewidth=2, label=f'rate={rate}')\nax.fill_between(x, pdf, alpha=0.3)\nax.set_xlabel('Time Between Events')\nax.set_ylabel('Density')\nax.set_title(f'Exponential Distribution (λ={rate}, mean={scale:.1f})')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Generate random exponential data\nnp.random.seed(42)\nexp_data = np.random.exponential(scale=scale, size=100)\nprint(f\"Theoretical mean (1/λ): {scale:.2f}\")\nprint(f\"Simulated mean: {np.mean(exp_data):.2f}\")\nprint(f\"\\nUse for: time between events (complement to Poisson)\")\nprint(f\"Example: time between customer arrivals\")\n\n\n\n\n\n\n\n\nTheoretical mean (1/λ): 2.00\nSimulated mean: 1.83\n\nUse for: time between events (complement to Poisson)\nExample: time between customer arrivals"
  },
  {
    "objectID": "01-EDA/stats_resource.html#key-takeaways",
    "href": "01-EDA/stats_resource.html#key-takeaways",
    "title": "Statistics for Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nEDA\n\nUnderstand your data types\nUse robust statistics\nVisualize distributions\nCheck for outliers\n\n\nSampling\n\nRandom sampling reduces bias\nBootstrap estimates uncertainty\nConfidence intervals quantify precision\nChoose appropriate distributions"
  },
  {
    "objectID": "01-EDA/stats_resource.html#practical-workflow",
    "href": "01-EDA/stats_resource.html#practical-workflow",
    "title": "Statistics for Machine Learning",
    "section": "Practical Workflow",
    "text": "Practical Workflow\n\nLoad and inspect data\nCalculate summary statistics (mean, median, std)\nVisualize distributions (histograms, boxplots)\nCheck for outliers and anomalies\nAssess relationships (correlation, scatterplots)\nEstimate uncertainty (bootstrap, confidence intervals)\nModel appropriately for your data type"
  },
  {
    "objectID": "01-EDA/stats_resource.html#complete-example",
    "href": "01-EDA/stats_resource.html#complete-example",
    "title": "Statistics for Machine Learning",
    "section": "Complete Example",
    "text": "Complete Example\n\n# Comprehensive analysis example\nnp.random.seed(42)\ndata = np.random.normal(100, 15, 200)\n\n# 1. Summary statistics\nprint(\"=== SUMMARY STATISTICS ===\")\nprint(f\"Sample size: {len(data)}\")\nprint(f\"Mean: {np.mean(data):.2f}\")\nprint(f\"Median: {np.median(data):.2f}\")\nprint(f\"Std Dev: {np.std(data, ddof=1):.2f}\")\nprint(f\"IQR: {np.percentile(data, 75) - np.percentile(data, 25):.2f}\")\nprint(f\"Range: [{np.min(data):.2f}, {np.max(data):.2f}]\")\n\n# 2. Bootstrap confidence interval\nn_boot = 1000\nboot_means = [np.mean(np.random.choice(data, len(data), replace=True))\n              for _ in range(n_boot)]\nci = np.percentile(boot_means, [2.5, 97.5])\nprint(f\"\\n=== UNCERTAINTY ===\")\nprint(f\"Standard Error: {np.std(data, ddof=1)/np.sqrt(len(data)):.2f}\")\nprint(f\"95% CI: ({ci[0]:.2f}, {ci[1]:.2f})\")\nprint(f\"CI Width: {ci[1] - ci[0]:.2f}\")\n\n=== SUMMARY STATISTICS ===\nSample size: 200\nMean: 99.39\nMedian: 99.94\nStd Dev: 13.97\nIQR: 18.09\nRange: [60.70, 140.80]\n\n=== UNCERTAINTY ===\nStandard Error: 0.99\n95% CI: (97.36, 101.36)\nCI Width: 4.00"
  },
  {
    "objectID": "01-EDA/stats_resource.html#complete-example-continued",
    "href": "01-EDA/stats_resource.html#complete-example-continued",
    "title": "Statistics for Machine Learning",
    "section": "Complete Example (continued)",
    "text": "Complete Example (continued)\n\n# 3. Comprehensive visualization\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Histogram\naxes[0, 0].hist(data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0, 0].axvline(np.mean(data), color='red', linestyle='--', label='Mean')\naxes[0, 0].axvline(np.median(data), color='green', linestyle='--', label='Median')\naxes[0, 0].set_title('Histogram', fontsize=12)\naxes[0, 0].set_xlabel('Value')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].legend()\n\n# Boxplot\naxes[0, 1].boxplot(data, vert=True)\naxes[0, 1].set_title('Boxplot', fontsize=12)\naxes[0, 1].set_ylabel('Value')\naxes[0, 1].grid(alpha=0.3, axis='y')\n\n# Q-Q Plot\nstandardized = (data - np.mean(data)) / np.std(data, ddof=1)\nstandardized = np.sort(standardized)\nn = len(standardized)\ntheoretical = np.array([np.percentile(np.random.standard_normal(10000), \n                        100 * (i - 0.5) / n) for i in range(1, n + 1)])\naxes[1, 0].scatter(theoretical, standardized, alpha=0.4, s=20)\naxes[1, 0].plot([-3, 3], [-3, 3], 'r--', linewidth=2)\naxes[1, 0].set_xlabel('Theoretical Quantiles')\naxes[1, 0].set_ylabel('Sample Quantiles')\naxes[1, 0].set_title('Q-Q Plot', fontsize=12)\naxes[1, 0].grid(alpha=0.3)\n\n# Bootstrap distribution\naxes[1, 1].hist(boot_means, bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\naxes[1, 1].axvline(ci[0], color='r', linestyle='--', linewidth=2, label='95% CI')\naxes[1, 1].axvline(ci[1], color='r', linestyle='--', linewidth=2)\naxes[1, 1].set_title('Bootstrap Distribution', fontsize=12)\naxes[1, 1].set_xlabel('Bootstrap Mean')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "01-EDA/stats_resource.html#resources",
    "href": "01-EDA/stats_resource.html#resources",
    "title": "Statistics for Machine Learning",
    "section": "Resources",
    "text": "Resources\nPython Libraries\n\nnumpy: Numerical computing\npandas: Data manipulation\nscipy.stats: Statistical functions\nmatplotlib: Visualization\nseaborn: Statistical visualization\n\nFurther Reading\n\nPractical Statistics for Data Scientists (Bruce & Bruce)\nPython for Data Analysis (McKinney)\nStatistical Inference via Data Science (Ismay & Kim)"
  },
  {
    "objectID": "01-EDA/stats_resource.html#thank-you",
    "href": "01-EDA/stats_resource.html#thank-you",
    "title": "Statistics for Machine Learning",
    "section": "Thank You!",
    "text": "Thank You!\nQuestions?\nRemember:\n\nStart with exploration\nQuantify uncertainty\nUse appropriate methods\nVisualize your results"
  },
  {
    "objectID": "01-EDA/rr-EDA.html",
    "href": "01-EDA/rr-EDA.html",
    "title": "EDA readings",
    "section": "",
    "text": "Stats distributions describes how by only knowing statistical properties we may be making inaccurate assumptions of what our data actually looks like visually. \nImplications behind visualizations (excusing the explicit language) has loads of examples of how biases can creep into our data insights. This article shows how we can initially interpret visualizations, and perhaps how to start questioning if the visualization is telling the story we think it is. \nUsing LLMs to storyboard goes through the process of how a data-driven story is created at The Pudding, but using an LLM. This article is a great way to understand how data-driven stories are put together, and how good (or not so good) LLMs are at replicating tasks associated with the design and implementation process. \nggplot vs plotnine compares the graphing power of R’s ggplot to Python’s plotnine (R wrapper for ggplot). Useful for those who have experience with creating visualizations in R, or for those who are unimpressed by matplotlib’s capabilities. Only read if you have a background using R.\nStatistics for ML walks through basic definitions of statistical measures, charts, and distributions.\n\n\n\n\nThe following cheatsheets may be helpful for your reference: data storytelling, data visualizations, matplotlib, plotly, seaborn."
  },
  {
    "objectID": "01-EDA/rr-EDA.html#related-articles",
    "href": "01-EDA/rr-EDA.html#related-articles",
    "title": "EDA readings",
    "section": "",
    "text": "Stats distributions describes how by only knowing statistical properties we may be making inaccurate assumptions of what our data actually looks like visually. \nImplications behind visualizations (excusing the explicit language) has loads of examples of how biases can creep into our data insights. This article shows how we can initially interpret visualizations, and perhaps how to start questioning if the visualization is telling the story we think it is. \nUsing LLMs to storyboard goes through the process of how a data-driven story is created at The Pudding, but using an LLM. This article is a great way to understand how data-driven stories are put together, and how good (or not so good) LLMs are at replicating tasks associated with the design and implementation process. \nggplot vs plotnine compares the graphing power of R’s ggplot to Python’s plotnine (R wrapper for ggplot). Useful for those who have experience with creating visualizations in R, or for those who are unimpressed by matplotlib’s capabilities. Only read if you have a background using R.\nStatistics for ML walks through basic definitions of statistical measures, charts, and distributions."
  },
  {
    "objectID": "01-EDA/rr-EDA.html#cheat-sheets",
    "href": "01-EDA/rr-EDA.html#cheat-sheets",
    "title": "EDA readings",
    "section": "",
    "text": "The following cheatsheets may be helpful for your reference: data storytelling, data visualizations, matplotlib, plotly, seaborn."
  },
  {
    "objectID": "course_information/schedule.html",
    "href": "course_information/schedule.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Note: Days that are bolded will be held asynchronously. Student will work through the material at their own pace, and schedule a time to meet with their lab partner (if needed).",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "course_information/schedule.html#week-1-jan-5---9",
    "href": "course_information/schedule.html#week-1-jan-5---9",
    "title": "Machine Learning",
    "section": "Week 1 (Jan 5 - 9)",
    "text": "Week 1 (Jan 5 - 9)\n\n\n\ndow\ndate\nprepare\ntopic\nagenda\n\n\n\n\nM\nJan 5\n📖 syllabus\nINTRO\n👩🏻‍🏫 INTRO ☑️PC0 🍽️ 💻Lab: Intro\n\n\nT\nJan 6\n📖 readings\nEDA\n👩🏻‍🏫EDA1 👩🏻‍🏫EDA2 🍽️ QUIZ INTRO 📝HW:EDA\n\n\nW\nJan 7\n📓EDA standards\nEDA\n👩🏻‍🏫EDA3 ☑️PC1 🍽️ 💻Lab:EDA 😵‍💫 TA help  7:30-8:30\n\n\nTh\nJan 8\n📖 readings\nEVL\n👩🏻‍🏫EVL1 👩🏻‍🏫EVL2 🍽️ QUIZ EDA 📝HW:EVL\n\n\nF\nJan 9\n📓EVL standards\nEVL\n👩🏻‍🏫EVL3 ☑️PC2 💻Lab:EVL 😵‍💫 TA help  2:00-3:00\n\n\nSu\nJan 11\n\n\n📰  article  review 😵‍💫 TA help  8:00-9:00",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "course_information/schedule.html#week-2-jan-12---16",
    "href": "course_information/schedule.html#week-2-jan-12---16",
    "title": "Machine Learning",
    "section": "Week 2 (Jan 12 - 16)",
    "text": "Week 2 (Jan 12 - 16)\n\n\n\ndow\ndate\nprepare\ntopic\nagenda\n\n\n\n\nM\nJan 12\n📖 readings\nCLS\n👩🏻‍🏫CLS1 👩🏻‍🏫CLS3 📝HW:CLS 🍽️ QUIZ EVL\n\n\nT\nJan 13\n📓CLS standards\nCLS\n👩🏻‍🏫CLS3 ☑️PC3 🍽️ 💻Lab:CLS QUIZ  RETAKES  EDA + EVL 😵‍💫 TA help  7:00-8:00\n\n\nW\nJan 14\n📖 readings\nDATA\n👩🏻‍🏫DATA1 👩🏻‍🏫DATA2 📝HW:DATA 🍽️ Worktime:  PC or Lab\n\n\nTh\nJan 15\n📓DATA standards\nDATA\n💻Lab:DATA🍽️👩🏻‍🏫TREE1 worksheetQUIZ CLS 😵‍💫 TA help  7:00-8:00\n\n\nF\nJan 16\n📖 readings\nTREE\n👩🏻‍🏫TREE2 👩🏻‍🏫TREE3 📝HW:TREE 🍽️ QUIZ DATA ☑️PC4\n\n\n\nOver the weekend, complete an article review",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "course_information/schedule.html#week-3-jan-19---23",
    "href": "course_information/schedule.html#week-3-jan-19---23",
    "title": "Machine Learning",
    "section": "Week 3 (Jan 19 - 23)",
    "text": "Week 3 (Jan 19 - 23)\n\n\n\ndow\ndate\nprepare\ntopic\nagenda\n\n\n\n\nM\nJan 19\n\nNo Class\n☑️PC5\n\n\nT\nJan 20\n📓TREE standards\nTREE\n👩🏻‍🏫LIN1 👩🏻‍🏫LIN2 📝HW:LIN 🍽️💻Lab:TREE 😵‍💫 TA help  7:00-8:00\n\n\nW\nJan 21\n📖 readings\nLIN\n👩🏻‍🏫KNN👩🏻‍🏫SVM 🍽️ QUIZ TREE QUIZ  RETAKES  CLS + DATA\n\n\nTh\nJan 22\n📖 readings\nENS\n👩🏻‍🏫ENS1 👩🏻‍🏫ESB2 📝HW:ENS 🍽️ QUIZ LIN☑️PC6\n\n\nF\nJan 23\n📓LIN standards\nENS\n💻Lab:MOD  Part 1\n\n\n\nOver the weekend, complete an article review",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "course_information/schedule.html#week-4-jan-26---30",
    "href": "course_information/schedule.html#week-4-jan-26---30",
    "title": "Machine Learning",
    "section": "Week 4 (Jan 26 - 30)",
    "text": "Week 4 (Jan 26 - 30)\n\n\n\ndow\ndate\nprepare\ntopic\nagenda\n\n\n\n\nM\nJan 26\n📖 readings\nNN\n👩🏻‍🏫ESB3 ☑️PC7 ☑️PC8 🍽️ 💻Lab:MOD Part 2\n\n\nT\nJan 27\n📓ENS standards\nNN\n👩🏻‍🏫NN1 👩🏻‍🏫NN2📝 HW:NN 🍽️ QUIZ ENS QUIZ  RETAKES  TREE+LIN\n\n\nW\nJan 28\n📖 readings\nDL\n👩🏻‍🏫NN3 ☑️PC9🍽️ 💻Lab:NN 😵‍💫 TA help  8:00-9:00\n\n\nTh\nJan 29\n📓NN + DL standards\nDL\nLast Day! 👩🏻‍🏫DL1 👩🏻‍🏫DL2 🍽️ QUIZ NN+DL QUIZ DL QUIZ  RETAKES  ENS+NN+DL\n\n\nF\nJan 30\nsubmitpresentation\nFinal\nPresentation Day! ☑️PREZ 🍽️ Quiz Retakes ANY",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Machine Learning",
    "section": "",
    "text": "You will complete three (3) reviews of articles related to machine learning. Each review is worth 5 points.\nThese assignments are designed to help you:\nEach review should demonstrate that you have read and understood the article, and thoughtfully reflected on its content. Most importantly, I should be able to get to know you personally through your writing of the review.",
    "crumbs": [
      "Materials",
      "Article Reviews"
    ]
  },
  {
    "objectID": "articles.html#finding-articles",
    "href": "articles.html#finding-articles",
    "title": "Machine Learning",
    "section": "Finding Articles",
    "text": "Finding Articles\nYou are encouraged to find articles from reputable sources. Some suggestions include:\n\nThe New York Times (free access through our library)\n\nThe Pudding has data-driven storytelling examples\nYouTube channels like 3Blue1Brown or Veritasium\n\nOther sources such as academic blogs, news outlets, Medium posts, or scientific publications\n\nTip: Pick articles that genuinely interest you. You will write better reflections if the topic captures your attention.",
    "crumbs": [
      "Materials",
      "Article Reviews"
    ]
  },
  {
    "objectID": "articles.html#article-review-instructions",
    "href": "articles.html#article-review-instructions",
    "title": "Machine Learning",
    "section": "Article Review Instructions",
    "text": "Article Review Instructions\nFor each article, provide clear and complete answers to the following questions.\nWrite in your own words. I do not want to read summarized slop from LLMs (these will receive 0 points).\n\nArticle Information: title of the article, author(s), publication month and year, link to the article\nIn your own words, summarize what the article covers. Focus on the key points and main ideas.\nIdentify any surprising facts, unique approaches, or compelling examples mentioned in the article.\nDiscuss how the ideas or technologies presented could affect society. Consider both positive and negative implications.\nReflect on how the article’s ideas might influence your own life, studies, or career.\nWhat would you ask the author to clarify, expand upon, or explain further?\nWhat question could you pose to readers or viewers to spark discussion?\nReflect on the article as a whole. What did you like or dislike? What did you agree or disagree with? Did it change your perspective on machine learning in any way? Would you recommend this article?",
    "crumbs": [
      "Materials",
      "Article Reviews"
    ]
  },
  {
    "objectID": "articles.html#submission-guidelines",
    "href": "articles.html#submission-guidelines",
    "title": "Machine Learning",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\n\nSubmit each review as pdf to Moodle\nEach review should be about a page or so in length.\nIt’s okay if your grammar is less than perfect.\n\nAll three are due before the final exam day, though it is encouraged to complete them early.",
    "crumbs": [
      "Materials",
      "Article Reviews"
    ]
  },
  {
    "objectID": "articles.html#grading-criteria",
    "href": "articles.html#grading-criteria",
    "title": "Machine Learning",
    "section": "Grading Criteria",
    "text": "Grading Criteria\n\nCompleteness (2 pts)\nPersonal (1 pt)\nInsightful impact (1pt)\nClarity + organization (1pt)\nTotal: 5 pt",
    "crumbs": [
      "Materials",
      "Article Reviews"
    ]
  },
  {
    "objectID": "articles.html#tips-for-success",
    "href": "articles.html#tips-for-success",
    "title": "Machine Learning",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nReflect on how the article connects to your understanding of machine learning.\n\nThink critically about the societal and personal implications.\n\nUse your own words to demonstrate understanding.\n\nChoose a mix of technical and non-technical articles for variety.",
    "crumbs": [
      "Materials",
      "Article Reviews"
    ]
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Homeworks are due at the start of the next class.\n\nHW: EDA\nThis homework goes over looking at summary tables for a dataset, explaining statistical terms to non-tech folks, exploring statistics, and interpretting statistical tests.\n\n\nHW: EVL\nThis homework goes over model selection in our 5-step ML workflow, as well as evaluation metrics for both classification and regression.\n\n\nHW: CLS\nThis homework breaks down the calculations used in clustering algorithms.\n\n\nHW: DATA\nThis homework walks through the data cleaning steps and thought process using AirBnB data.\n\n\nHW: TREE\nThis homework walks through the data cleaning steps and thought process using AirBnB data.",
    "crumbs": [
      "Materials",
      "Homework"
    ]
  },
  {
    "objectID": "05-TREE/std-TREE.html",
    "href": "05-TREE/std-TREE.html",
    "title": "Machine Learning",
    "section": "",
    "text": "The following standards will be assessed on the TREE quiz:"
  },
  {
    "objectID": "05-TREE/std-TREE.html#alg-tree",
    "href": "05-TREE/std-TREE.html#alg-tree",
    "title": "Machine Learning",
    "section": "ALG-TREE",
    "text": "ALG-TREE\nYou should be able to:\n\nDescribe how the decision tree algorithm works\nDefine what the terms entropy, gini, uncertainty, decision boundary, region means in the context of this algorithm\nDraw and interpret the output of a decision tree and graph its associated regions"
  },
  {
    "objectID": "05-TREE/std-TREE.html#train-tree",
    "href": "05-TREE/std-TREE.html#train-tree",
    "title": "Machine Learning",
    "section": "TRAIN-TREE",
    "text": "TRAIN-TREE\nYou should be able to:\n\nExplain overfitting and underfitting in the context of this algorithm\nInterpret results from a decision tree model\nGive suggestions for improving the decision tree model"
  },
  {
    "objectID": "05-TREE/std-TREE.html#study-tips",
    "href": "05-TREE/std-TREE.html#study-tips",
    "title": "Machine Learning",
    "section": "Study Tips",
    "text": "Study Tips\nReview the lecture slides, workbooks, homework assignments, and the associated lab.\nQuiz questions are similar (though not identical) to the types of questions covered in these materials. You should be able to use appropriate terminology and apply the concepts covered in class to a variety of new scenarios.\nThe primary goal of each quiz is to give you an opportunity to apply concepts with clear and well-explained reasoning.\nQuizzes are also a valuable opportunity to receive feedback on your reasoning skills."
  },
  {
    "objectID": "05-TREE/std-TREE.html#formulas-given",
    "href": "05-TREE/std-TREE.html#formulas-given",
    "title": "Machine Learning",
    "section": "Formulas Given",
    "text": "Formulas Given\nEntropy: \\(-p*log_2(p) - (1-p)log_2(1-p)\\)  Gini impurity: \\(2*p*(1-p)\\) \nImpurity\\(_{\\text{total}}:   2*P(target=yes)*P(target=no)\\) \nImpurity\\(_{Question}:   P(\\text{Q=yes})*\\text{Impurity}(\\text{Q=yes})*P(\\text{Q=no})*\\text{Impurity}(\\text{Q=no})\\)"
  },
  {
    "objectID": "04-DATA/std-DATA.html",
    "href": "04-DATA/std-DATA.html",
    "title": "Machine Learning",
    "section": "",
    "text": "The following standards will be assessed on the DATA quiz:"
  },
  {
    "objectID": "04-DATA/std-DATA.html#data-prep",
    "href": "04-DATA/std-DATA.html#data-prep",
    "title": "Machine Learning",
    "section": "DATA-PREP",
    "text": "DATA-PREP\nYou should be able to:\n\nIdentify which kind of features require preparation\nOffer appropriate methods for preparing those features\nExplain the steps necessary to implement the methods selected"
  },
  {
    "objectID": "04-DATA/std-DATA.html#flow2",
    "href": "04-DATA/std-DATA.html#flow2",
    "title": "Machine Learning",
    "section": "FLOW2",
    "text": "FLOW2\nYou should be able to:\n\nDescribe the five steps of the ML workflow\nGive examples of tasks you can do for each step"
  },
  {
    "objectID": "04-DATA/std-DATA.html#study-tips",
    "href": "04-DATA/std-DATA.html#study-tips",
    "title": "Machine Learning",
    "section": "Study Tips",
    "text": "Study Tips\nReview the lecture slides, workbooks, homework assignments, and the associated lab.\nQuiz questions are similar (though not identical) to the types of questions covered in these materials. You should be able to use appropriate terminology and apply the concepts covered in class to a variety of new scenarios.\nThe primary goal of each quiz is to give you an opportunity to apply concepts with clear and well-explained reasoning.\nQuizzes are also a valuable opportunity to receive feedback on your reasoning skills."
  },
  {
    "objectID": "02-EVL/std-EVL.html",
    "href": "02-EVL/std-EVL.html",
    "title": "Machine Learning",
    "section": "",
    "text": "The following standards will be assessed on the EVL quiz:"
  },
  {
    "objectID": "02-EVL/std-EVL.html#eval",
    "href": "02-EVL/std-EVL.html#eval",
    "title": "Machine Learning",
    "section": "EVAL",
    "text": "EVAL\nYou should be able to:\n\nIdentify which evaluation metrics to use for various ML tasks\nCompute metrics (formulas provided)\nInterpret metrics in an accessible way"
  },
  {
    "objectID": "02-EVL/std-EVL.html#train",
    "href": "02-EVL/std-EVL.html#train",
    "title": "Machine Learning",
    "section": "TRAIN",
    "text": "TRAIN\nYou should be able to:\n\nKnow ways to separate train, validation, and test sets\nDefine overfitting, underfitting, cross-fold validation\nUnderstand and give examples of the bias(underfitting) - variance (overfitting) tradeoff"
  },
  {
    "objectID": "02-EVL/std-EVL.html#study-tips",
    "href": "02-EVL/std-EVL.html#study-tips",
    "title": "Machine Learning",
    "section": "Study Tips",
    "text": "Study Tips\nReview the lecture slides, workbooks, homework assignments, and the associated lab.\nQuiz questions are similar (though not identical) to the types of questions covered in these materials. You should be able to use appropriate terminology and apply the concepts covered in class to a variety of new scenarios.\nThe primary goal of each quiz is to give you an opportunity to apply concepts with clear and well-explained reasoning.\nQuizzes are also a valuable opportunity to receive feedback on your reasoning skills."
  },
  {
    "objectID": "02-EVL/std-EVL.html#formulas-given",
    "href": "02-EVL/std-EVL.html#formulas-given",
    "title": "Machine Learning",
    "section": "Formulas given",
    "text": "Formulas given\n\\(\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\\)\n\\(\\text{Precision} = \\frac{TP}{TP + FP}\\)\n\\(\\text{Recall} = \\frac{TP}{TP + FN}\\)\n\\(F1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\n\\(MAE = \\frac{1}{n} \\sum |y_{obs} - y_{pred}|\\)\n\\(MSE = \\frac{1}{n} \\sum (y_{obs} - y_{pred})^2\\)\n\\(RMSE = \\sqrt{MSE}\\)"
  }
]