[
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "Datasets\nSlides"
  },
  {
    "objectID": "standards/std_EDA.html",
    "href": "standards/std_EDA.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Practice questions around EDA topic."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Stuff about homework",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#overview",
    "href": "homework.html#overview",
    "title": "Homework",
    "section": "",
    "text": "Stuff about homework",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#list",
    "href": "homework.html#list",
    "title": "Homework",
    "section": "List",
    "text": "List\nHW 1",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "labs/lab0.html",
    "href": "labs/lab0.html",
    "title": "Task1: Understanding the data",
    "section": "",
    "text": "Group Members:\n\nTask1: Understanding the data\n\nFollow the link https://github.com/washingtonpost/data-police-shootings to read about the data. Write a three sentence summary of what is included in this data, and where it comes from. This dataset allows only a small number of categories for race, and the dataset fails to recognize that some people are multiracial. This is a weakness of the dataset. Include at least one other observation that may be a weakness of the data\n\n\nWritten Answer\n\n\nYou can view csv by running the code chunk below.\n\n\nimport pandas as pd\n\nfatal_police_shootings_data = pd.read_csv('https://raw.githubusercontent.com/washingtonpost/data-police-shootings/refs/heads/master/v2/fatal-police-shootings-data.csv')\n\nfatal_police_shootings_data.head(10)\n\n\n    \n\n\n\n\n\n\nid\ndate\nthreat_type\nflee_status\narmed_with\ncity\ncounty\nstate\nlatitude\nlongitude\nlocation_precision\nname\nage\ngender\nrace\nrace_source\nwas_mental_illness_related\nbody_camera\nagency_ids\n\n\n\n\n0\n3\n2015-01-02\npoint\nnot\ngun\nShelton\nMason\nWA\n47.246826\n-123.121592\nnot_available\nTim Elliot\n53.0\nmale\nA\nnot_available\nTrue\nFalse\n73\n\n\n1\n4\n2015-01-02\npoint\nnot\ngun\nAloha\nWashington\nOR\n45.487421\n-122.891696\nnot_available\nLewis Lee Lembke\n47.0\nmale\nW\nnot_available\nFalse\nFalse\n70\n\n\n2\n5\n2015-01-03\nmove\nnot\nunarmed\nWichita\nSedgwick\nKS\n37.694766\n-97.280554\nnot_available\nJohn Paul Quintero\n23.0\nmale\nH\nnot_available\nFalse\nFalse\n238\n\n\n3\n8\n2015-01-04\npoint\nnot\nreplica\nSan Francisco\nSan Francisco\nCA\n37.762910\n-122.422001\nnot_available\nMatthew Hoffman\n32.0\nmale\nW\nnot_available\nTrue\nFalse\n196\n\n\n4\n9\n2015-01-04\npoint\nnot\nother\nEvans\nWeld\nCO\n40.383937\n-104.692261\nnot_available\nMichael Rodriguez\n39.0\nmale\nH\nnot_available\nFalse\nFalse\n473\n\n\n5\n11\n2015-01-04\nattack\nnot\ngun\nGuthrie\nLogan\nOK\n35.876991\n-97.423454\nnot_available\nKenneth Joe Brown\n18.0\nmale\nW\nnot_available\nFalse\nFalse\n101\n\n\n6\n13\n2015-01-05\nshoot\ncar\ngun\nChandler\nMaricopa\nAZ\n33.327887\n-111.840959\nnot_available\nKenneth Arnold Buck\n22.0\nmale\nH\nnot_available\nFalse\nFalse\n195\n\n\n7\n15\n2015-01-06\npoint\nnot\ngun\nAssaria\nSaline\nKS\n38.703755\n-97.563904\nnot_available\nBrock Nichols\n35.0\nmale\nW\nnot_available\nFalse\nFalse\n490\n\n\n8\n16\n2015-01-06\naccident\nnot\nunarmed\nBurlington\nDes Moines\nIA\n40.809250\n-91.118875\nnot_available\nAutumn Steele\n34.0\nfemale\nW\nnot_available\nFalse\nTrue\n287\n\n\n9\n17\n2015-01-06\npoint\nnot\nreplica\nKnoxville\nAllegheny\nPA\n40.412936\n-79.991408\nnot_available\nLeslie Sapp III\n47.0\nmale\nB\nnot_available\nFalse\nFalse\n26254\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nList out some of the columns that are contained in this csv.\n\nWritten Answer\n\nFor each of those columns, give some examples for the data in that column, and what they mean. For example, some examples of values in the first column id are 22, 325, and 140.\n\nWritten Answer\n\nIn order to compare the representation of black people among the subjects of fatal police shootings to their representation in the general population, we need to know the percent of the United States population which is black. You can find this by googling ‚Äôracial demographics of the United States‚Äò. Include a citation with the organization and url.\n\nWritten Answer\n\n\n\n\nTask2: Reading data into a dict\nRun the code chunk below. This will select some of the features from the dataset, and put it into a Python dictionary.\n\npolicing = fatal_police_shootings_data[['id', 'name', 'date', 'armed_with', 'age', 'gender', 'race', 'state']]\npolicing_dict = policing.set_index('id').to_dict(orient='index')\n\n\nWhat information from the original csv is stored in policing_dict? What information from the original csv do we use as the keys in policing_dict? What is the type of the values in policing_dict? Hint: use type(dict[key]).\n\n\nStudent Answer Hint: create a code chunk with type(dict[key]) to find the data type of the values in our dictionary\n\n\n\nTask3: Using the database\n\nFinding a particular record: Find the record of the fatal police shooting with ID number 1694. If you‚Äôve followed current events in the past few years, there should be a familiar name here.\n\n\nStudent Answer Add policing_dict[1694] to a code chunk\n\n\nDisplaying data from a single record: Print the name and state of the individual with ID number 1694.\n\nStudent Answer\n\nDisplaying data from multiple records: The following code will display the entire dictionary using a for loop:\n\nfor person,data in policing dict.items():\nprint(policing dict[person])\nAdd an if statement to the for loop to filter out only individuals from Minnesota. This means you‚Äôll need to check the state for each person and only print data if the state is Minnesota.\n\nStudent Answer\n\n\n\nTask4: Summarizing our data\n\nCreating a new dictionary as a subset: Instead of displaying all the information, let‚Äôs create a new dictionary that only contains inviduals from Minnesota. The syntax for this is: key:values for key, values in dict.items() if condition where condition is what you‚Äôre filtering on and dict is your dictionary. Name this new dictionary MN_selection.\n\n\nStudent Answer\n\n\nCreating summarization functions: We‚Äôd like to create a function that takes in our database dictionary and returns a dictionary with counts of occurrences of each race among subjects of fatal police shootings. The keys in this dictionary should be races, and the corresponding values should be the number of subjects of that race.\n\n\nStudent Answer The starter code for this function is below:\n\n\ndef get_race_counts(database:dict)-&gt;dict:\n  race_counts = {}\n  for person, data in database.items():\n    #add logic to populate race counts\n\n\n  return race_counts\n\n\nUsing a summarization function: Print the fraction of fatal police shootings with a black subject. This should be a number between 0 and 1, and can be computed by dividing the number of fatal police shootings with a black subject, by the total number of fatal police shootings.\n\n\nStudent Answer Note: using get, len, and round may be helpful.\n\n\nHow does the proportion of black subjects in fatal police shootings compare to the proportion of black people in the United States population?\n\n\nStudent Answer\n\n\n\nTask5: Comparing Summarizations\n\nCreate a new dictionary called unarmed_selection, which is built using dictionary comprehension on the original dictionary policing_dict. This dictionary should have the same structure as policing dict, except it will only contain entries for fatal police shootings where the subject was unarmed.\n\n\nStudent Answer\n\n\nCreate a new dictionary, called unarmed race counts, which is created using the get_race_counts function on the dictionary unarmed_selection. The purpose of this dictionary will be to count the number of occurrences of each race among subjects of fatal police shootings, including only those where the subject is unarmed.\n\n\nStudent Answer\n\n\nPrint the fraction of unarmed fatal police shootings with a black subject. This should be a number between 0 and 1, and can be computed by dividing the number of unarmed fatal police shootings with a black subject (you can get this from the dictionary unarmed race counts), by the total number of unarmed fatal police shootings.\n\n\nStudent Answer\n\n\nHow does the proportion of black subjects in fatal police shootings where the subject is unarmed compare to the proportion of black people in the United States population? How does it compare to the proportion of black subjects in all police shootings?\n\n\nStudent Answer\n\n\n\nTask6: Trends over time\n\nCreating a list of a single feature: Let‚Äôs now take a look at creating a list with our data. Using the code below, edit it so that only dates are shown (hint: do something to data). Now, get rid of anything that isn‚Äôt the year (use some string slicing). Name this list year_col, which contains only the year for each record in our database.\n\n\nStudent Answer Edit [data for person, data in policing dict.items()]\n\n\nCreating counts from a list: Summarize the list you just created into a dictionary where each key is a single year and the value is the count of that year in our database. Note: If you do this with dictionary comprehension use the list method count and set(year col).\n\n\nStudent Answer\n\n\nHow do the number of fatal police shootings change over time? Do you notice any patterns or trends?\n\n\nStudent Answer\n\n\n\nTask7: More stories to be told\nSources: * U.S. Crime and Arrest Data (FBI UCR) * U.S. Demographic Data (Census Bureau) * Police Departments‚Äô Use of Force Data (BJS) * Pew Research on Policing * Other relevant sources (Reddit, social media, etc.)\n\nList at least two sources you found informative. Include the organization, url, and brief description of the information the source contains. This source could be an additional dataset, or it could be a result of an analysis done. Our goal is to try to link the insights from the previous tasks to other insights found elsewhere.\n\n\nStudent Answer\n\n\nCome up with at least three questions/insights that could supplement the analysis you have done so far. These questions should address some aspects of the following:\n\n\nvalidation: how can the data from these sources support or challenge the findings you‚Äôve already made about racial disparities in fatal police shootings?\nexpansion: how can additional data be used to expand upon the analysis in new areas, or further explore systemic issues related to police shootings?\ncomparisons: how do the findings from these sources compare to the conclusion you‚Äôve drawn from other tasks? Are there any contradictions or surprising findings?\n\n\nStudent Answer\n\n\n\nTask8: Reflection\n\nWrite a reflection (at least five sentences) on what you learned from this project. This can include your reaction to the results of the project, as well as the process of working with the data.\n\n\nStudent Answer\n\nWhen your group is done, each partner must submit a link to their workbook to Moodle. Make sure the link you shared is public."
  },
  {
    "objectID": "course_information/syllabus.html",
    "href": "course_information/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "CSCI 200B Interim 2026 Special Topics: Machine Learning Morning Sessions: 10:40-12:40 in RNS 203 Afternoon Sessions: 1-3 in RNS 203\n\n\nMission: This is a welcoming, inclusive, encouraging, and failure-tolerant class.  Instructor: Kim Mandery  E-mail: mander1@stolaf.edu  Office: RMS 407\nTextbook: Introduction to Statistical Learning with Python (ISL-P). This book is free and online.\n\n\n\nIt has become increasingly common to use machine learning algorithms to analyze data, draw conclusions, and build models, without direct human instruction. These algorithms have been used in a wide variety of applications, including Netflix recommendations, predicting healthcare outcomes, criminal justice, and many more. In this course, we‚Äôll explore several common machine learning algorithms, learning how they work, and applying them to real datasets. We will cover the strengths and limitations of machine learning algorithms. We will also explore real-world applications of machine learning, and discuss the ethical and societal consequences of the use of these algorithms.\n\n\n\nThrough the lens of machine learning, we will:  - develop and interpret applications of algorithms to domain use-cases - develop working software that satisfies coding best practices - work effectively, both individually and in teams - communicate information effectively to both technical and non-technical audiences\nCourse Schedule: The schedule for this course is available here. This document is likely to be updated throughout the semester, and will be announced in class if it is.\nActive Learning and Engagement: I expect you to take an active role in class each day, participating positively, wholeheartedly, and respectfully in class/group discussions and other activities. You may need sick/mental health days. You are responsible for learning the material on the day you missed. If you are experiencing issues causing you to miss classes often, please speak with me as this may be evident of a possible failing grade. You can also visit your class dean as they are a great resource."
  },
  {
    "objectID": "course_information/syllabus.html#general-information",
    "href": "course_information/syllabus.html#general-information",
    "title": "Syllabus",
    "section": "",
    "text": "Mission: This is a welcoming, inclusive, encouraging, and failure-tolerant class.  Instructor: Kim Mandery  E-mail: mander1@stolaf.edu  Office: RMS 407\nTextbook: Introduction to Statistical Learning with Python (ISL-P). This book is free and online."
  },
  {
    "objectID": "course_information/syllabus.html#course-description",
    "href": "course_information/syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "It has become increasingly common to use machine learning algorithms to analyze data, draw conclusions, and build models, without direct human instruction. These algorithms have been used in a wide variety of applications, including Netflix recommendations, predicting healthcare outcomes, criminal justice, and many more. In this course, we‚Äôll explore several common machine learning algorithms, learning how they work, and applying them to real datasets. We will cover the strengths and limitations of machine learning algorithms. We will also explore real-world applications of machine learning, and discuss the ethical and societal consequences of the use of these algorithms."
  },
  {
    "objectID": "course_information/syllabus.html#course-goals",
    "href": "course_information/syllabus.html#course-goals",
    "title": "Syllabus",
    "section": "",
    "text": "Through the lens of machine learning, we will:  - develop and interpret applications of algorithms to domain use-cases - develop working software that satisfies coding best practices - work effectively, both individually and in teams - communicate information effectively to both technical and non-technical audiences\nCourse Schedule: The schedule for this course is available here. This document is likely to be updated throughout the semester, and will be announced in class if it is.\nActive Learning and Engagement: I expect you to take an active role in class each day, participating positively, wholeheartedly, and respectfully in class/group discussions and other activities. You may need sick/mental health days. You are responsible for learning the material on the day you missed. If you are experiencing issues causing you to miss classes often, please speak with me as this may be evident of a possible failing grade. You can also visit your class dean as they are a great resource."
  },
  {
    "objectID": "course_information/syllabus.html#readings-and-reflections",
    "href": "course_information/syllabus.html#readings-and-reflections",
    "title": "Syllabus",
    "section": "Readings and Reflections",
    "text": "Readings and Reflections\n(3pt * 10 ‚Üí 30 pt): Weekly reading assignments are due at 11:59pm (midnight) on the days indicated in the course schedule (typically Sundays). These readings consist of textbook passages from Introduction to Statistical Learning (ISL-P), as well as articles on ethics and/or domain use-cases. The reflections will be posted on Moodle and are graded based off completion. These will be used to gauge the difficulty of the material and guide certain aspects of the lecture. Late work: Submissions submitted after the deadline will receive no credit."
  },
  {
    "objectID": "course_information/syllabus.html#homework-assignments",
    "href": "course_information/syllabus.html#homework-assignments",
    "title": "Syllabus",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n(5pt * 9 ‚Üí 45 pt): Weekly homework assignments are due at the start of class on the days indicated in the course schedule (typically Mondays). These homework assignments consist of exercises to complete by hand (rather than programming), to reinforce your understanding of the concepts. Collaboration on homework is encouraged, and can be beneficial. However, remember that simply sharing answers is considered academic dishonesty, and is counterproductive, as it will not help you learn the material. Late work: Submissions one day late will receive a minus 1 point late penalty; submissions between one day and one week late will receive a minus 3 point late penalty. Submissions more than one week late will receive no credit."
  },
  {
    "objectID": "course_information/syllabus.html#lab-assignments",
    "href": "course_information/syllabus.html#lab-assignments",
    "title": "Syllabus",
    "section": "Lab Assignments",
    "text": "Lab Assignments\n(5pt * 7 ‚Üí 35 pt): We will have labs associated with every module. These are due before we take the corresponding module quiz. See the course schedule for more details. These assignments consist of programming exercises to practice applying the techniques we cover. Typically, these assignments will be completed by filling in missing code into Jupyter notebooks using Google Colab, and then submitting your completed notebook. Each exercise will be graded all-or-nothing, with no partial credit. Late work: Submissions one day late will receive a minus 1 point late penalty; submissions between one day and one week late will receive a minus 3 point late penalty. Submissions more than one week late will receive no credit."
  },
  {
    "objectID": "course_information/syllabus.html#semester-project",
    "href": "course_information/syllabus.html#semester-project",
    "title": "Syllabus",
    "section": "Semester Project",
    "text": "Semester Project\n(60 pts): Towards the end of the course, you will complete a final project where you apply the concepts covered in the course to a topic of your choice. The final project may be completed individually or in a group of at most three students. It will be divided into the following components (a more detailed rubric will be available later on in the semester):  ‚Ä¢ Plan, Implementation, and Submission. You will submit a plan for your project using the machine learning workflow, and implement the workflow using coding best practices. Your final code workbook will be submitted to Moodle prior to your presentation day. ‚Ä¢ Peer Evaluations and Workdays. We will have several project workdays in class where you will be asked to critique group projects in small groups, as well as give constructive feedback of all projects during our two presentation days. After you present your own project, you will complete a self reflection to assess what you did well, what you could improve upon, and the most important learning you discovered along the way. ‚Ä¢ Presentation Day. You will give a five-minute presentation on your project to the class. Late Work: Late penalties for each component will be included in the instructions for that component. Late submissions for some components may not receive credit."
  },
  {
    "objectID": "course_information/syllabus.html#standards-based-quizzes",
    "href": "course_information/syllabus.html#standards-based-quizzes",
    "title": "Syllabus",
    "section": "Standards based Quizzes",
    "text": "Standards based Quizzes\n(8pt * 9 ‚Üí 72 pt): There will be five (5) quizzes or mini-exams over the course of the semester that will be assessed using standards-based grading. Each quiz will contain between 2-4 standards, covering in total 16 standards. You will be given three attempts to pass each standard. The first attempt will be during a quiz as indicated in the course schedule. There will be two retake days for any second attempts. All final attempts for standards will take place during our final exam time. The standards are graded on an 8-point scale and binned into four categories: P - proficient (8) R - small revision needed S - not satisfactory (4) I - incomplete (0) Revisions must be handed in during class on the days indicated on the schedule. Standards with an R that hand in a fully correct solution will be bumped up to a P. Otherwise it will get bumped down to an S."
  },
  {
    "objectID": "course_information/syllabus.html#extra-credit",
    "href": "course_information/syllabus.html#extra-credit",
    "title": "Syllabus",
    "section": "Extra Credit",
    "text": "Extra Credit\nYou can earn 2pt of extra credit for every CS-related event you attend (up to three events total). You must fill out the EC form on Moodle to earn credit for the event. An additional 1pt will be added to any selfie taken during the event with either the presenter, poster, or other student(s) attending (you must get permission of all folks in the selfie beforehand). ## Final Exam The final exam is during the assigned period: Friday, Jan 30 from 1:00 - 3:00 P.M. Any final attempt on standards will take place during our final time. If you successfully passed each standard, you do not have to attend the final exam. Any student with S or I for three or more standards will be ineligible for a final grade greater than a B- in the course.\n\nFinal Grade Scale\n97 ‚â§ A+ ‚â§ 100 87 ‚â§ B+ ‚â§ 90 77 ‚â§ C+ ‚â§ 80 67 ‚â§ D+ ‚â§ 70 93 ‚â§ A ‚â§ 97 83 ‚â§ B ‚â§ 87 73 ‚â§ C ‚â§ 77 63 ‚â§ D ‚â§ 67 90 ‚â§ A- ‚â§ 93 80 ‚â§ B- ‚â§ 83 70 ‚â§ C- ‚â§ 73 60 ‚â§ D- ‚â§ 63\nExplanations of each letter grade range can be found at https://catalog.stolaf.edu/academic-regulations-procedures/grades/"
  },
  {
    "objectID": "course_information/syllabus.html#syllabus-changes",
    "href": "course_information/syllabus.html#syllabus-changes",
    "title": "Syllabus",
    "section": "Syllabus Changes",
    "text": "Syllabus Changes\nAll course information provided in this syllabus is subject to change and/or elimination. Changes will only be made when the instructor feels they are in the best interest of the class. It is the responsibility of every student to attend class, check e-mail, and communicate with the instructor to be informed and understand these changes. Most importantly HAVE FUN ‚Äì I am really looking forward to this semester!"
  },
  {
    "objectID": "course_information/syllabus.html#workflow",
    "href": "course_information/syllabus.html#workflow",
    "title": "Syllabus",
    "section": "Workflow",
    "text": "Workflow\ndefinition and examples of a broad variety of machine learning tasks: supervised learning (classification,regression), unsupervised learning (clustering); ability to identify, construct, and critique ML workflows."
  },
  {
    "objectID": "course_information/syllabus.html#eda",
    "href": "course_information/syllabus.html#eda",
    "title": "Syllabus",
    "section": "EDA",
    "text": "EDA\nunderstanding of statistical measures, distributions, and tests; exploration of data to guide workflow design and model selection."
  },
  {
    "objectID": "course_information/syllabus.html#data",
    "href": "course_information/syllabus.html#data",
    "title": "Syllabus",
    "section": "Data",
    "text": "Data\ndata preprocessing (importance and pitfalls); handling missing values (imputing, flag-as-missing, implications); encoding categorical variables and real-valued data; normalization and standardization."
  },
  {
    "objectID": "course_information/syllabus.html#selection",
    "href": "course_information/syllabus.html#selection",
    "title": "Syllabus",
    "section": "Selection",
    "text": "Selection\nimportance of understanding what your model is actually doing, where its pitfalls or shortcomings are, and the implications of its decisions; no free lunch (no one learner can solve all problems); representational design decisions have consequences; sources of error and undecidability in machine learning."
  },
  {
    "objectID": "course_information/syllabus.html#algorithms",
    "href": "course_information/syllabus.html#algorithms",
    "title": "Syllabus",
    "section": "Algorithms",
    "text": "Algorithms\nfundamentals of understanding how common ML algorithms work (including but not limited to: clustering, tree-based, regression-based, ensemble, neural nets, deep learning); ability to explain algorithms and their associated terms to a non-technical audience (including but not limited to: objective function, gradient descent, regularization, entropy)"
  },
  {
    "objectID": "course_information/syllabus.html#training",
    "href": "course_information/syllabus.html#training",
    "title": "Syllabus",
    "section": "Training",
    "text": "Training\nseparation of train, validation, and test sets; tuning the parameters of a machine learning model with a validation set; cross validation; overfitting problem / controlling solution complexity (regularization, pruning ‚Äì intuition only); the bias(underfitting) - variance (overfitting) tradeoff."
  },
  {
    "objectID": "course_information/syllabus.html#evaluation",
    "href": "course_information/syllabus.html#evaluation",
    "title": "Syllabus",
    "section": "Evaluation",
    "text": "Evaluation\nperformance metrics for classifiers; estimation of test performance on held-out data; other metrics for classification (e.g., error, precision, recall); performance metrics for regressors; confusion matrix;"
  },
  {
    "objectID": "course_information/syllabus.html#ethics",
    "href": "course_information/syllabus.html#ethics",
    "title": "Syllabus",
    "section": "Ethics",
    "text": "Ethics\nfocus on real data, scenarios, and case studies; bias present in datasets, algorithms, evaluation; privacy; fairness; ethical matrix"
  },
  {
    "objectID": "course_information/syllabus.html#inclusivity-and-community",
    "href": "course_information/syllabus.html#inclusivity-and-community",
    "title": "Syllabus",
    "section": "Inclusivity and Community",
    "text": "Inclusivity and Community\nIn keeping with St.¬†Olaf College‚Äôs mission statement, this class strives to be an inclusive and antiracist learning community, respecting and promoting those of differing backgrounds and beliefs. As a community, we will be to be respectful to all citizens in this class, regardless of race, ethnicity, religion, gender, or sexual orientation. This course affirms people of all gender expressions and gender identities. If you prefer to be called a different name or pronoun than what is on the class roster, please let me know."
  },
  {
    "objectID": "course_information/syllabus.html#illness-and-community-standards",
    "href": "course_information/syllabus.html#illness-and-community-standards",
    "title": "Syllabus",
    "section": "Illness and Community Standards",
    "text": "Illness and Community Standards\nOut of respect for our learning community, if you are experiencing any symptoms of an illness, do not come to class or my office. Please contact me before class time to let me know of your absence. ## Cell Phone Policy and Classroom Atmosphere You are expected to contribute to a positive classroom atmosphere, which includes arriving on time, not being disruptive, being respectful, actively involving yourself in class, and silencing and putting away cell phones. There may come a time where we use technology in class ‚Äì during this time you can use a laptop, tablet, or your phone if you do not have the other options available to you. At any other times, however, I do not want to see your phone out. I‚Äôll ask you to put them away if they show up. ## Mental and Physical Health I greatly value your experience in this class, and it is my duty to facilitate a safe, caring, and productive learning environment. I recognize that you may experience a range of emotional, physical, and/or psychological issues, both in and out of the classroom, that may distract you from your learning. If you are experiencing such issues, please do not hesitate to see me‚Äì I am here to listen. We can also discuss what further resources might be available to you. ## Academic Accommodations I am committed to supporting the learning of all students in my class. If you have already registered with Disability and Access (DAC) and have your letter of accommodations, please meet with me as soon as possible to discuss, plan, and implement your accommodations in the course. If you have or think you have a disability (learning, sensory, physical, chronic health, mental health or attentional), please contact Disability and Access staff at 507-786-3288 or by visiting https://wp.stolaf.edu/academic-support/dac. ## Multilingual Students I am committed to making course content accessible to all students. If English is not your first language and this causes you concern about the course, please speak with me. Students who would like extra support with writing or speaking in English can also contact the language specialist (berryag@stolaf.edu) in CAAS. ## Managing stress, anxiety, and other issues I greatly value your experience in this class, and it is my duty to facilitate a safe, caring, and productive learning environment. I recognize that you may experience a range of emotional, physical, and/or psychological issues, both in and out of the classroom, that may distract you from your learning. If you are experiencing such issues, please do not hesitate to come see me ‚Äì I am here to listen. We can also discuss what further resources might be available to you. ## Plagiarism & Academic Integrity Plagiarism, the unacknowledged appropriation of another person‚Äôs words or ideas, is a serious academic offense. It is imperative that you hand in work that is your own, and that cites or gives credit to others whenever you draw from their work. This includes citing prompts when using genAI tools or an internet search. Please see St.Olaf‚Äôs statements on academic integrity and plagiarism at: https://wp.stolaf.edu/thebook/academic/integrity/. See also the description of St.Olaf‚Äôs honor system at: https://wp.stolaf.edu/honorcouncil/. ## Communication Expectations: ‚Ä¢ Instructor/Student Expectations: I check my email frequently, and attempt to respond to questions within 24 hours. It should not be expected that I check my email after 9 PM, on Saturdays, or regularly during breaks. ‚Ä¢ E-mail Etiquette: If you have multiple questions, fewer emails with more information per email is preferred. Please include the following when sending an email: course number with section number, a description of your question(s) within the subject line, a greeting and your instructors preferred name (mine is Kim), and a sign-off using your preferred name (this is what you want others to refer to you in class by). Add screenshots of your work for clarity. ## The St.¬†Olaf Honor System The St.¬†Olaf Honor System has been in place at St.¬†Olaf College since 1911. All tests, quizzes, and examinations of any kind are taken under the St.¬†Olaf Honor System. Each student is responsible for adhering to all principles of the Honor System regardless of the individual circumstances associated with their assessment environment."
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html",
    "href": "PA4_Data_Cleaning_with_AirBnB.html",
    "title": "Task 1: NYC Data",
    "section": "",
    "text": "Name:\nWho you worked with:"
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#objectives",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#objectives",
    "title": "Task 1: NYC Data",
    "section": "Objectives",
    "text": "Objectives\nThe goals of this project are to: - Perform EDA and data cleaning - Implement transformations and filtering functions - Look for EDA and data cleaning inspiration from outside sources"
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#overview",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#overview",
    "title": "Task 1: NYC Data",
    "section": "Overview",
    "text": "Overview\nFor this programming assignment, you will practice some data cleaning and preprocessing, using a dataset of AirBNB data from New York City. In working with this dataset, our goal will be to train a model that can predict the price of an AirBNB rental. Imagine that you are a data scientist working for AirBNB, and your boss has asked you to develop this as a tool that can suggest prices for new listings."
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#schedule",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#schedule",
    "title": "Task 1: NYC Data",
    "section": "Schedule",
    "text": "Schedule\nHere is the suggested schedule for working on this project: - Weekend: Read through project instructions, complete Task 1. - Tuesday: Complete Tasks 2-3. - Wednesday: Complete Tasks 4-5. - Thursday: Complete Task 6.\nThis project is due on Thursday, 3/13, by 11:59pm."
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#held-out-set",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#held-out-set",
    "title": "Task 1: NYC Data",
    "section": "Held-Out Set",
    "text": "Held-Out Set\nNow, I‚Äôm going to select a few records that we‚Äôll use at the end to test a prediction function. I‚Äôm selecting these here, since we‚Äôll be making changes to the dataset later.\n\nchosen_indices = range(0, 9000, 1000)\n\nheld_out = nyc.iloc[chosen_indices]\n\nheld_out"
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#price",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#price",
    "title": "Task 1: NYC Data",
    "section": "Price",
    "text": "Price\nWe‚Äôll start by looking at our target, price. Let‚Äôs look at a histogram, to see how the prices are distributed.\n\nnyc['price'].hist(bins = 80)\n\nThis is extremely skewed. We have a price that‚Äôs $100,000, but the vast majority have a price under $1000.\nThinking about the tool we‚Äôre building, we might want to focus on training a model that predicts well for lower/typical prices, and ignore the really expensive ones. Let‚Äôs look at what happens with a few different choices for restricting the prices.\nFirst, we look at prices below $2,000.\n\nprint(\"Number of entries: \", len(nyc[nyc['price'] &lt; 2000]))\n\nnyc[nyc['price'] &lt; 2000]['price'].hist(bins = 100)\n\nNext, let‚Äôs look at prices below $1,000.\n\nprint(\"Number of entries: \", len(nyc[nyc['price'] &lt; 1000]))\n\nnyc[nyc['price'] &lt; 1000]['price'].hist(bins = 100)\n\nHere, we look at prices below $500.\n\nprint(\"Number of entries: \", len(nyc[nyc['price'] &lt; 500]))\n\nnyc[nyc['price'] &lt; 500]['price'].hist(bins = 100)\n\nLet‚Äôs focus on prices below $500. This still captures a lot of the data, while limiting us to trying to predict prices for more typical listings. Note that the distribution is still skewed - we‚Äôll handle this in a bit.\n\nnyc = nyc[nyc['price'] &lt; 500]\n\nnyc.describe(include = \"all\")\n\nNotice that the minimum price is $0, which seems like nonsense. Let‚Äôs look at the listings with a price of $0.\n\nnyc[nyc['price'] &lt;= 0]\n\nThere aren‚Äôt too many of them, so let‚Äôs just exclude those.\n\nnyc = nyc[nyc['price'] &gt; 0]\n\nnyc.describe(include = \"all\")\n\nWe‚Äôve seen that even restricting to prices below $500, we still have a skewed distribution. Let‚Äôs see if applying a transformation can help. First, we try applying a log function.\n\nnyc[\"price\"].apply(np.log).hist(bins = 50)\n\nLet‚Äôs also see what happens with applying a square root function.\n\nnyc[\"price\"].apply(np.sqrt).hist(bins = 50)\n\nWe‚Äôll choose to apply the log transformation. Note that applying such a transformation does make sense here, because the difference between prices $400 and $450 is less significant than the difference between prices $50 and $100.\n##üíª Q5: Log Price Add a new column called log_price, which has the log transformation appied to the column price. Then, drop price from the dataframe.\n\n# add column log_price\nnyc[\"log_price\"] = #your code here\n\n#drop price\n#your code here\n\nnyc.describe(include = \"all\")\n\n\nassert(np.isclose(nyc[\"log_price\"].mean(), 4.60618))\nassert(\"price\" not in nyc.columns)"
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#latitude-and-longitude",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#latitude-and-longitude",
    "title": "Task 1: NYC Data",
    "section": "Latitude and Longitude",
    "text": "Latitude and Longitude\nNow, let‚Äôs take a look at latitude and longitude. We‚Äôll look at the maximum and mininum values, to see the range they fall in.\n\nprint(nyc['latitude'].max())\nprint(nyc['latitude'].min())\nprint(nyc['longitude'].max())\nprint(nyc['longitude'].min())\n\nThis range makes sense here, since all of the listings are in New York City. Now, let‚Äôs look at histograms.\n\nnyc['latitude'].hist(bins = 50)\n\n\nnyc['longitude'].hist(bins = 50)\n\nThe distributions look pretty close to normal, so we‚Äôll leave them as they are."
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#number-of-reviews",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#number-of-reviews",
    "title": "Task 1: NYC Data",
    "section": "Number of Reviews",
    "text": "Number of Reviews\nNow, let‚Äôs look at number of reviews. We‚Äôll create a histogram to see the distribution of the data.\n\nnyc['number_of_reviews'].hist(bins = 50)\n\nThis has a very dramatic right skew, so let‚Äôs try applying some transformations. We‚Äôll start with a log transformation.\nNow, if we were to apply the log transformation to this feature we would end up with an error. To fix this we need to add 1 to each value prior to log transforming it. This process is called smoothing and can be particularly helpful when working with text data in the future.\n##‚úè Q6: Add one smoothing\nExplain why we need to add 1. Hint: are there any values that when you apply the log transformation you would run into issues?\n[your answer here]\nNow let‚Äôs visualize this add-one smoothing log transformation\n\nnyc[\"number_of_reviews\"].apply(lambda x: (np.log(x+1))).hist(bins = 50)\n\nThis is an improvement, but still right skewed. Let‚Äôs look at a square root transformation next.\n\nnyc[\"number_of_reviews\"].apply(np.sqrt).hist(bins = 50)\n\nThat‚Äôs not as good as the log transformation.\nSince the result of the log transformation is still pretty skewed, let‚Äôs try applying the log transformation again. Again, we‚Äôll have to add 1 before applying the log transformation.\n##üíª Q7: log-log function In the cell below, define a function called log_log that does the following computation: \\[ f(x) = \\log(\\log(x+1)+1)\\].\nNote: replace the keyword pass with the computation above.\n\ndef log_log(value):\n  pass\n\n# create histogram\nnyc[\"number_of_reviews\"].apply(log_log).hist(bins = 50)\n\nThis still isn‚Äôt great, but we can see that we‚Äôre going to have trouble improving much more, since the number of reviews is going to be very discrete for small numbers (hence the tall, isolated columns on the left side). So, we‚Äôll use the log-log transformation.\nNow construct a new column, log_log_number_of_reviews, by applying the log_log function to the column number_of_reviews. Drop the old column, number_of_reviews, from the data frame\n\n# your code here\n\n\nnyc.describe(include = \"all\")\n\n\nassert(\"log_log_number_of_reviews\" in nyc.columns)\nassert(\"number_of_reviews\" not in nyc.columns)\nassert(np.isclose(nyc[\"log_log_number_of_reviews\"].mean(), 0.896842))"
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#calculated-host-listings-count",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#calculated-host-listings-count",
    "title": "Task 1: NYC Data",
    "section": "Calculated Host Listings Count",
    "text": "Calculated Host Listings Count\nNow, let‚Äôs look at the column calculated_host_listings_count. We‚Äôll look at the distribution.\n\nnyc['calculated_host_listings_count'].hist(bins = 50)\n\nSince this is very skewed, let‚Äôs look at what happens when we apply a log transformtion.\n\nnyc[\"calculated_host_listings_count\"].apply(np.log).hist(bins = 50)\n\nStill very skewed, so let‚Äôs try the log log transformation\n\nnyc[\"calculated_host_listings_count\"].apply(log_log).hist(bins = 50)\n\nStill skewed, and we can tell that the data becoming very discrete is going to be an issue. This might not be the best choice, but let‚Äôs just stick with applying a log transformation.\n\nnyc[\"log_calculated_host_listings_count\"] = nyc[\"calculated_host_listings_count\"].apply(np.log)\n\nnyc.drop(\"calculated_host_listings_count\", axis = 1, inplace = True)\n\nnyc.describe(include = \"all\")"
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#room-type",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#room-type",
    "title": "Task 1: NYC Data",
    "section": "Room Type",
    "text": "Room Type\nWe‚Äôll look at room type next.\n\nnyc['room_type'].value_counts()\n\n##‚úè Q10: Reasoning for OHE\nWe‚Äôre going to apply one-hot encoding to this feature as it is. Why are we choosing to do the OHE immediately instead of doing additional cleaning on this feature first?\n[your answer here]\nLet‚Äôs go ahead and OHE room_type\n\nroom_type_dummies = pd.get_dummies(nyc[\"room_type\"], prefix = \"room_type\")\n\nnyc = nyc.join(room_type_dummies)\nnyc.drop('room_type', axis = 1, inplace = True)\n\nnyc.describe(include = \"all\")"
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#data-cleaning-relates-to-model-performance",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#data-cleaning-relates-to-model-performance",
    "title": "Task 1: NYC Data",
    "section": "Data cleaning relates to model performance",
    "text": "Data cleaning relates to model performance\nNow that we have gone through our features and made some transformations, we would typically train a model to see how well it performs. Remember, data cleaning is the most impactful thing you can do to improve your model performance.\nThe following steps would create a model to predict the price of an AirBNB rental based off our transformed feature set.\nSteps: * Selecting our features and target. * Split the data into testing and training sets. * Fit selected predictive model to the training data. * Check appropriate evaluation metric for our training data and for our testing data. * Plot predicted values again true target values, to visualize how the selected model is performing.\nThis last step means that we will need to apply transformations to the parameters, that correspond to the transformations we did on the features of the dataset.\nAn example for implementation with our data set: * Use selected model trained above to make a predict for log_price. * ‚ÄúUndo‚Äù the log transformation, and return the predicted price. * Compare your predictions to the actual prices for the records we pulled out.\n##‚úè Q11: Next Steps Even after some data cleaning, suppose that we are not getting very good performance. What are two things you could try next?\nOption1:\nOption2:"
  },
  {
    "objectID": "PA4_Data_Cleaning_with_AirBnB.html#finding-inspiration-in-others",
    "href": "PA4_Data_Cleaning_with_AirBnB.html#finding-inspiration-in-others",
    "title": "Task 1: NYC Data",
    "section": "Finding inspiration in others",
    "text": "Finding inspiration in others\nThe best way to learn how to clean data is to see what other people do!\nFor this last task, scroll down and select one of the notebooks someone has published on the Kaggle site (linked in Task 1). Note: you may have to dig a bit to see one you like.\nOnce you have a notebook selected, read through it and answer the following questions.\n##‚úè Q12: Url of notebook used:\n[your answer here]\n##‚úè Q13: EDA\n\nDescribe something new this workbook has done with EDA.\nWhat is one reason this may be a good choice for our data.\nWhat is one reason that this may not be a good choice for our data.\nHow does this compare to what we‚Äôve done so far with the data?\n\n[your answer here]\n##üíª Q14: EDA plot Now, try to replicate this EDA step in the code chunk below. You can copy-paste from the author‚Äôs work, but you may need to change a few things so it can work on our dataframe nyc\n\n#your code here\n\n##‚úè Q15: Data Cleaning\n\nDescribe something new this workbook has done with data cleaning.\nWhat is one reason this may be a good choice for our data.\nWhat is one reason that this may not be a good choice for our data.\nHow does this compare to what we‚Äôve done so far with the data?\n\n[your answer here]\n##üíª Q16: Data cleaning step Now, try to replicate this data cleaning step in the code chunk below. You can copy-paste from the author‚Äôs work, but you may need to change a few things so it can work on our dataframe nyc\n\n#your code here\n\n#Task 6: Reflection\nTake a moment to reflect on the assingment\n##‚úè Q17: Reflection\nWhat did you like about it? What could be improved? Your answers will not affect your overall grade. This feedback will be used to improve future programming assignments.\n#Grading\nFor each of the following accomplishments, there is a breakdown of points which total to 20. The fraction of points earned out of 20 will be multiplied by 5 to get your final score (e.g.¬†17 points earned will be 17/20 * 5 ‚Üí 4.25) * (1pt) Task1 q1: Correctly filters dataset * (2pt) Task1 q2: Gives 1-2 sectence descriptions for each feature * (2pt) Task2 q3: Discussed reasons for why all the listed features were dropped * (1pt) Task2 q4: Discussed reasons for why all the listed features were kept * (1pt) Task3 q5: Correctly implemented a log function * (1pt) Task3 q6: Identified the reason for adding 1 to each value * (1pt) Task3 q7: Correctly implemented a log_log with add-one function * (1pt) Task4 q8: Correclt implemented a filtering function for neighborhoods * (1pt) Task4 q9: Replicated the OHE code for neighborhoods * (1pt) Task4 q10: Discusses why we can do OHE without cleaning * (2pt) Task5 q11: Gives two scenarios to improve model performance for our airbnb dataset. Must include specific examples from the data. * (1pt) Task5 q12: Url of inspiration is included * (1pt) Task5 q13: Discusses new EDA from inspiration notebook * (1pt) Task5 q14: Replicates (successfully) the EDA plot * (1pt) Task5 q15: Discusses new data cleaning from inspiration notebook * (1pt) Task5 q16: Replicates (successfully) the data cleaning * (1pt) Task6 q17: Thoughtfully reflected on the assignment"
  },
  {
    "objectID": "course_information/schedule.html",
    "href": "course_information/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "week\ndow\ndate\nsession\nprepare\ntopic\nmaterials\n\n\n\n\n1\nM\nJan 5\nZ\nread the syllabus\nIntro to ML  Using GitHub\nüë©üèª‚Äçüè´intro to ml  üë©üèª‚Äçüè´github websites  üíªLab0\n\n\n\nT\nJan 6\nA\nüìö gg-book - grammar  üìö socviz\nEDA: graphs\nüë©üèª‚Äçüè´EDA1  üìùHW1  ‚òëÔ∏èPC1\n\n\n\nW\nJan 7\nB\nüìã EDA Standard\nEDA: statistics  Quiz: EDA\nüë©üèª‚Äçüè´EDA2  üíª Lab1",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This is the homepage for CSCI 200 - Introduction to Machine Learning taught by Prof.¬†Kim Mandery in January 2026 at St.¬†Olaf College. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here .",
    "crumbs": [
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Machine Learning",
    "section": "",
    "text": "This is the homepage for CSCI 200 - Introduction to Machine Learning taught by Prof.¬†Kim Mandery in January 2026 at St.¬†Olaf College. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here .",
    "crumbs": [
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "slides/00-intro-ml.html#daily-structure",
    "href": "slides/00-intro-ml.html#daily-structure",
    "title": "Welcome to ML!",
    "section": "Daily Structure",
    "text": "Daily Structure\n\nA Days:\n\ntwo morning lectures\nafternoon homework and project check\n\nB Days:\n\none morning lecture\none morning Lab\nafternoon standard quiz"
  },
  {
    "objectID": "slides/00-intro-ml.html#themes-what-why-and-how",
    "href": "slides/00-intro-ml.html#themes-what-why-and-how",
    "title": "Welcome to ML!",
    "section": "Themes: what, why, and how",
    "text": "Themes: what, why, and how\n\nWhat: the plot\n\nblah\n\n\n\n\nHow: the process\n\nblah\n\n\n\n\n\nWhy: the theory\n\nTie together ‚Äúhow‚Äù and ‚Äúwhat‚Äù"
  },
  {
    "objectID": "slides/00-intro-ml.html#show-and-tell",
    "href": "slides/00-intro-ml.html#show-and-tell",
    "title": "Welcome to ML!",
    "section": "Show and tell",
    "text": "Show and tell\n\n\nForm a small group (2-4 people) with people sitting around you\nFirst, introduce yourselves to each other ‚Äì name (and proper pronunciation of name), year, major, where are you from, etc."
  },
  {
    "objectID": "slides/00-intro-ml.html#course-website",
    "href": "slides/00-intro-ml.html#course-website",
    "title": "Welcome to ML!",
    "section": "Course website",
    "text": "Course website\n\nML_J2026\n\n\naka ‚Äúthe one link to rule them all‚Äù"
  },
  {
    "objectID": "slides/00-intro-ml.html#lectures",
    "href": "slides/00-intro-ml.html#lectures",
    "title": "Welcome to ML!",
    "section": "Lectures",
    "text": "Lectures\n\nIn person\nAttendance is required\nA little bit of everything:\n\nTraditional lecture\nLive coding + demos\nShort exercises\nDiscussions"
  },
  {
    "objectID": "slides/00-intro-ml.html#labs",
    "href": "slides/00-intro-ml.html#labs",
    "title": "Welcome to ML!",
    "section": "Labs",
    "text": "Labs\n\nOccurs every other day in the morning (check schedule for üíª)"
  },
  {
    "objectID": "slides/00-intro-ml.html#quizzes",
    "href": "slides/00-intro-ml.html#quizzes",
    "title": "Welcome to ML!",
    "section": "Quizzes",
    "text": "Quizzes\n\nOccurs every other day in the afternoon (check schedule for üìÑ)"
  },
  {
    "objectID": "slides/00-intro-ml.html#announcements",
    "href": "slides/00-intro-ml.html#announcements",
    "title": "Welcome to ML!",
    "section": "Announcements",
    "text": "Announcements\n\nSent via email"
  },
  {
    "objectID": "slides/00-intro-ml.html#this-weeks-tasks",
    "href": "slides/00-intro-ml.html#this-weeks-tasks",
    "title": "Welcome to ML!",
    "section": "This week‚Äôs tasks",
    "text": "This week‚Äôs tasks\n\nCreate a GitHub account if you don‚Äôt have one\nRead the syllabus\nPost on Discussion board\nComplete the readings"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "Stuff about labs",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#overview",
    "href": "labs.html#overview",
    "title": "Labs",
    "section": "",
    "text": "Stuff about labs",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#list",
    "href": "labs.html#list",
    "title": "Labs",
    "section": "List",
    "text": "List\nLab 0",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "homework/hw1.html",
    "href": "homework/hw1.html",
    "title": "Machine Learning",
    "section": "",
    "text": "resting place for HW1"
  },
  {
    "objectID": "standards/standards_overview.html",
    "href": "standards/standards_overview.html",
    "title": "Machine Learning",
    "section": "",
    "text": "General information around standards and FAQ."
  }
]