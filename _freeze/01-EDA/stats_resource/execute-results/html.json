{
  "hash": "594043cc1d93c32025ecc30029a0ae1f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Statistics for Machine Learning\"\nsubtitle: \"Essential Concepts with Python\"\nformat:\n  revealjs:\n    theme: white\n    slide-number: true\n    chalkboard: true\n    code-fold: false\n    code-line-numbers: false\n    scrollable: true\n    smaller: true\n    progress: true\n    highlight-style: tango\n    self-contained: true\njupyter: python3\n---\n\n## Setup\n\n::: {#f48a3cf8 .cell execution_count=1}\n``` {.python .cell-code}\n# Grab the libraries we want to use\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Clear out pesky warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better visibility\nplt.style.use('ggplot')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n```\n:::\n\n\n::: {#bda3977e .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLibraries loaded successfully!\nNumPy version: 2.3.4\nPandas version: 2.3.3\n```\n:::\n:::\n\n\nYou can update the style to your liking by looking at this [matplotlib style guide](https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html).\n\n## Introduction\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Key Topics**\n\n- Exploratory Data Analysis\n- Data & Sampling Distributions\n- Statistical Inference\n- Practical Applications\n:::\n\n::: {.column width=\"50%\"}\n**Why This Matters**\n\n- Data science merges statistics, CS, and domain knowledge\n- Understanding variability is crucial\n- Modern tools enable practical analysis\n:::\n::::\n\n---\n\n# Chapter 1: Exploratory Data Analysis\n\n---\n\n## Data Types\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Numeric Data**\n\n- **Continuous**: Any value\n    - `25.2, 24, 29.131414`\n    - Useful for precise measurements and calculations\n- **Discrete**: Integer values  \n    - `25, 24, 29`\n    - Useful for counts\n:::\n\n::: {.column width=\"50%\"}\n**Categorical Data**\n\n- **Categorical**: Fixed set of values \n    - `'Dog', 'Cat', 'Bat'`\n    - typical for factors/ bins / categories\n- **Binary**: Two categories \n    - `0, 1` or `True, False`\n    - Useful for indicators\n- **Ordinal**: Ordered categories\n    - `'low', 'medium', 'high'`\n    - categories more similar to neighbor categories\n:::\n::::\n\n---\n\n## Data Types in Python\n\n::: {#94af8218 .cell execution_count=3}\n``` {.python .cell-code}\n# Create sample data\ndata = {\n    'age': [25, 30, 35, 40],  # Continuous\n    'visits': [3, 5, 2, 7],  # Discrete\n    'category': ['A', 'B', 'A', 'C'],  # Categorical\n    'is_member': [True, False, True, True],  # Binary\n    'rating': ['low', 'medium', 'high', 'medium']  # Ordinal\n}\n\ndf = pd.DataFrame(data)\n```\n:::\n\n\n::: {#97d65088 .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSince this is a small dataset, we can display the entire thing:\n   age  visits category  is_member  rating\n0   25       3        A       True     low\n1   30       5        B      False  medium\n2   35       2        A       True    high\n3   40       7        C       True  medium\n\nIf this was a larger dataset, use `df.head()` to see the first couple of entries\n\nWe can also see the data type for each feature (column) using `df.dtypes`:\nage           int64\nvisits        int64\ncategory     object\nis_member      bool\nrating       object\ndtype: object\n```\n:::\n:::\n\n\n---\n\n## Tabular Data Structure\n\n**Key Terms**\n\n- **Data Frame**: Basic structure for analysis\n- **Feature**: A column (attribute, predictor, variable)\n- **Record**: A row (case, observation, sample)\n- **Outcome**: Target variable to predict\n\n::: {#4770f1b0 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-6-output-1.png){width=771 height=374}\n:::\n:::\n\n\n---\n\n## Estimates of Location\n\n- **Mean**: Average value which is sensitive to outliers (extreme values)\n$$\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n$$\n\n- **Median**: Middle value (50th percentile) which is more robust to outliers \n$$\n\\text{Median} =\n\\begin{cases} \nx_{\\frac{n+1}{2}}, & n \\text{ odd} \\\\[2mm]\n\\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}, & n \\text{ even} \n\\end{cases}\n$$\n\n- **Trimmed Mean**: Average after removing extremes  \n$$\n\\bar{x}_{\\text{trim}} = \\frac{1}{n - 2k} \\sum_{i=k+1}^{n-k} x_{(i)}\n$$\nwhere \\(k\\) values are removed from each end.\n\n\n---\n\n## Computing Location Estimates\n\n::: {#b8a336e5 .cell execution_count=6}\n``` {.python .cell-code}\n# Sample data: state populations (in millions)\npopulation = np.array([4.78, 0.71, 6.39, 2.92, 37.25, \n                       5.03, 3.57, 0.90])\n\nmean_pop = np.mean(population)\n\nmedian_pop = np.median(population)\n\ndef trim_mean(data, trim_percent):\n    n = len(data)\n    k = int(n * trim_percent)\n    sorted_data = np.sort(data)\n    return np.mean(sorted_data[k:n-k])\n\ntrimmed_mean = trim_mean(population, 0.1) #trimming 10%\n```\n:::\n\n\n::: {#49d2960e .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPopulation data: [ 4.78  0.71  6.39  2.92 37.25  5.03  3.57  0.9 ]\n\nMean: 7.69M\nMedian: 4.17M\nTrimmed Mean (10%): 7.69M\n\nNote: Mean > Trimmed Mean > Median due to outliers\n```\n:::\n:::\n\n\n---\n\n## Weighted Mean\n\n::: {#5278bc49 .cell execution_count=8}\n``` {.python .cell-code}\n# Murder rates and populations by state\nmurder_rate = np.array([5.7, 5.6, 4.7, 5.6, 4.4])\npopulation = np.array([4.78, 0.71, 6.39, 2.92, 37.25])\nstates = ['AL', 'AK', 'AZ', 'AR', 'CA']\n\n# Simple mean (treats all states equally)\nsimple_mean = np.mean(murder_rate)\n\n# Weighted mean (accounts for population)\nweighted_mean = np.average(murder_rate, weights=population)\n```\n:::\n\n\n::: {#6de81101 .cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nState Data:\n  AL: Rate=5.7, Pop=4.78M\n  AK: Rate=5.6, Pop=0.71M\n  AZ: Rate=4.7, Pop=6.39M\n  AR: Rate=5.6, Pop=2.92M\n  CA: Rate=4.4, Pop=37.25M\n\nSimple Mean Murder Rate: 5.20\nWeighted Mean Murder Rate: 4.64\n\nWeighted mean is lower because CA (largest state) has lowest rate\n```\n:::\n:::\n\n\n---\n\n## Estimates of Variability\n\n- **Range**: Difference between maximum and minimum  \n$$\n\\text{Range} = x_{\\text{max}} - x_{\\text{min}}\n$$\n\n- **Interquartile Range (IQR)**: Difference between 75th and 25th percentile  \n$$\n\\text{IQR} = Q_3 - Q_1\n$$\n\n- **Variance**: Average squared deviation from mean  \n$$\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n$$\n\n- **Standard Deviation**: Square root of variance \n\n- **Mean Absolute Deviation (MAD)**: Average absolute deviation  \n$$\n\\text{MAD} = \\frac{1}{n} \\sum_{i=1}^{n} \\big| x_i - \\bar{x} \\big|\n$$\n\n\n\n---\n\n## Computing Variability\n\n::: {#da087952 .cell execution_count=10}\n``` {.python .cell-code}\ndata = np.array([1, 4, 4, 7, 10, 15])\n\ndata_range = np.max(data) - np.min(data)\nq75, q25 = np.percentile(data, [75, 25])\niqr = q75 - q25\nvariance = np.var(data, ddof=1)  # ddof=1 for sample variance\nstd_dev = np.std(data, ddof=1)\nmad = np.mean(np.abs(data - np.mean(data)))\n```\n:::\n\n\n::: {#be52d9c1 .cell execution_count=11}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nData: [ 1  4  4  7 10 15]\n\nMeasures of Variability:\n  Variance: 25.37\n  Standard Deviation: 5.04\n  Mean Absolute Deviation: 3.83\n  Range: 14.00\n  IQR (Q75 - Q25): 5.25\n```\n:::\n:::\n\n\n---\n\n## Interpreting Variability\n\nSo what did all those values even mean? \n\nThe numbers in our dataset are spread out, but not too wildly. \n\n\nThe `range` of 14 tells us how far apart the smallest and largest numbers are. \n\nLooking closer, the middle half of the numbers (the `IQR` of 6) are much closer together, which means a few unusually high or low numbers are stretching the range. \n\nMeasures like `variance` (36.80) and `standard deviation` (6.07) give a sense of how much the numbers tend to differ from the average, while the `mean absolute deviation` (4.83) shows a typical “typical difference” in a way that isn’t affected as much by extreme values. \n\n\nAltogether, this tells us that while there are a few numbers that are far from the rest, most of the data is relatively clustered near the middle.\n\n---\n\n## Percentiles and Boxplots\n\n::: {#74dc97d8 .cell fig-height='3.5' execution_count=12}\n``` {.python .cell-code}\nnp.random.seed(42)\ndata = np.random.exponential(scale=2, size=100)\npercentiles = np.percentile(data, [5, 25, 50, 75, 95])\n```\n:::\n\n\n::: {#1dd3a09d .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\nPercentiles (5th, 25th, 50th, 75th, 95th):\n  5th percentile: 0.09\n  25th percentile: 0.43\n  50th percentile: 1.25\n  75th percentile: 2.62\n  95th percentile: 5.95\n```\n:::\n:::\n\n\n::: {#7838ab41 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show/Hide Code\"}\nfig, ax = plt.subplots(figsize=(10, 3))\nbp = ax.boxplot(data, vert=False, widths=0.5)\nax.set_xlabel('Value')\nax.set_title('Boxplot of Exponential Distribution')\nax.grid(alpha=0.3)\n\n# Add percentile markers\nfor p, val in zip([25, 50, 75], [percentiles[1], percentiles[2], percentiles[3]]):\n    ax.axvline(val, color='red', linestyle='--', alpha=0.5, linewidth=1)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-15-output-1.png){width=950 height=278}\n:::\n:::\n\n\n---\n\n## Frequency Tables and Histograms\n\n::: {#384ae3e1 .cell execution_count=15}\n``` {.python .cell-code}\nnp.random.seed(42)\ndata = np.random.normal(100, 15, 1000)\n```\n:::\n\n\n::: {#7386c469 .cell fig-height='3.5' execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show/Hide Code\"}\nfig, ax = plt.subplots(figsize=(10, 3.5))\ncounts, bins, patches = ax.hist(data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\nax.set_xlabel('Value')\nax.set_ylabel('Frequency')\nax.set_title('Histogram of Normal Distribution (μ=100, σ=15)')\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-17-output-1.png){width=951 height=326}\n:::\n:::\n\n\n::: {#a233704b .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFrequency Table (first 5 bins):\n  [51.4, 54.9): 1 observations\n  [54.9, 58.5): 0 observations\n  [58.5, 62.0): 3 observations\n  [62.0, 65.6): 3 observations\n  [65.6, 69.1): 6 observations\n\nTotal observations: 1000\nMean: 100.29, Std: 14.69\n```\n:::\n:::\n\n\n---\n\n## Density Plots\n\n::: {#10252498 .cell fig-height='3.5' execution_count=18}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show/Hide Code\"}\n# Kernel density estimation (simple implementation)\ndef kde(data, x_eval, bandwidth=0.5):\n    \"\"\"Simple Gaussian kernel density estimation\"\"\"\n    n = len(data)\n    density = np.zeros_like(x_eval, dtype=float)\n    for xi in data:\n        kernel = np.exp(-0.5 * ((x_eval - xi) / bandwidth) ** 2)\n        kernel /= (bandwidth * np.sqrt(2 * np.pi))\n        density += kernel\n    return density / n\n\n# Generate data\nnp.random.seed(42)\ndata = np.random.normal(100, 15, 1000)\n\n# Generate density estimate\nxs = np.linspace(data.min(), data.max(), 200)\ndensity = kde(data, xs, bandwidth=3)\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.hist(data, bins=30, density=True, alpha=0.5, \n        edgecolor='black', label='Histogram', color='lightblue')\nax.plot(xs, density, 'r-', linewidth=2, label='Density Estimate')\nax.set_xlabel('Value')\nax.set_ylabel('Density')\nax.set_title('Density Plot: Smoothed Distribution')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-19-output-1.png){width=951 height=326}\n:::\n:::\n\n\nDensity estimation smooths the histogram. \nBandwidth parameter controls smoothness\n\n\n---\n\n## Correlation\n\n**Pearson Correlation Coefficient**\n\n- Measures linear association between variables\n- Ranges from -1 (perfect negative) to +1 (perfect positive)\n- 0 indicates no linear correlation\n\n$$r = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y})}{(N-1)s_x s_y}$$\n\n---\n\n## Computing Correlation\n\n::: {#48f063d5 .cell execution_count=19}\n``` {.python .cell-code}\n# Generate correlated data\nnp.random.seed(42)\nx = np.random.normal(0, 1, 100)\ny = 2 * x + np.random.normal(0, 0.5, 100)\n\n# Correlation coefficient\ncorrelation = np.corrcoef(x, y)[0, 1]\nprint(f\"Correlation between x and y: {correlation:.3f}\")\nprint(f\"Interpretation: Strong positive linear relationship\")\n\n# Correlation matrix for multiple variables\nz = x + y  # Create third variable\ndata_matrix = np.column_stack([x, y, z])\ncorr_matrix = np.corrcoef(data_matrix.T)\n\nprint(\"\\nCorrelation Matrix:\")\nprint(\"      x      y      z\")\nfor i, row in enumerate(corr_matrix):\n    label = ['x', 'y', 'z'][i]\n    print(f\"{label}: {row[0]:6.3f} {row[1]:6.3f} {row[2]:6.3f}\")\n\nprint(\"\\nNote: Correlation ranges from -1 (perfect negative) to +1 (perfect positive)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorrelation between x and y: 0.965\nInterpretation: Strong positive linear relationship\n\nCorrelation Matrix:\n      x      y      z\nx:  1.000  0.965  0.985\ny:  0.965  1.000  0.996\nz:  0.985  0.996  1.000\n\nNote: Correlation ranges from -1 (perfect negative) to +1 (perfect positive)\n```\n:::\n:::\n\n\n---\n\n## Scatterplot with Correlation\n\n::: {#2c900baf .cell fig-height='3.5' execution_count=20}\n``` {.python .cell-code}\nnp.random.seed(42)\nx = np.random.normal(0, 1, 100)\ny = 2 * x + np.random.normal(0, 0.5, 100)\ncorrelation = np.corrcoef(x, y)[0, 1]\n\nfig, ax = plt.subplots(figsize=(8, 3.5))\nax.scatter(x, y, alpha=0.6, s=50, color='steelblue', edgecolors='black', linewidth=0.5)\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('Y', fontsize=12)\nax.set_title(f'Scatterplot with Correlation (r = {correlation:.3f})', fontsize=14)\nax.grid(alpha=0.3)\n\n# Add regression line\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nx_line = np.linspace(x.min(), x.max(), 100)\nax.plot(x_line, p(x_line), \"r-\", linewidth=2, alpha=0.8, label='Best Fit Line')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Slope of regression line: {z[0]:.3f}\")\nprint(f\"Intercept: {z[1]:.3f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-21-output-1.png){width=759 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSlope of regression line: 1.928\nIntercept: 0.004\n```\n:::\n:::\n\n\n---\n\n## Hexagonal Binning\n\n::: {#33ca3224 .cell fig-height='3.5' execution_count=21}\n``` {.python .cell-code}\n# Large dataset example\nnp.random.seed(42)\nn = 10000\nx = np.random.normal(0, 1, n)\ny = 2 * x + np.random.normal(0, 1, n)\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nhb = ax.hexbin(x, y, gridsize=30, cmap='Blues', mincnt=1, edgecolors='black', linewidths=0.2)\ncb = plt.colorbar(hb, ax=ax, label='Count')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title(f'Hexagonal Binning for Large Datasets (n={n:,})')\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Total data points: {n:,}\")\nprint(f\"Correlation: {np.corrcoef(x, y)[0,1]:.3f}\")\nprint(f\"\\nHexagonal binning aggregates dense point clouds into bins\")\nprint(f\"Useful when scatterplots become too dense to interpret\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-22-output-1.png){width=888 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal data points: 10,000\nCorrelation: 0.894\n\nHexagonal binning aggregates dense point clouds into bins\nUseful when scatterplots become too dense to interpret\n```\n:::\n:::\n\n\n---\n\n## Categorical Data: Bar Charts\n\n::: {#a1de0b44 .cell fig-height='3.5' execution_count=22}\n``` {.python .cell-code}\n# Flight delay causes\ncauses = ['Carrier', 'ATC', 'Weather', 'Security', 'Inbound']\npercentages = [23.02, 30.40, 4.03, 0.12, 42.43]\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nbars = ax.bar(causes, percentages, color='steelblue', edgecolor='black', alpha=0.8)\nax.set_ylabel('Percentage (%)', fontsize=12)\nax.set_title('Airline Delays by Cause at DFW Airport', fontsize=14)\nax.grid(axis='y', alpha=0.3)\n\n# Add percentage labels on bars\nfor bar, pct in zip(bars, percentages):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{pct:.1f}%', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Delay Causes Summary:\")\nfor cause, pct in zip(causes, percentages):\n    print(f\"  {cause:12s}: {pct:5.2f}%\")\nprint(f\"\\nTotal: {sum(percentages):.2f}%\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-23-output-1.png){width=951 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nDelay Causes Summary:\n  Carrier     : 23.02%\n  ATC         : 30.40%\n  Weather     :  4.03%\n  Security    :  0.12%\n  Inbound     : 42.43%\n\nTotal: 100.00%\n```\n:::\n:::\n\n\n---\n\n## Contingency Tables\n\n::: {#1e6c1c4e .cell execution_count=23}\n``` {.python .cell-code}\n# Create sample loan data\nnp.random.seed(42)\ngrades = np.random.choice(['A', 'B', 'C', 'D'], size=1000, \n                          p=[0.2, 0.3, 0.3, 0.2])\nstatus = np.random.choice(['Paid', 'Current', 'Late', 'Charged Off'],\n                          size=1000, p=[0.25, 0.65, 0.05, 0.05])\n\n# Create contingency table\nct = pd.crosstab(grades, status, margins=True)\nprint(\"Contingency Table (Counts):\")\nprint(ct)\n\n# With row percentages\nct_pct = pd.crosstab(grades, status, normalize='index') * 100\nprint(\"\\n\\nRow Percentages (by Grade):\")\nprint(ct_pct.round(1))\n\nprint(\"\\n\\nInterpretation:\")\nprint(\"Each cell shows percentage of loans in that grade with that status\")\nprint(\"Example: What % of Grade A loans are Current, Paid, Late, etc.?\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContingency Table (Counts):\ncol_0  Charged Off  Current  Late  Paid   All\nrow_0                                        \nA               12      143     9    61   225\nB               14      174    14    76   278\nC               20      196    10    72   298\nD               13      132     5    49   199\nAll             59      645    38   258  1000\n\n\nRow Percentages (by Grade):\ncol_0  Charged Off  Current  Late  Paid\nrow_0                                  \nA              5.3     63.6   4.0  27.1\nB              5.0     62.6   5.0  27.3\nC              6.7     65.8   3.4  24.2\nD              6.5     66.3   2.5  24.6\n\n\nInterpretation:\nEach cell shows percentage of loans in that grade with that status\nExample: What % of Grade A loans are Current, Paid, Late, etc.?\n```\n:::\n:::\n\n\n---\n\n## Violin Plots\n\n::: {#ccb6e478 .cell fig-height='3.5' execution_count=24}\n``` {.python .cell-code}\n# Compare distributions across groups\nnp.random.seed(42)\ngroup_a = np.random.normal(10, 2, 100)\ngroup_b = np.random.normal(12, 3, 100)\ngroup_c = np.random.normal(11, 1.5, 100)\n\ndata_violin = [group_a, group_b, group_c]\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nparts = ax.violinplot(data_violin, positions=[1, 2, 3], \n                      showmeans=True, showmedians=True)\nax.set_xticks([1, 2, 3])\nax.set_xticklabels(['Group A', 'Group B', 'Group C'])\nax.set_ylabel('Value', fontsize=12)\nax.set_title('Violin Plot: Comparing Distribution Shapes', fontsize=14)\nax.grid(alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprint(\"Summary Statistics by Group:\")\nfor name, data in [('A', group_a), ('B', group_b), ('C', group_c)]:\n    print(f\"  Group {name}: Mean={np.mean(data):.2f}, Std={np.std(data, ddof=1):.2f}\")\nprint(\"\\nViolin width shows density at each value level\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-25-output-1.png){width=951 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary Statistics by Group:\n  Group A: Mean=9.79, Std=1.82\n  Group B: Mean=12.07, Std=2.86\n  Group C: Mean=11.10, Std=1.63\n\nViolin width shows density at each value level\n```\n:::\n:::\n\n\n---\n\n# Chapter 2: Data and Sampling Distributions\n\n---\n\n## Random Sampling Concepts\n\n**Key Terms**\n\n- **Population**: The complete dataset (real or theoretical)\n- **Sample**: A subset drawn from the population\n- **Random Sampling**: Each element has equal chance of selection\n- **Sample Bias**: Sample systematically differs from population\n- **Stratified Sampling**: Random sampling from defined subgroups\n\n---\n\n## Random Sampling in Python\n\n::: {#87dd9037 .cell execution_count=25}\n``` {.python .cell-code}\n# Create a population\nnp.random.seed(42)\npopulation = np.random.normal(100, 15, 10000)\n\n# Simple random sample\nsample_size = 100\nsimple_sample = np.random.choice(population, size=sample_size, \n                                 replace=False)\n\nprint(f\"Population size: {len(population):,}\")\nprint(f\"Sample size: {sample_size}\")\nprint(f\"\\nPopulation Mean: {np.mean(population):.2f}\")\nprint(f\"Population Std Dev: {np.std(population, ddof=1):.2f}\")\nprint(f\"\\nSample Mean: {np.mean(simple_sample):.2f}\")\nprint(f\"Sample Std Dev: {np.std(simple_sample, ddof=1):.2f}\")\n\n# Show difference\ndiff = abs(np.mean(simple_sample) - np.mean(population))\nprint(f\"\\nDifference in means: {diff:.2f}\")\nprint(f\"Sampling error (expected): ~{15/np.sqrt(sample_size):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPopulation size: 10,000\nSample size: 100\n\nPopulation Mean: 99.97\nPopulation Std Dev: 15.05\n\nSample Mean: 101.75\nSample Std Dev: 16.06\n\nDifference in means: 1.78\nSampling error (expected): ~1.50\n```\n:::\n:::\n\n\n---\n\n## Stratified Sampling\n\n::: {#a88995a7 .cell execution_count=26}\n``` {.python .cell-code}\n# Create stratified population (3 distinct groups)\nnp.random.seed(42)\nstratum_a = np.random.normal(100, 10, 7000)  # 70% of population\nstratum_b = np.random.normal(120, 15, 2000)  # 20% of population\nstratum_c = np.random.normal(90, 12, 1000)   # 10% of population\n\nfull_population = np.concatenate([stratum_a, stratum_b, stratum_c])\ntrue_mean = np.mean(full_population)\n\n# Sample from each stratum proportionally\nn = 300\nsample_a = np.random.choice(stratum_a, size=int(n*0.7))\nsample_b = np.random.choice(stratum_b, size=int(n*0.2))\nsample_c = np.random.choice(stratum_c, size=int(n*0.1))\n\nstratified_sample = np.concatenate([sample_a, sample_b, sample_c])\n\n# Compare with simple random sample\nsimple_sample = np.random.choice(full_population, size=n)\n\nprint(f\"True Population Mean: {true_mean:.2f}\")\nprint(f\"Stratified Sample Mean: {np.mean(stratified_sample):.2f}\")\nprint(f\"Simple Random Sample Mean: {np.mean(simple_sample):.2f}\")\nprint(f\"\\nStratified sampling ensures representation from all groups\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue Population Mean: 103.00\nStratified Sample Mean: 102.56\nSimple Random Sample Mean: 101.53\n\nStratified sampling ensures representation from all groups\n```\n:::\n:::\n\n\n---\n\n## Sampling Distribution\n\nThe **sampling distribution** is the distribution of a sample statistic over many samples.\n\n**Key Insight**: Even if individual data isn't normally distributed, sample means tend toward normality (Central Limit Theorem)\n\n---\n\n## Demonstrating Sampling Distribution\n\n::: {#58b32bb9 .cell fig-height='3.5' execution_count=27}\n``` {.python .cell-code}\n# Draw many samples and compute means\nnp.random.seed(42)\npopulation = np.random.exponential(scale=2, size=10000)\nsample_means = [np.mean(np.random.choice(population, 30)) \n                for _ in range(1000)]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\n\naxes[0].hist(population, bins=50, edgecolor='black', alpha=0.7, color='lightcoral')\naxes[0].set_title('Population Distribution (Exponential)', fontsize=12)\naxes[0].set_xlabel('Value')\naxes[0].set_ylabel('Frequency')\n\naxes[1].hist(sample_means, bins=30, edgecolor='black', alpha=0.7, color='lightblue')\naxes[1].set_title('Sampling Distribution of Mean (n=30, 1000 samples)', fontsize=12)\naxes[1].set_xlabel('Sample Mean')\naxes[1].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Population mean: {np.mean(population):.3f}\")\nprint(f\"Mean of sample means: {np.mean(sample_means):.3f}\")\nprint(f\"Std dev of sample means: {np.std(sample_means):.3f}\")\nprint(f\"\\nNote: Even though population is skewed, sampling distribution is normal!\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-28-output-1.png){width=1143 height=327}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nPopulation mean: 1.955\nMean of sample means: 1.968\nStd dev of sample means: 0.343\n\nNote: Even though population is skewed, sampling distribution is normal!\n```\n:::\n:::\n\n\n---\n\n## Central Limit Theorem\n\n**The CLT states**: As sample size increases, the sampling distribution of the mean approaches a normal distribution, regardless of the population's distribution.\n\n**Requirements**:\n- Large enough sample size (typically n > 30)\n- Population not too heavily skewed\n\n---\n\n## Standard Error\n\n**Standard Error (SE)**: The standard deviation of a sampling distribution\n\n$$SE = \\frac{s}{\\sqrt{n}}$$\n\nWhere:\n- s = sample standard deviation\n- n = sample size\n\n**Square-root of n rule**: To reduce SE by half, need 4× the sample size\n\n---\n\n## Computing Standard Error\n\n::: {#7d380363 .cell execution_count=28}\n``` {.python .cell-code}\n# Generate sample\nnp.random.seed(42)\nsample = np.random.normal(100, 15, 50)\n\n# Calculate standard error (formula-based)\nse_formula = np.std(sample, ddof=1) / np.sqrt(len(sample))\n\nprint(f\"Sample size: {len(sample)}\")\nprint(f\"Sample Mean: {np.mean(sample):.2f}\")\nprint(f\"Sample Std Dev: {np.std(sample, ddof=1):.2f}\")\nprint(f\"Standard Error (formula): {se_formula:.2f}\")\n\n# Verify with multiple samples (empirical approach)\nsample_means = [np.mean(np.random.normal(100, 15, 50)) \n                for _ in range(1000)]\nempirical_se = np.std(sample_means)\n\nprint(f\"\\nEmpirical SE (from 1000 samples): {empirical_se:.2f}\")\nprint(f\"\\nThey match! SE = σ / √n = 15 / √50 ≈ {15/np.sqrt(50):.2f}\")\n\n# Demonstrate square-root of n rule\nprint(f\"\\nSquare-root of n rule:\")\nprint(f\"  To halve SE, need 4× sample size: {se_formula:.2f} → {se_formula/2:.2f}\")\nprint(f\"  New sample size needed: {50 * 4} (was 50)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample size: 50\nSample Mean: 96.62\nSample Std Dev: 14.01\nStandard Error (formula): 1.98\n\nEmpirical SE (from 1000 samples): 2.15\n\nThey match! SE = σ / √n = 15 / √50 ≈ 2.12\n\nSquare-root of n rule:\n  To halve SE, need 4× sample size: 1.98 → 0.99\n  New sample size needed: 200 (was 50)\n```\n:::\n:::\n\n\n---\n\n## The Bootstrap\n\n**Bootstrap Resampling**: Sample WITH replacement from observed data to estimate sampling distribution\n\n**Algorithm**:\n1. Draw n samples with replacement from original data\n2. Calculate statistic of interest\n3. Repeat B times (e.g., 1000)\n4. Use distribution of statistics for inference\n\n---\n\n## Bootstrap Implementation\n\n::: {#0618904b .cell fig-height='3.5' execution_count=29}\n``` {.python .cell-code}\n# Original sample\nnp.random.seed(42)\noriginal_sample = np.random.normal(100, 15, 50)\n\n# Bootstrap resampling\nn_bootstrap = 1000\nbootstrap_means = []\n\nfor _ in range(n_bootstrap):\n    resample = np.random.choice(original_sample, \n                                size=len(original_sample), \n                                replace=True)  # WITH replacement!\n    bootstrap_means.append(np.mean(resample))\n\nbootstrap_means = np.array(bootstrap_means)\n\n# Plot results\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.hist(bootstrap_means, bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\nax.axvline(np.mean(original_sample), color='red', \n           linestyle='--', linewidth=2, label='Original Sample Mean')\nax.set_xlabel('Bootstrap Mean')\nax.set_ylabel('Frequency')\nax.set_title(f'Bootstrap Distribution ({n_bootstrap} resamples)')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original Sample Mean: {np.mean(original_sample):.2f}\")\nprint(f\"Bootstrap SE: {np.std(bootstrap_means):.2f}\")\nprint(f\"Formula SE: {np.std(original_sample, ddof=1)/np.sqrt(50):.2f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-30-output-1.png){width=951 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal Sample Mean: 96.62\nBootstrap SE: 2.00\nFormula SE: 1.98\n```\n:::\n:::\n\n\n---\n\n## Confidence Intervals\n\n**Confidence Interval**: A range that likely contains the true population parameter\n\n**90% CI from Bootstrap**:\n- Take the 5th and 95th percentiles of bootstrap distribution\n- Interpretation: If we repeated this process many times, 90% of intervals would contain the true parameter\n\n---\n\n## Computing Confidence Intervals\n\n::: {#3cbc0366 .cell execution_count=30}\n``` {.python .cell-code}\n# Use bootstrap distribution from previous slide\nnp.random.seed(42)\noriginal_sample = np.random.normal(100, 15, 50)\nbootstrap_means = [np.mean(np.random.choice(original_sample, \n                           len(original_sample), replace=True))\n                   for _ in range(1000)]\n\n# 90% Confidence interval\nci_90_lower = np.percentile(bootstrap_means, 5)\nci_90_upper = np.percentile(bootstrap_means, 95)\n\n# 95% Confidence interval\nci_95_lower = np.percentile(bootstrap_means, 2.5)\nci_95_upper = np.percentile(bootstrap_means, 97.5)\n\nprint(f\"Original Sample Mean: {np.mean(original_sample):.2f}\")\nprint(f\"\\n90% Confidence Interval: ({ci_90_lower:.2f}, {ci_90_upper:.2f})\")\nprint(f\"  Width: {ci_90_upper - ci_90_lower:.2f}\")\nprint(f\"\\n95% Confidence Interval: ({ci_95_lower:.2f}, {ci_95_upper:.2f})\")\nprint(f\"  Width: {ci_95_upper - ci_95_lower:.2f}\")\n\nprint(f\"\\nInterpretation:\")\nprint(f\"We are 95% confident the true population mean lies between\")\nprint(f\"{ci_95_lower:.2f} and {ci_95_upper:.2f}\")\nprint(f\"\\nNote: Higher confidence → wider interval\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal Sample Mean: 96.62\n\n90% Confidence Interval: (93.43, 99.96)\n  Width: 6.52\n\n95% Confidence Interval: (92.73, 100.68)\n  Width: 7.95\n\nInterpretation:\nWe are 95% confident the true population mean lies between\n92.73 and 100.68\n\nNote: Higher confidence → wider interval\n```\n:::\n:::\n\n\n---\n\n## Normal Distribution\n\n::: {#76167ed2 .cell fig-height='3.5' execution_count=31}\n``` {.python .cell-code}\n# Normal PDF function\ndef normal_pdf(x, mu=0, sigma=1):\n    \"\"\"Calculate normal probability density function\"\"\"\n    return (1 / (sigma * np.sqrt(2 * np.pi))) * \\\n           np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n\n# Generate normal distribution\nx = np.linspace(-4, 4, 1000)\ny = normal_pdf(x, 0, 1)\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.plot(x, y, 'b-', linewidth=2, label='Standard Normal')\nax.fill_between(x, y, where=(x >= -1) & (x <= 1), \n                alpha=0.3, label='68% (±1 SD)', color='blue')\nax.fill_between(x, y, where=((x >= -2) & (x < -1)) | ((x > 1) & (x <= 2)), \n                alpha=0.2, label='95% (±2 SD)', color='green')\nax.set_xlabel('Z-score (Standard Deviations)')\nax.set_ylabel('Density')\nax.set_title('Standard Normal Distribution')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Standard Normal Properties:\")\nprint(\"  68% of data within ±1 SD\")\nprint(\"  95% of data within ±2 SD\")\nprint(\"  99.7% of data within ±3 SD\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-32-output-1.png){width=951 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nStandard Normal Properties:\n  68% of data within ±1 SD\n  95% of data within ±2 SD\n  99.7% of data within ±3 SD\n```\n:::\n:::\n\n\n---\n\n## Q-Q Plots\n\n::: {#61f07367 .cell fig-height='3.5' execution_count=32}\n``` {.python .cell-code}\ndef qq_plot(data, ax, title):\n    \"\"\"Create Q-Q plot manually\"\"\"\n    standardized = (data - np.mean(data)) / np.std(data, ddof=1)\n    standardized = np.sort(standardized)\n    n = len(standardized)\n    theoretical = np.array([np.percentile(np.random.standard_normal(10000), \n                            100 * (i - 0.5) / n) for i in range(1, n + 1)])\n    ax.scatter(theoretical, standardized, alpha=0.6, s=30)\n    ax.plot([-3, 3], [-3, 3], 'r--', linewidth=2, label='Perfect Normal')\n    ax.set_xlabel('Theoretical Quantiles')\n    ax.set_ylabel('Sample Quantiles')\n    ax.set_title(title)\n    ax.grid(alpha=0.3)\n    ax.legend()\n\nnp.random.seed(42)\nnormal_data = np.random.normal(0, 1, 100)\nexp_data = np.random.exponential(1, 100)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\nqq_plot(normal_data, axes[0], 'Q-Q Plot: Normal Data')\nqq_plot(exp_data, axes[1], 'Q-Q Plot: Exponential Data')\nplt.tight_layout()\nplt.show()\n\nprint(\"Interpretation:\")\nprint(\"  Left: Points on line → data is normally distributed\")\nprint(\"  Right: Points deviate → data is NOT normally distributed\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-33-output-1.png){width=1143 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nInterpretation:\n  Left: Points on line → data is normally distributed\n  Right: Points deviate → data is NOT normally distributed\n```\n:::\n:::\n\n\n---\n\n## Binomial Distribution\n\n**Binomial**: Distribution of number of successes in n independent trials\n\n**Parameters**:\n- n: number of trials\n- p: probability of success\n\n**Example**: 10 coin flips, probability of getting 6 heads\n\n---\n\n## Binomial Distribution in Python\n\n::: {#5c2c1b77 .cell fig-height='3.5' execution_count=33}\n``` {.python .cell-code}\ndef binomial_pmf(k, n, p):\n    \"\"\"Calculate binomial probability mass function\"\"\"\n    def factorial(x):\n        if x <= 1: return 1\n        return x * factorial(x - 1)\n    def comb(n, k):\n        return factorial(n) // (factorial(k) * factorial(n - k))\n    return comb(n, k) * (p ** k) * ((1 - p) ** (n - k))\n\nn = 20  # trials\np = 0.3  # probability of success\n\nx = np.arange(0, n+1)\npmf = np.array([binomial_pmf(k, n, p) for k in x])\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.bar(x, pmf, edgecolor='black', alpha=0.7, color='steelblue')\nax.set_xlabel('Number of Successes')\nax.set_ylabel('Probability')\nax.set_title(f'Binomial Distribution (n={n} trials, p={p} success probability)')\nax.grid(alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprob_5 = binomial_pmf(5, n, p)\nprob_le_5 = sum([binomial_pmf(k, n, p) for k in range(6)])\nprint(f\"P(exactly 5 successes) = {prob_5:.4f}\")\nprint(f\"P(5 or fewer successes) = {prob_le_5:.4f}\")\nprint(f\"Expected value (mean) = n×p = {n*p:.1f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-34-output-1.png){width=951 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nP(exactly 5 successes) = 0.1789\nP(5 or fewer successes) = 0.4164\nExpected value (mean) = n×p = 6.0\n```\n:::\n:::\n\n\n---\n\n## Poisson Distribution\n\n**Poisson**: Distribution of events occurring at a constant rate\n\n**Parameter**: λ (lambda) = average rate\n\n**Examples**:\n- Website visits per hour\n- Customer calls per minute\n- Defects per unit\n\n---\n\n## Poisson Distribution in Python\n\n::: {#3915e9ad .cell fig-height='3.5' execution_count=34}\n``` {.python .cell-code}\nimport math\n\ndef poisson_pmf(k, lam):\n    \"\"\"Calculate Poisson probability mass function\"\"\"\n    return (lam ** k) * np.exp(-lam) / math.factorial(k)\n\nlambda_rate = 3  # average events per interval\n\nx = np.arange(0, 15)\npmf = np.array([poisson_pmf(k, lambda_rate) for k in x])\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.bar(x, pmf, edgecolor='black', alpha=0.7, color='coral')\nax.set_xlabel('Number of Events')\nax.set_ylabel('Probability')\nax.set_title(f'Poisson Distribution (λ={lambda_rate} events/interval)')\nax.grid(alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n# Generate random Poisson data (simulation)\nnp.random.seed(42)\npoisson_data = np.random.poisson(lambda_rate, 100)\nprint(f\"Theoretical mean (λ): {lambda_rate}\")\nprint(f\"Simulated mean: {np.mean(poisson_data):.2f}\")\nprint(f\"Theoretical variance (also λ): {lambda_rate}\")\nprint(f\"Simulated variance: {np.var(poisson_data, ddof=1):.2f}\")\nprint(f\"\\nUse for: arrivals, calls, defects per time/space unit\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-35-output-1.png){width=951 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheoretical mean (λ): 3\nSimulated mean: 2.78\nTheoretical variance (also λ): 3\nSimulated variance: 3.14\n\nUse for: arrivals, calls, defects per time/space unit\n```\n:::\n:::\n\n\n---\n\n## Exponential Distribution\n\n**Exponential**: Time between Poisson events\n\n**Parameter**: λ (rate parameter)\n\n**Example**: Time between customer arrivals\n\n---\n\n## Exponential Distribution in Python\n\n::: {#cf097302 .cell fig-height='3.5' execution_count=35}\n``` {.python .cell-code}\ndef exponential_pdf(x, lam):\n    \"\"\"Calculate exponential probability density function\"\"\"\n    return lam * np.exp(-lam * x)\n\nrate = 0.5  # events per unit time\nscale = 1/rate  # mean time between events\n\nx = np.linspace(0, 10, 1000)\npdf = exponential_pdf(x, rate)\n\nfig, ax = plt.subplots(figsize=(10, 3.5))\nax.plot(x, pdf, 'b-', linewidth=2, label=f'rate={rate}')\nax.fill_between(x, pdf, alpha=0.3)\nax.set_xlabel('Time Between Events')\nax.set_ylabel('Density')\nax.set_title(f'Exponential Distribution (λ={rate}, mean={scale:.1f})')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Generate random exponential data\nnp.random.seed(42)\nexp_data = np.random.exponential(scale=scale, size=100)\nprint(f\"Theoretical mean (1/λ): {scale:.2f}\")\nprint(f\"Simulated mean: {np.mean(exp_data):.2f}\")\nprint(f\"\\nUse for: time between events (complement to Poisson)\")\nprint(f\"Example: time between customer arrivals\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-36-output-1.png){width=951 height=326}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheoretical mean (1/λ): 2.00\nSimulated mean: 1.83\n\nUse for: time between events (complement to Poisson)\nExample: time between customer arrivals\n```\n:::\n:::\n\n\n---\n\n## Key Takeaways\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**EDA**\n\n- Understand your data types\n- Use robust statistics\n- Visualize distributions\n- Check for outliers\n:::\n\n::: {.column width=\"50%\"}\n**Sampling**\n\n- Random sampling reduces bias\n- Bootstrap estimates uncertainty\n- Confidence intervals quantify precision\n- Choose appropriate distributions\n:::\n::::\n\n---\n\n## Practical Workflow\n\n1. **Load and inspect** data\n2. **Calculate** summary statistics (mean, median, std)\n3. **Visualize** distributions (histograms, boxplots)\n4. **Check** for outliers and anomalies\n5. **Assess** relationships (correlation, scatterplots)\n6. **Estimate** uncertainty (bootstrap, confidence intervals)\n7. **Model** appropriately for your data type\n\n---\n\n## Complete Example\n\n::: {#b7e5d359 .cell execution_count=36}\n``` {.python .cell-code}\n# Comprehensive analysis example\nnp.random.seed(42)\ndata = np.random.normal(100, 15, 200)\n\n# 1. Summary statistics\nprint(\"=== SUMMARY STATISTICS ===\")\nprint(f\"Sample size: {len(data)}\")\nprint(f\"Mean: {np.mean(data):.2f}\")\nprint(f\"Median: {np.median(data):.2f}\")\nprint(f\"Std Dev: {np.std(data, ddof=1):.2f}\")\nprint(f\"IQR: {np.percentile(data, 75) - np.percentile(data, 25):.2f}\")\nprint(f\"Range: [{np.min(data):.2f}, {np.max(data):.2f}]\")\n\n# 2. Bootstrap confidence interval\nn_boot = 1000\nboot_means = [np.mean(np.random.choice(data, len(data), replace=True))\n              for _ in range(n_boot)]\nci = np.percentile(boot_means, [2.5, 97.5])\nprint(f\"\\n=== UNCERTAINTY ===\")\nprint(f\"Standard Error: {np.std(data, ddof=1)/np.sqrt(len(data)):.2f}\")\nprint(f\"95% CI: ({ci[0]:.2f}, {ci[1]:.2f})\")\nprint(f\"CI Width: {ci[1] - ci[0]:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n=== SUMMARY STATISTICS ===\nSample size: 200\nMean: 99.39\nMedian: 99.94\nStd Dev: 13.97\nIQR: 18.09\nRange: [60.70, 140.80]\n\n=== UNCERTAINTY ===\nStandard Error: 0.99\n95% CI: (97.36, 101.36)\nCI Width: 4.00\n```\n:::\n:::\n\n\n---\n\n## Complete Example (continued)\n\n::: {#531d520b .cell fig-height='4' execution_count=37}\n``` {.python .cell-code}\n# 3. Comprehensive visualization\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Histogram\naxes[0, 0].hist(data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0, 0].axvline(np.mean(data), color='red', linestyle='--', label='Mean')\naxes[0, 0].axvline(np.median(data), color='green', linestyle='--', label='Median')\naxes[0, 0].set_title('Histogram', fontsize=12)\naxes[0, 0].set_xlabel('Value')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].legend()\n\n# Boxplot\naxes[0, 1].boxplot(data, vert=True)\naxes[0, 1].set_title('Boxplot', fontsize=12)\naxes[0, 1].set_ylabel('Value')\naxes[0, 1].grid(alpha=0.3, axis='y')\n\n# Q-Q Plot\nstandardized = (data - np.mean(data)) / np.std(data, ddof=1)\nstandardized = np.sort(standardized)\nn = len(standardized)\ntheoretical = np.array([np.percentile(np.random.standard_normal(10000), \n                        100 * (i - 0.5) / n) for i in range(1, n + 1)])\naxes[1, 0].scatter(theoretical, standardized, alpha=0.4, s=20)\naxes[1, 0].plot([-3, 3], [-3, 3], 'r--', linewidth=2)\naxes[1, 0].set_xlabel('Theoretical Quantiles')\naxes[1, 0].set_ylabel('Sample Quantiles')\naxes[1, 0].set_title('Q-Q Plot', fontsize=12)\naxes[1, 0].grid(alpha=0.3)\n\n# Bootstrap distribution\naxes[1, 1].hist(boot_means, bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\naxes[1, 1].axvline(ci[0], color='r', linestyle='--', linewidth=2, label='95% CI')\naxes[1, 1].axvline(ci[1], color='r', linestyle='--', linewidth=2)\naxes[1, 1].set_title('Bootstrap Distribution', fontsize=12)\naxes[1, 1].set_xlabel('Bootstrap Mean')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_resource_files/figure-revealjs/cell-38-output-1.png){width=1143 height=759}\n:::\n:::\n\n\n---\n\n## Resources\n\n**Python Libraries**\n\n- [`numpy`](https://numpy.org/doc/stable/user/index.html#user): Numerical computing\n- [`pandas`](https://pandas.pydata.org/docs/user_guide/index.html#user-guide): Data manipulation\n- [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html): Statistical functions\n- [`matplotlib`](https://matplotlib.org/stable/api/index.html): Visualization\n- [`seaborn`](https://seaborn.pydata.org/api.html): Statistical visualization\n\n**Further Reading**\n\n- [Practical Statistics for Data Scientists](../files/Stats4ML.pdf) (Bruce & Bruce)\n- Python for Data Analysis (McKinney)\n- Statistical Inference via Data Science (Ismay & Kim)\n\n---\n\n## Thank You!\n\n**Questions?**\n\nRemember: \n\n- Start with exploration\n- Quantify uncertainty\n- Use appropriate methods\n- Visualize your results\n\n",
    "supporting": [
      "stats_resource_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}